## 2018-08-28

对sysfs中的节点的一次读写操作最多值允许传递一个PAGE_SIZE的数据

---

在sysfs中，所有数据均以 `kobject`或 `kset`者两种数据结构表示:

+ `kobject`: 在 Linux 设备模型中最基本的对象，它的功能是提供引用计数和维持父子(parent)结构、平级(sibling)目录关系， `device`, `device_driver` 等各对象都是以 kobject 基础功能之上实现的:

```c
struct kobject {
    const char              *name;
    struct list_head        entry;
    struct kobject          *parent;
    struct kset             *kset;
    struct kobj_type        *ktype;
    struct sysfs_dirent     *sd;
    struct kref             kref;
    unsigned int state_initialized:1;
    unsigned int state_in_sysfs:1;
    unsigned int state_add_uevent_sent:1;
    unsigned int state_remove_uevent_sent:1;
};
```

其中 struct kref 内含一个 atomic_t 类型用于引用计数， parent 是单个指向父节点的指针， entry 用于父 kset 以链表头结构将 kobject 结构维护成双向链表；

+ `kset`: kset: 它用来对同类型对象提供一个包装集合，在内核数据结构上它也是由内嵌一个 `kboject` 实现，因而它同时也是一个 `kobject` (面向对象 OOP 概念中的继承关系) ，具有 `kobject` 的全部功能；

```c
struct kset {
        struct list_head list;
        spinlock_t list_lock;
        struct kobject kobj;
        struct kset_uevent_ops *uevent_ops;
};
```

其中的 struct list_head list 用于将集合中的 kobject 按 struct list_head entry 维护成双向链表；

---

Cannot set Attribute Permissions to 0666 in sysfs:
在新内核的kernel.h中，有:

```c
#define VERIFY_OCTAL_PERMISSIONS(perms)                
     ...
     /* OTHER_WRITABLE?  Generally considered a bad idea. */ \
     BUILD_BUG_ON_ZERO((perms) & 2) + \
 ...
```

因此 ，不能使用 `__ATTR`等宏来为 `other`用户设置 `write`权限，只能手动设置:

```c
attr.mode = 0666;
```

---

Linux sysfs中，属性分为两种:普通的attribute和二进制attribute，如下：

```c
/* include/linux/sysfs.h, line 26 */
struct attribute {
    const char *name;
     umode_t         mode;
#ifdef CONFIG_DEBUG_LOCK_ALLOC
    bool ignore_lockdep:1;
    struct lock_class_key   *key;
    struct lock_class_key   skey;
#endif
};
  
/* include/linux/sysfs.h, line 100 */
struct bin_attribute {
    struct attribute    attr;
    size_t          size;
    void *private;
    ssize_t (*read)(struct file *, struct kobject *, struct bin_attribute *,
                    char *, loff_t, size_t);
    ssize_t (*write)(struct file *,struct kobject *, struct bin_attribute *,
                    char *, loff_t, size_t);
    int (*mmap)(struct file *, struct kobject *, struct bin_attribute *attr,
                    struct vm_area_struct *vma);
};
```

struct attribute为普通的attribute，使用该attribute生成的sysfs文件，只能用字符串的形式读写。而struct bin_attribute在struct attribute的基础上，增加了read、write等函数，因此它所生成的sysfs文件可以用任何方式读写。

---

## 2018-09-03

Note that input core keeps track of number of users for the device and makes sure that dev->open() is called only when the first user connects to the device and that dev->close() is called when the very last user disconnects. Calls to both callbacks are serialized.

---

input子系统的abs设备，可以设置坐标最大值、最小值和一些其他参数:

```c
button_dev.absmin[ABS_X] = 0;
button_dev.absmax[ABS_X] = 255;
button_dev.absfuzz[ABS_X] = 4;
button_dev.absflat[ABS_X] = 8;
```

上述四行代码也等价于如下代码：

```c
input_set_abs_params(button_dev, ABS_X, 0, 255, 4, 8);
```

`absfuzz`为偏移噪声，`absflat`设置了一个坐标原点周围的区域，在此区域内的abs值将被视作0。这两个参数在为手柄编写驱动时较常见。

---

For input subsystem, the `input_dev::name` (`const char *`) should be set before registering the input device by the input device driver. It should be a user-friendly string, list "Generic button device". If it is available for x11, it would be listed in `xinput list`.

---

`input_dev::id` (`struct input_id`) contains the bus ID (PCI, USB, ...), vendor ID and device ID of the device. The vendor and device ids are defined in `pci_ids.h`, `usb_ids.h` and similar include files. These fields should be set by the input device driver before registering it.
The `idtype` field can be used for specific information for the input device driver.
The `id` and `name` fields can be passed to userland via the `evdev` interface

---

The `keycode`, `keycodemax`, `keycodesize` fields
These three fields should be used by input devices that have dense keymaps. The keycode is an array used to map from scancodes to input system keycodes. The keycode max should contain the size of the array and keycodesize the size of each entry in it (in bytes).
Userspace can query and alter current scancode to keycode mappings using EVIOCGKEYCODE and EVIOCSKEYCODE ioctls on corresponding evdev interface. When a device has all 3 aforementioned fields filled in, the driver may rely on kernel’s default implementation of setting and querying keycode mappings.

---

`getkeycode()` and `setkeycode()` callbacks allow drivers to override default `keycode`/`keycodesize`/`keycodemax` mapping mechanism provided by input core and implement sparse keycode maps.

---

Auto keyrepeat is handled by the `input.c` module. Hardware autorepeat is not used, because it’s not present in many devices and even where it is present, it is broken sometimes (at keyboards: Toshiba notebooks). To enable autorepeat for your device, just set `EV_REP` in `dev->evbit`. All will be handled by the input system.

---

There are two other event types:

+ EV_LED - used for the keyboard LEDs.
+ EV_SND - used for keyboard beeps.
  They are very similar to for example key events, but they go in the other direction - from the system to the input device driver. If your input device driver can handle these events, it has to set the respective bits in evbit, and also the callback routine:

```c
button_dev->event = button_event;

int button_event(struct input_dev *dev, unsigned int type,
                 unsigned int code, int value)
{
        if (type == EV_SND && code == SND_BELL) {
                outb(value, BUTTON_BELL);
                return 0;
        }
        return -1;
}
```

This callback routine can be called from an interrupt or a BH (although that isn’t a rule), and thus must not sleep, and must not take too long to finish.

---

## 2018-09-04

---

minicom -c on -C x50-log-$(date +%Y-%m-%d_%H:%M:%S).log
保存minicom串口记录

---

### Linux多点触控(1):

Linux多点触控协议(MT protocal)分为两种 - A协议和B协议。现在A协议已经废止，主要使用B协议。
A协议和B协议的区别在于，B协议可以独立跟踪每一个触控点，而A协议在上报触控点时不会区分触控点。
A协议实现多点触控的事件流程，此流程为实现一个A协议多点触控的示例事件流程:

```
    ABS_MT_POSITION_X x[0]
    ABS_MT_POSITION_Y y[0]
    SYN_MT_REPORT

    ABS_MT_POSITION_X x[1]
    ABS_MT_POSITION_Y y[1]
    SYN_MT_REPORT

    SYN_REPORT
```

每一个点均使用 `SYN_MT_REPORT`结尾，所有点报完后都使用 `SYN_REPORT`。如果驱动在报告多点触控的同时，还报告了 `BTN_TOUCH`或 `ABS_PRESSURE`，那么最后一个 `SYN_MT_REPORT`可以被省略。
由于Type A不独立跟踪每个触控点，因此0点和1点的先后顺序可以调换。
下面是一个B协议多点触控的最小事件流，与上例一样，此过程描述了一个两点触控:

```
ABS_MT_SLOT 0
ABS_MT_TRACKING_ID 45
ABS_MT_POSITION_X x[0]
ABS_MT_POSITION_Y y[0]
ABS_MT_SLOT 1
ABS_MT_TRACKING_ID 46
ABS_MT_POSITION_X x[1]
ABS_MT_POSITION_Y y[1]
SYN_REPORT
```

协议A为在每个触控点的事件流结尾加上 `SYN_MT_REPORT`, 而协议B在每个触控点事件流起始处加上 `ABS_MT_SLOT x`.
与协议A不同， 协议B采用增量更新的模式，对于不变的信息，不必再次上报。
因此，当只有一个点移动且其只在一个坐标轴上移动，也可以像下例一样，只报告此坐标轴的新值:

```
ABS_MT_SLOT 0
ABS_MT_POSITION_X x[0]
SYN_REPORT
```

当一个触控点失效后，可以报告如下事件流:

```
ABS_MT_SLOT 1
ABS_MT_TRACKING_ID -1
SYN_REPORT
```

在此事件流被处理后，原来与slot 1相连的contact被destroy; slot 1被free，可以和其他contact自由连接。
在一个事件流内可以对一个触控点做多个描述，这些描述将依序被执行。例如下列事件流是合法的:

```
ABS_MT_SLOT 0
ABS_MT_TRACKING_ID 45
ABS_MT_POSITION_X x[0]
ABS_MT_POSITION_Y y[0]
ABS_MT_TRACKING_ID -1
SYN_REPORT
```

此操作将tracking id 45 设置为(x[0],y[0])坐标, 然后移除它。

---

### Linux多点触控(2):

由上节可知，多点触控设备发出的部分事件与单点设备的不同，因此，若对一个只支持单点设备的应用程序发送多点信息，则多点触控独有的事件将被忽略，应用程序只能识别事件流中与单点共有的部分。(_Since these events are ignored by current single-touch (ST)
applications, the MT protocol can be implemented on top of the ST protocol
in an existing driver._)
在使用Linux多点触控协议时，还有其他的事件可以使用: [Event Semantics](https://www.kernel.org/doc/html/latest/input/multi-touch-protocol.html#event-usage), 其中的大部分事件可以使用 `input_report_abs()`来生成。

---

### Linux多点触控(3):

使用输入子系统的API来产生上述事件:
协议A示例:

```c
/*
*report the point information
*/
static void ftxxxx_report_value(struct ftxxxx_ts_data *data)
{
	struct ts_event *event = &data->event;
	int i;
	static u8 uppoint=0;//抬起点数
	//protocol A
	for(i=0;i < event->touch_point;i++)//因为是多点触控，所以要循环内处理所有点
	{
		if((event->au8_touch_event[i] == 0)||(event->au8_touch_event[i] ==2))//手指按下
		{
			input_report_key(data->input_dev, BTN_TOUCH, 1);//发送按下BTN_TOUCH,massage，表示有手指按下 
			input_report_abs(data->input_dev, ABS_MT_PRESSURE, event->pressure[i]);//压力pressure信息
			input_report_abs(data->input_dev, ABS_MT_TOUCH_MAJOR, event->area[i]);//区域
			input_report_abs(data->input_dev, ABS_MT_POSITION_X, event->au16_x[i]);//坐标
			input_report_abs(data->input_dev, ABS_MT_POSITION_Y, event->au16_y[i]);
			input_mt_sync(data->input_dev);
		}
		else
		{
			uppoint++;//否则没有按下，也就是手指已经抬起，累计抬起点数
			input_mt_sync(data->input_dev);//产生SYN_MT_REPORT,对数据包进行分割
		}
	}
	if(uppoint==event->touch_point)//点处理完后，如果累计的抬起点等于按下点数
	{
		input_report_key(data->input_dev,BTN_TOUCH,0);//所有手指已经抬起，发送BTN_TOUCH抬起事件
	}
	else
		input_report_key(data->input_dev,BTN_TOUCH,event->touch_point>0);//还有手指没有抬起，发送BTN_TOUCH按下事件

	input_sync(data->input_dev);//产生SYN_REPORT事件，标志一次触控结束
}
```

协议B示例:

```c
static void ftxxxx_report_value(struct ftxxxx_ts_data *data)
{
	struct ts_event *event = &data->event;
	int i;
	static u8 uppoint=0;    //the number of points that had left screen
	//protocol B
	for(i=0;i < event->touch_point;i++)   //all the points in memory
	{
		input_mt_slot(data->input_dev,event->au8_finger_id[i]);
		//input_report_abs(data->input_dev, ABS_MT_TRACKING_ID, event->au8_finger_id[i]);

		if((event->au8_touch_event[i] == 0)||(event->au8_touch_event[i] ==2))//if the point pressed
		{
		         //input_mt_slot(data->input_dev,event->au8_finger_id[i]);
				 
		        input_mt_report_slot_state(data->input_dev,MT_TOOL_FINGER,true); //report slot active
			input_report_key(data->input_dev, BTN_TOUCH, 1);                               //press down           
			//input_report_abs(data->input_dev, ABS_MT_TRACKING_ID, event->au8_finger_id[i]);

			input_report_abs(data->input_dev, ABS_MT_PRESSURE, event->pressure[i]);
			input_report_abs(data->input_dev, ABS_MT_TOUCH_MAJOR, event->area[i]);
			input_report_abs(data->input_dev, ABS_MT_POSITION_X, event->au16_x[i]);
			input_report_abs(data->input_dev, ABS_MT_POSITION_Y, event->au16_y[i]);
			//input_mt_sync(data->input_dev);这是protocol A用来分割数据包的，在protocol B中并不需要，有slot万事足
		}
		else
		{
			uppoint++;
			input_mt_report_slot_state(data->input_dev,MT_TOOL_FINGER,false);  //report finger had uplift
		}
	}
	if(uppoint==event->touch_point)
	{
		input_report_key(data->input_dev,BTN_TOUCH,0);//all finger has uplift,send BTN_TOUCH up message
	}
	else
		input_report_key(data->input_dev,BTN_TOUCH,event->touch_point>0);//still exsist fingers have not uplift,send BTN_TOUCH down message

	input_sync(data->input_dev);//indicate end of one touch
}
```

上述示例来自[修改触控A协议为B协议，完成触控事件上报 - 博客园](http://www.voidcn.com/article/p-ahdwwzoy-ben.html)。
关于tracking id: 使用 `input_mt_report_slot_state(data->input_dev,MT_TOOL_FINGER,true)`可以让内核自动设置和关联tracking-id，即内核自动报告了 `ABS_MT_TRACKING_ID x`事件; 使用 `input_mt_report_slot_state(data->input_dev,MT_TOOL_FINGER,false)`则相当于 `ABS_MT_TRACKING_ID -1`。如果硬件设备会自行跟踪tracking-id, 则可以自行使用 `input_report_abs(data->input_dev, ABS_MT_TRACKING_ID, event->au8_finger_id[i])`来制定相应的tracking-id。

---

## 2018-09-06

关于 `spin_lock()`和 `spin_lock_irq()`:
在任何情况下使用 `spin_lock_irq`都是安全的。因为它既禁止本地中断，又禁止内核抢占。
`spin_lock`比 `spin_lock_irq`速度快，但是它并不是任何情况下都是安全的。
举个例子:进程A中调用了 `spin_lock（&lock）`然后进入临界区，此时来了一个中断(interrupt），该中断也运行在和进程A相同的CPU上，并且在该中断处理程序中恰巧也会 `spin_lock(&lock)`试图获取同一个锁。由于是在同一个CPU上被中断，进程A会被设置为 `TASK_INTERRUPT`状态，中断处理程序无法获得锁，会不停的忙等，由于进程A被设置为中断状态，`schedule()`进程调度就无法再调度进程A运行，这样就导致了死锁！
但是如果该中断处理程序运行在不同的CPU上就不会触发死锁。 因为在不同的CPU上出现中断不会导致进程A的状态被设为 `TASK_INTERRUPT`，只是换出。当中断处理程序忙等被换出后，进程A还是有机会获得CPU，执行并退出临界区。
所以在使用 `spin_lock`时要明确知道该锁不会在中断处理程序中使用。

---

## 2018-09-18

自2.6.39开始, i2c子系统的结构发生了变化.

---

## 2018-10-11

关于设备树, 可以查看: [Linux设备树语法详解](https://www.cnblogs.com/xiaojiang1025/p/6131381.html).

---

## 2018-10-12

Linux设备体系中, 每一个设备均对应一个 `struct device`. 不过其一般不被单独使用, 而是被包含在一些类似于 `platform_device` 等等的结构体中使用.

---

## 2018-10-29

在Android源码中添加驱动需要修改相应的Kconfig和Makefile和 defconfig. (比如 `kernel/设备名/drivers/net/wireless/Kconfig`和同一路径下的Makefile 以及 `kernel/设备名/arch/架构名/configs/相应架构设备_defconfig`).

---

## 2018-11-02

[Git修改前一次提交的方法(特别注意保持Change-Id不变)](https://blog.csdn.net/jfkidear/article/details/17795781)

## 2019-01-13

`fork`在父线程中的返回值是子线程的ID, 在子线程中的返回值是0.

File descriptor等在Linux中采用引用计数制, 多个进程/线程可以拥有一个descriptor的多个副本.只有当最后一个descriptor被close后, 相应的资源才会被销毁. 因此在fork/vfork/close时应注意这点.

通常, 在进程执行 `exec*` 之后, file descriptors 仍然有效, 除非在 `exec*` 前使用 `fcntl` 设置 `FD_CLOEXEC`, 或者在 `open` 时设定 `O_CLOEXEC`.

---

## 2019-01-16

next_permutation算法:
    1. 若数组为逆序, 直接reverse
    2. 若数组不为逆序, 则先找到最大的 `i` , 使得 `arr[i] < arr[i + 1]`. 再找到 `j`, `j`满足 `j ∈ [i + 1, n - 1), arr[j] = min{arr[i + 1], arr[i + 2], arr[i + 3], ..., arr[n]}`. 由于 `arr[i + 1], ..., arr[n - 1]` 一定逆序, 因此从后向前迭代x, 到第一个 `arr[x] > arr[i]`时, 此时的 `j` 一定就是所需的 `j`. 然后 `swap(arr[i], arr[j])` , 再 `reverse(arr + i, arr + n)`即可.

---

## 2019-01-17

```c
// From sys/wait.h

/* Blocks until the first child process terminates, returns its pid and passes back its termination status. 
 *
 * Parameter statloc indicates the position to store the termination status of the child process.
 * Here are some important macros in this value for Linux:
 * WIFEXITED(*statloc) returns a non zero value if this process terminates normally.
 * WEXITSTATUS(*statloc) returns the exiting code if the child process returns with function exit. This is applicable only if WIFEXITED(*statloc) is not false (0).
 * WIFSIGNALED(*statloc) returns a non-zero value if it is killed for not capturing a signal.
 * WTERMSIG(*statloc) returns the signal that killed this process. Only applicable if WIFSIGNALED(*statloc) is not false (0).
 * WIFSTOPPED(*statloc) returns a non-zero value if this process is stopped.
 * WSTOPSIG(*statloc) returns the signal that stops this process if WIFSTOPPED(*statloc) is not false (0).
 *
 * Returns -1 or 0 on error.
 */
pid_t wait(int *statloc); 

/* Blocks until the specified child proess terminates. 
 * 
 * If the argument pid is set to -1, it waits until the first child process terminates;
 * 0 on Linux means any process in the same process group of this process;
 * any positive value means the specific child process's pid;
 * any value less than -1 on Linux means its opposite value.
 *
 * Parameter statloc is of the same usage as the one in wait. 
 * 
 * Parameter options can is usually set to 0. Here are two important alternatives for this argument:
 * WNOHANG: Immediately returns 0 if the specified process(es) are not terminating or terminated.
 * WUNTRACED: Immediately returns if the child process(es) are stopped.
 */

pid_t waitpid(pid_t pid, int *statloc, int options);
```

多线程环境, 发送给当前进程的信号可能被发送给其中的任一线程. 线程可以单独屏蔽信号处理, 当随机挑选的线程不能处理当前信号时, 系统将重新将信号发送给当前进程中能够处理此信号的线程中的线程号最小的那个.

在某些实现中, `SIGCHLD` 并不会被queue起来. 如果在一个 `SIGCHLD` 的被sigaction处理时, 另一个child死亡, 那么这个child的 `SIGCHLD` 可能会被丢失, 导致它成为zombie process. 因此在多进程处理过程中, 可以在 `SIGCHLD` 的 handler 内用如下代码来处理子进程的死亡问题:

```c
void sigchld_handler(int signo) {
	int statloc;
	pid_t pid;
	while((pid = waitpid(-1, &stat, WNOHANG)) > 0) {
		// Store the pid, then indicates that this pid is dead.
	}
	return;
}
```

---

## 2019-01-19

When a Unix system is shutting down, it sends `SIGTERM` to all processes, and wait for 5 to 20 seconds, and then sends `SIGKILL` to them.

---

## 2019-04-08

### `kmalloc`, `kzalloc` and `vmalloc`

Referenced: [Linux内核空间内存申请函数kmalloc、kzalloc、vmalloc的区别【转】](https://www.cnblogs.com/sky-heaven/p/7390370.html)

#### `kmalloc`:

```c
void* kmalloc(size_t size, gfp_t flags);
```

申请的内存位于物理内存映射区域，而且在物理上也是连续的。其地址相对于物理地址只有一个固定的偏移。由于其转换简单，因此一次申请的量不能大于128KB。

常用的flags:

| Flag                         |                                                                                                                                                                                                                                                                    介绍 |
| :--------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |
| `GFP_ATOMIC`               |                                                                                                                                                                                                                  分配过程是原子过程，不会被（高优先级进程或中断）打断。 |
| `GFP_KERNEL`               |                                                                                                                                                                                                                                                            正常分配内存 |
| `GFP_DMA`                  |                                                                                                                                                                                                                给DMA控制器分配内存（DMA要求分配虚拟地址和物理地址连续） |
| `GFP_USER`                 |                                                                                                                                                                                                                                 用来为用户空间页来分配内存; 它可能睡眠. |
| `GFP_HIGHUSER`             |                                                                                                                                                                                                如同 `GFP_USER`, 但是从高端内存分配, 如果有. 高端内存在下一个子节描述. |
| `GFP_NOIO` 和 `GFP_NOFS` | 这个标志功能如同 `GFP_KERNEL`, 但是它们增加限制到内核能做的来满足请求. 一个 `GFP_NOFS` 分配不允许进行任何文件系统调用, 而 `GFP_NOIO` 根本不允许任何 I/O 初始化. 它们主要地用在文件系统和虚拟内存代码, 那里允许一个分配睡眠, 但是递归的文件系统调用会是一个坏注意. |

flags的常用组合：

| 组合           |                                                        用途 |
| :------------- | ----------------------------------------------------------: |
| `GFP_KERNEL` |                                        进程上下文，可以睡眠 |
| `GFP_ATOMIC` | 进程上下文，不可以睡眠，可用于中断和软中断处理程序和Tasklet |
| `GFP_DMA       |                                                 GFP_KERNEL` |
| `GFP_DMA       |                                                 GFP_ATOMIC` |

对应的释放函数:

```c
void kfree(const void *objp);
```

#### `kzalloc`:

与 `kmalloc`类似，只是申请后会自动将数据清零。

```c
/** 
  * kzalloc - allocate memory. The memory is set to zero. * @size: how many bytes of memory are required. * @flags: the type of memory to allocate (see kmalloc). 
  */
static inline void *kzalloc(size_t size, gfp_t flags){
	    return kmalloc(size, flags | __GFP_ZERO);
}
```

其对应的释放函数也是 `kfree`。

#### `vmalloc`:

```c
void *vmalloc(unsigned long size);
```

`vmalloc` 函数则会在虚拟内存空间给出一块连续的内存区，但这片连续的虚拟内存在物理内存中并不一定连续。由于 `vmalloc` 没有保证申请到的是连续的物理内存，因此对申请的内存大小没有限制，如果需要申请较大的内存空间就需要用此函数了。

对应的内存释放函数为：

```c
void vfree(const void *addr);
```

注意：`vmalloc` 和 `vfree` 可以睡眠，因此不能从中断上下文调用。

`kmalloc` 分配内存的开销小，因此 `kmalloc` 比 `vmalloc` 要快。

一般情况下，内存只有在要被 DMA 访问的时候才需要物理上连续，但为了性能上的考虑，内核中一般使用 `kmalloc` ，而只有在需要获得大块内存时才使用 `vmalloc` 。例如，当模块被动态加载到内核当中时，就把模块装载到由 `vmalloc` 分配的内存上。

---

# 2019-05-06

使用fio写满整盘:
fio -filename=/dev/device -direct=1 -rw=write -bs=128k -numjobs=1 -ioengine=psync -group_reporting -iodepth=128 -name=mytest

---

# 2019-07-22

CPU使用率测试工具: perf
可以使用 `perf top`来观察占用cpu较高的symbol和pid:

```sh
perf top --sort symbol,pid,comm
```

iowait时间指CPU在等待io期间无事可做的时间。在早期linux版本中，它其实是idle的一部分。

---

# 2020-09-14

Kernel 宏 `READ_ONCE()`、`WRITE_ONCE()`和 `ACCESS_ONCE()`用于保证变量在访问时只被/必须被读取/写入一次，以阻止编译器优化。它们的实现原理是，将目标变量临时转型成volatile型。
但其中，某些版本的GCC存在bug,可能使得 `ACCESS_ONCE()`在non-scaler types (即非基础类型)上失效。因此，建议使用 `READ_ONCE()`和 `WRITE_ONCE()`。
参考[ACCESS_ONCE()](https://lwn.net/Articles/508991/)和[ACCESS_ONCE() and compiler bugs](https://lwn.net/Articles/624126/)。

---

# 2021-11-29

Docker run 选项 --shm-size: /tmp最大大小

# 2022-09-19

## 基础概念

### 读请求:

#### 1.

SYSCALL_DEFINE3(read, .....); (fs/read_write.c)

SYSCALL_DEFINE3(read. ...) -> ksys_read()

ksys_read(): 获取fd, 管理f_pos。

ksys_read() -> vfs_read()

vfs_read(): 权限检查, 参数合法性检查

vfs_read() 调用 file->f_op->read()或new_sync_read(), fsnotify_access(), 更新current->ioac (struct task_io_accounting)。

#### 2.

以ext2为例, ext2_file_operations::read 为空, 但是存在 .read_iter, 因此调用new_sync_read()。

new_sync_read(): 初始化 iov (struct iovec) 和 kiocb (struct kiocb) 和 iter (struct iov_iter)。

随后调用 call_read_iter() , 并在其返回后更新ppos。

#### 3.

call_read_iter() 调用 file->f_op->read_iter, 在ext2中即 ext2_file_read_iter()

#### 4.

ext2_file_read_iter() 调用 generic_file_read_iter()。

#### 5.

generic_file_read_iter() 在不考虑 Direct IO 的情况下, 将调用filemap_read(), 其参数already_read将设置为0。

#### 6.

struct xarray、struct xa_node等, 是新的基础设施XARRAY的数据结构。XARRAY被用来替代旧有的radix tree。

#### 7.

struct folio 用于表示一组连续的内存, 且内存的大小为2的幂。struct folio内只有一个union。如果struct folio指代
一段连续的内存page, 则struct folio 就是一个struct page的包装。

目前struct adress_space内的radix tree和struct page都已经被替换为了XArray和struct folio。

struct folio_batch就是此前的struct pagevec的平替。

#### 8.

mapping_writeably_mapped() 返回 一个address_space的i_mmap_writable是否大于0。如果其大于0, 就代表有进程shared map
了这个page cache, 并且这个map是writable的。

#### 9.

https://zhuanlan.zhihu.com/p/91338338 :

问：Linux的中断可以嵌套吗？

答：以前是可以嵌套的，现在不可以！

历史
早前的Linux内核版本，中断分为两种：

快中断，申请的时候带IRQF_DISABLED标记，在IRQ HANDLER里面不允许新的中断进来;

慢中断，申请的时候不带IRQF_DISABLED标记，在IRQ HANDLER里面允许新的其他中断嵌套进来。

老的Linux内核中，如果一个中断服务程序不想被别的中断打断，我们能看到这样的代码：

request_irq(FLOPPY_IRQ, floppy_interrupt,\

- IRQF_DISABLED, "floppy", NULL)
  现在
  在2010年如下的commit中，IRQF_DISABLED被作废了：

https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=e58aa3d2d0cc

#### 10.

Linux 中定义, intterupt为内部定时器或IO设备所发出的、引起执行的信号, 这类信号通常引起执行流跳转至ISR;
 exception是指由程序错误引发的信号, 例如除0错误、缺页等, 对这些问题, kernel可能会向目标进程发送信号、执行异常对应的处理代码等。

中断分为maskable interrupts 和 nonmaskable interrupts (NMI)

异常分为
    Faults: 发生时栈上所保存的ip是引起此fault的指令。也就是说在异常处理完成后, 可以重新执行问题指令。Fault的代表是page fault。
    Traps: 发生时栈上所保存的ip是引起此trap的指令的下一条指令。典型的应用场景是debug程序时的trace功能。另外, 早期的x86系统调用也以trap的形式存在。用户态程序执行int指令陷入系统调用。
    Aborts: 发生时相应的control unit不能再继续执行, 因此其所保存的ip是不可执行的。此时内核将终止此control unit。

#### 11.

Oops: Kernel在出错时所打印的寄存器状态、调用栈、堆栈信息等。
Panic: 内核崩溃, 指内核遇到严重错误必须停止内核自身的运行。

#### 12.

[Linux Workqueue 机制分析 - 博客 - binsite (binss.me)](https://www.binss.me/blog/analysis-of-linux-workqueue/)

> 定义

workqueue 是自 kernel 2.6 引入的一种任务执行机制，和 softirq 、 tasklet 并称下半部 (bottom half) 三剑客。比起这两者，workqueue 在进程上下文异步执行任务，能够进行睡眠，很快就受到内核开发者们的追捧。

workqueue 最核心的思想是分离了任务 (work) 的发布者和执行者。当需要执行任务的时候，构造一个 work，塞进相应的 workqueue，由 workqueue 所绑定的 worker (thread) 去执行。如果任务不重要，我们可以让多种任务共享一个 worker，而无需每种任务都开一个 thread 去处理(n -> 1)；相反如果任务很多很重要，那么我们可以开多个 worker 加速处理(1 -> n)，类似于生产者消费者模型。

在传统的实现中，workqueue 和 worker 的对应关系是二元化的：要么使用 Multi Threaded (MT) workqueue 每个 CPU 一个 worker，要么使用 Single Threaded (ST) workqueue 整个系统只有一个 worker。但即使是通过 MT 每个 CPU 都开一个 worker ，它们相互之间是独立的，在哪个 CPU 上挂入的 work 就只能被那个 CPU 上的 worker 处理，这样当某个 worker 因为处理 work 而阻塞时，位于其他 CPU 上的 worker 只能干着急，这并不是我们所期待的并行。更麻烦的是，它很容易导致死锁，比如有 A 和 B 两个 work，B 依赖于 A 的执行结果，而此时如果 A 和 B 被安排由同一个 worker 来做，而 B 恰好在 A 前面，于是形成死锁。

为了解决这个问题，Tejun Heo 在 2009 年提出了 CMWQ(Concurrency Managed Workqueue) ，于 2.6.36 进入 kernel 。

>  CMWQ

相比传统实现，CMWQ 做了一个很重要的改进就是 workqueue 和 worker 进行解耦，提出了 worker pool 的概念。worker 的创建不再与 workqueue 相关，而是由 worker pool 统一管理。不同 workqueue 共享全局的 worker pool，但 workqueue 可以根据需要 (flags) 选择使用特定的 worker pool 。

整理一下 workqueue 中出现的角色：

* work ：工作。被加到 workqueue 中
* workqueue ：工作队列。逻辑上是 work 排成的队列
* worker：工人。本质上是内核线程(kthread)，负责真正执行 work
* worker pool：worker 的集合。负责管理 worker。

#### worker pool

按照运行特性，主要 CPU bound 和 unbound 分为两类。

##### CPU bound worker pool

绑定特定 CPU，其管理的 worker 都运行在该 CPU 上。

根据优先级分为 normal pool 和 high priority pool，后者管理高优先级的 worker。

Linux 会为每个 online CPU 都创建 1 个 normal pool 和 1 个 high priority pool，并在命名上进行区分。

比如 `[kworker/1:1]` 表示 CPU 1 上 normal pool 的第 1 个 worker ，而 `[kworker/2:0H]` 表示 CPU 2 上 high priority pool 的第 0 个 worker

##### unbound

其管理的 worker 可以运行在任意的 CPU 上。

比如 `[kworker/u32:2]` 表示 unbound pool 32 的第 2 个 worker 进程

#### 13.

[任务工厂 - Linux中的workqueue机制 [一] - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/91106844) :

work的定义:

```c
struct work_struct {
	struct list_head entry;
	work_func_t func;
	atomic_long_t data;
};
```

其中， **"entry"** 表示其所挂载的workqueue队列的节点， **"func"** 就是要执行的任务的入口函数。而 **"data"** 表示的意义就比较丰富了：最后的4个bits是作为"flags"标志位使用的，中间的4个bits是用于flush功能的"color"（flush的功能是在销毁workqueue队列之前，等待workqueue队列上的任务都处理完成）。

![1663575718028](image/杂项/1663575718028.png)

剩下的bits在不同的场景下有不同的含义(相当于C语言里的"union")，它可以指向work item所在的workqueue队列的地址，由于低8位被挪作他用，因此要求workqueue队列的地址是按照256字节对齐的。它还可以表示处理work item的worker线程所在的pool的ID(关于pool将在本文的后半部分介绍)。

为了充分利用locality，通常选择将处理hardirq的CPU作为该hardirq对应的workqueue底半部的执行CPU，
在早期Linux的实现中，每个CPU对应一个workqueue队列，并且每个CPU上只有一个worker线程来处理这个workqueue队列，
也就是说workqueue队列和worker线程都是per-CPU的，且一一对应。

![1663575782195](image/杂项/1663575782195.png)

当然, 这样会造成, 如果多个work被queue进一个workqueue中, 后进入的work必须等到先进入的work先结束执行, 才能被处理。因此, 后续的linux版本中, 在一个cpu core上放置多个了worker线程。

> cmwq

这种在一个CPU上运行多个worker线程的做法，就是 v2.6.36 版本引入的，也是现在Linux内核所采用的concurrency managed workqueue，简称cmwq。一个CPU上是不可能“同时”运行多个线程的，所以这里的名称是concurrency(并发)，而不是parallelism(并行)。

显然，设置合适的worker线程数目是很关键的，多了浪费资源，少了又不能充分利用CPU。大体的原则就是：如果现在一个CPU上的所有worker线程都进入了睡眠状态，但workqueue队列上还有未处理的work item，那么就再启动一个worker线程。

一个CPU上同一优先级的所有worker线程（优先级的概念见下文）共同构成了一个**worker pool**(此概念由内核v3.8引入)，我们可能比较熟悉memory pool，当需要内存时，就从空余的memory pool中去获取，同样地，当workqueue上有work item待处理时，我们就从worker pool里挑选一个空闲的worker线程来服务这个work item。

worker pool在代码中由 `worker_pool` 结构体表示(定义在kernel/workqueue.c)：

```c
struct worker_pool {
	spinlock_t		lock;		/* the pool lock */
	int			cpu;		/* the associated cpu */
	int			id;		/* pool ID */

	struct list_head	idle_list;	/* list of idle workers */
	DECLARE_HASHTABLE(busy_hash, 6);        /* hash of busy workers */
    ...
}
```

如果一个worker正在处理work item，那么它就是busy的状态，将挂载在busy workers组成的6阶的hash表上。既然是hash表，那么就需要key，充当这个key的是正在被处理的work item的内存地址。

如果一个worker没有处理work item，那么它就是idle的状态，将挂载在idle workers组成的链表上。

前面说过，有未处理的work item，内核就会启动一个新的worker线程，以提高效率。有创建就有消亡，当现在空闲的worker线程过多的时候，就需要销毁一部分worker线程，以节省CPU资源。就像一家公司，在项目紧张，人员不足的时候需要招人，在项目不足，人员过剩的时候可能就会裁员。

这种 **动态的资源伸缩** ，就有点“云”的味道了，即根据负载的变化，动态地扩容和缩容，目标是用尽可能少的资源，来完成尽可能多的事。至于保留多少空闲线程可以取得较理想的平衡，则涉及到一个颇为复杂的算法，在此就不展开了。

![1663576154111](image/杂项/1663576154111.png)

worker线程在代码中由"`worker`"结构体表示(定义在kernel/workqueue_internal.h)：

```c
struct worker {
	struct worker_pool	 *pool;		/* the associated pool */
	union {
		struct list_head  entry;	/* while idle */
		struct hlist_node hentry;	/* while busy */
	};

	struct work_struct	*current_work;	  /* work being processed */
	work_func_t		 current_func;	  /* current_work's fn */
	struct task_struct	*task;		  /* worker task */

	struct pool_workqueue	*current_pwq;     /* current_work's pwq */
        ...
}
```

其中，"`pool`"是这个worker线程所在的worker pool，根据worker线程所处的状态，它要么在idle worker组成的空闲链表中，要么在busy worker组成的hash表中。

"current_work"和"current_func"分别是worker线程正在处理的work item和其对应的入口函数。既然worker线程是一个内核线程，那么不管它是idle，还是busy的，都会对应一个task_struct(由"task"表示)。

#### 14.

[任务工厂 - Linux中的workqueue机制 [二] - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/94561631) :

在多个worker线程的cmwq模式下，按理一个CPU依然只对应一个workqueue队列，该队列由该CPU的worker pool里的线程共同服务(共享)，但别忘了任务是有优先级的，虽不说像完整的系统那样将优先级划分地很细，至少要分成低优先级和高优先级两类吧。为此，目前的设计是一个CPU对应两个workqueue队列，相应地也有两个worker pool，分别服务于这个2个队列。

用"ps"命令来看下系统中都有哪些worker线程，worker线程被命名成了"kworker/n:x"的格式，其中n是worker线程所在的CPU的编号，x是其在worker pool中的编号，如果带了"H"后缀，说明这是高优先级的worker pool。

![1663579143258](image/杂项/1663579143258.png)

还有一些带"u"前缀的，它表示"unbound"，意思是这个worker线程不和任何的CPU绑定，而是被所有CPU共享，这种设计主要是为了增加灵活性。"u"后面的这个数字也不再表示CPU的编号，而是表示由这些unbound的worker线程组成的worker pool的ID号。

假设一个系统有4个CPU，那么它就对应至多8个"bound"的worker pool(4个普通优先级+4个高优先级)和8个workqueue队列，每个worker pool里有若干个worker线程，每个workqueue队列上有若干个work items。至于"unbound"的worker pool数目，则是由具体的需求决定的。

![1663587454233](image/杂项/1663587454233.png)

上图的"pwq"就是表示workqueue队列的" *pool_workqueue * "结构体(定义在kernel/workqueue.c)。这里有个对齐的要求，原因就是上文介绍的"work_struct->data"的低8位被占用的问题。

```c
struct pool_workqueue {
	int	nr_active;	/* nr of active works */
	int	max_active;	/* max active works */

	struct  worker_pool	   *pool;   /* the associated pool */
	struct  workqueue_struct   *wq;	    /* the owning workqueue */
        ..
}__aligned(1 << WORK_STRUCT_FLAG_BITS);
```

* "max_active"和"nr_active"分别是该workqueue队列最大允许和实际挂载的work item的数目。最大允许的work item数目也就决定了该workqueue队列所对应的work pool上最多可能的活跃(busy)的worker线程的数目。
* "pool"指向服务这个workqueue队列的worker pool，至于这个"wq"，它的数据类型是"workqueue_struct"，从名字上看这个"workqueue_struct"好像也是表示workqueue队列的，那它和pwd(pool_workqueue)有什么区别呢？

来看下" *workqueue_struct * "结构体的定义(代码位于kernel/workqueue.c)：

```c
struct workqueue_struct {
	struct list_head	list;		                   /* list of all workqueues */
	struct list_head	pwqs;		           /* all pwqs of this wq */
	struct pool_workqueue  *cpu_pwqs;         /* per-cpu pwqs */
        ...
}
```

"list"是workqueue_struct自身串接而成的链表，以方便内核管理。

![1663588544811](image/杂项/1663588544811.png)

"pwqs"是同种类型的pwq组成的链表。

![1663588569893](image/杂项/1663588569893.png)

> **初始化**

至此，有关workqueue机制的5个结构体以及它们之间的相互关系就介绍完了，如果做个类比的话，那么work item就是工件，pwq队列就是这些工件组成的流水线，worker线程就是工人，worker pool就是一个班组的工人构成的集合。

假设一个集团有4个工厂(4个CPU)，每个工厂都分了两条流水线，一条是由高级工构成的班组负责的高效流水线，一条是由初级工构成的班组负责的普通流水线，那么这4个工厂的高效流水线都属于同一个技术级别(workqueue_struct)，普通流水线则同属于另一个技术级别。

至于unbound的生产班组，就理解为外包吧，它们从组织关系上不属于任何一个工厂，但是可以综合这4个工厂的生产任务的波动，提供对人力资源更机动灵活的配置。

来看一下workqueue机制所需的这一套东西都是怎么创建出来的。负责创建的函数主要是workqueue_init_early()，它的调用关系大致如下：

```go
workqueue_init_early() 
    --> alloc_workqueue() 
        --> alloc_and_link_pwqs()
             --> init_pwq()
```

在workqueue_init_early()中，初始化了per-CPU的worker pool，并为这些worker pool指定了ID和关联了CPU，工厂是工人劳动的场所，这一步相当于是把工厂的基础设施建好了。

```go
int __init workqueue_init_early(void)
{
	// 创建worker pool 
	for_each_possible_cpu(cpu) {
		struct worker_pool *pool;
		for_each_cpu_worker_pool(pool, cpu) {
                        init_worker_pool(pool);
			pool->cpu = cpu;
                        worker_pool_assign_id(pool);
	...
        }

        // 创建workqueue队列
	system_wq = alloc_workqueue("events", 0, 0);
	system_highpri_wq = alloc_workqueue("events_highpri", WQ_HIGHPRI, 0);
	system_unbound_wq = alloc_workqueue("events_unbound", WQ_UNBOUND, WQ_UNBOUND_MAX_ACTIVE);
        ...				  
}
```

接下来就是调用alloc_workqueue()创建各种类型的workqueue_struct，基础的是三种：普通优先级的，高优先级的，以及不和CPU绑定的，这一步相当于是把流水线的分类，以及采用自产还是外包的形式确定了。

在alloc_workqueue()中，第3个参数指定了可以并发的worker线程的数目(工厂流水线的容量)，最大为512(WQ_MAX_ACTIVE)，设为0则表示使用默认值(WQ_MAX_ACTIVE/2=256)。至于为什么是512，仅仅是一种经验的估算而已：

![1663588666224](image/杂项/1663588666224.png)

在alloc_and_link_pwqs()中，才是创建per-CPU的pwq队列，并把pwq队列和对应的workqueue_struct关联起来。这一步相当于是按之前确定的分类，依次建好了4个工厂对应的流水线。

```go
int alloc_and_link_pwqs(struct workqueue_struct *wq)
{
	bool highpri = wq->flags & WQ_HIGHPRI;

	if (!(wq->flags & WQ_UNBOUND)) {
		wq->cpu_pwqs = alloc_percpu(struct pool_workqueue);
		for_each_possible_cpu(cpu) {
			init_pwq(pwq, wq, &cpu_pools[highpri]);
	...
}
```

然后就是各个工厂根据市场上来的订单(产生的work items)确立生产任务的要求，并根据订单生产所需的技能级别，招募高级工或者初级工，安排到对应的流水线上。在这个过程中，还需要根据订单量的变化，动态地调整工人的数目，以实现效益的最大化(CPU资源最充分的利用)。
