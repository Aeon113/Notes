影响:
云盘io抖动2s左右

发布版本（可选）：
chunk-22.04.12-8666e1b-centos7-x86_64-haswell
chunk-22.04.12-8666e1b-centos7-x86_64-skylake

发布范围：

hd04 3004 3006 3007 3008 3009 3201 3202 3203 4000
hd03 3205
hn02 3200 3201 3202 3203 5004 5005 5006 5007
ap02 1200 3200 3203
ap03 12000 3000 3001 3003 3005 4000 4001 4003 4005 4006
taibei02 1000 3002 3003 3004 3005 3006 4000 4001 4002 5006 5007 5008 5009 5011 5012 5014

配置变更：


变更步骤：
自动化变更：
进入要变更发布可用区的跳板机，进入/root/limax/udisk/tool/release/chunk_auto_release目录，参考readme.md文件.
cd /root/limax/udisk/tool/release/chunk_auto_release && /usr/bin/python chunk_auto_release.py


变更验证：
变更前记录云盘io，变更后检查云盘io是否恢复


回滚步骤：
  优先回退脚本：
python roolback_chunk.py hd04 3004 [chunk_roolback_uuid] [optional: conf_roolback_uuid]


上述回退失败后紧急回退：
pssh -l root -h /root/limax/udisk/region/hd04/3004/host_info -i "ln -sf /root/udisk/chunk/chunk-22.03.18-5e7f615-centos7-x86_64-haswell /root/udisk/chunk/chunk"
pssh -h /root/limax/udisk/region/hd04/3004/host_info -i "ps aux | grep chunk/conf | grep -v grep | awk '{print \$2}' | grep -v $$ | xargs kill -9"


回滚验证：
验证io是否恢复

-------------------
UDiskHandle::OpenPCResCb()
DiskIOCb的作用: 向上层返回io结果, 包括磁盘错误和重定向错误: UDiskHandle::LocalAioCb
PCHandle::Redirect() -> RawChunkStorage::OpenParentPCByBit()
RawChunkPool::UpdatePC()




----------------


<lc 5, pc 0> -> <lc 4, pc 0>


---------
重定向错误: retcode -1, redirect_errcode: UDISK_INCORRECT_REDIRECTION_ERROR
磁盘错误: retcode <- 下层

特殊的retcode, 

-----------
重定向第二阶段逻辑:

写入:
锁: 1. 同PC的重定向写入互斥 2. 保证bitmap成功落盘前对同一个256k不会发生其它写入
cache 为0 -> 直接写本PC -> callback
cache 为1 -> 加锁成功 -> bitmap 为1 -> rmw ->内存里PCMeta bitmap 修改为0 -> PCMeta bitmap落盘 -> 解锁-> PCHandle cache修改
#不可行: 可选: cache 为1 -> 加锁成功 -> bitmap 为1 -> rmw ->内存里PCMeta bitmap 修改为0 -> 解锁 -> PCMeta bitmap落盘 -> 加锁 -> PCHandle cache修改 -> 解锁
cache 为1 -> 加锁成功 -> bitmap 为0 -> 更新cache为0 -> 解锁 -> 数据落盘到当前PCHandle
cache 为1 -> 加锁失败 -> 将当前写入任务放入pending队列

读取:
cache 为0 -> 直接读本PC 
cache 为1 -> bitmap为0 -> 加锁成功 -> 更新cache为0 -> 解锁 -> 读当前PCHandle数据
cache 为1 -> bitmap为0 -> 加锁失败 -> 不更新cache, cache还是1 -> 从当前PCHandle读数据
cache 为1 -> bitmap为1 -> 找对应的parent -> 从parent读


----------

RawChunkStorage -> PCHandle ->RWriteContext

1. IO完成后的重调度
2. 定时器重调度


----
不再每次extract pc handle的时候把它remove掉,改成:

1. 如果pc handle里为空，才remove
2. 如果不为空，先remove，再添加到尾部。
这样，如果dispatch之后，pc handle变为空，会在下次定时器dispatch时被remove掉。


-------------------

{ "_id" : ObjectId("623bfb591218754902ad2743"), "chunk_0_id" : 126, "chunk_1_id" : 118, "primary_chunk_id" : 126, "id" : 25, "chunk_2_id" : 108, "pg_version" : 39 }
{ "_id" : ObjectId("623bfb591218754902ad2744"), "chunk_0_id" : 118, "chunk_1_id" : 126, "primary_chunk_id" : 118, "id" : 26, "chunk_2_id" : 108, "pg_version" : 39 }
{ "_id" : ObjectId("623bfb591218754902ad2745"), "chunk_0_id" : 126, "chunk_1_id" : 118, "primary_chunk_id" : 126, "id" : 27, "chunk_2_id" : 109, "pg_version" : 39 }

{ "_id" : ObjectId("623bfb591218754902ad2746"), "chunk_0_id" : 118, "chunk_1_id" : 109, "primary_chunk_id" : 118, "id" : 28, "chunk_2_id" : 126, "pg_version" : 39 }

{ "_id" : ObjectId("623bfb581218754902ad2742"), "chunk_0_id" : 108, "chunk_1_id" : 126, "primary_chunk_id" : 108, "id" : 24, "chunk_2_id" : 118, "pg_version" : 39 }
{ "_id" : ObjectId("623bfb591218754902ad2747"), "chunk_0_id" : 109, "chunk_1_id" : 118, "primary_chunk_id" : 109, "id" : 29, "chunk_2_id" : 126, "pg_version" : 39 }

pg: 24, 25, 26, 27, 28, 29
chunks: 108, 109




-------------------

6e66818303f1b3ebfdddeb5bc8f3d1d7f165dfc8

6874dcfb014e93d97ce2914734d103130071a27e

---------------

×
√
×>×
×>×>√
×>×>〇>〇>√

------------

测试盘: bsi-1797-10


------------

chunk-22.04.28-c835dc2-centos7-x86_64-haswell       7a1bf55aad5a38d26853039db9df622c
chunk-22.04.28-c835dc2-centos7-x86_64-skylake       9c3579f8378cf1655e51163b4e5d6b58

seoul02: 1000 3000 3002 4000
hd03: 5022 5023 5024 5025
hn02: 5017 5018 5019  5020
hb03: 3228 3229 3230 3231
hb04: 3216 3217 3218 3219

-----------

<lc 205, random 1638802178> -> <lc 204, random 520190662>

dev/nvme2n1 chunk-117 10.72.141.215

-----

Offline switch off:
hb06 3219
hb03 5033
hb04 3230

----

麻烦确认下曼谷的这台机器net0是否有问题:

10.65.136.145

6月3日22:40至6月4日0:30期间一直提示net0有出向丢包, 但监控里带宽不大, 已经影响业务。

-------------------

麻烦确认一下hb的下面几对机器互为源目的，在12:

10.77.147.145 <-> 10.77.149.204
10.77.147.145 <-> 10.77.146.103
10.77.147.145 <-> 10.77.145.98
10.77.147.145 <-> 10.77.149.81
10.77.147.145 <-> 10.77.145.214


麻烦帮忙排查一下今天12:28和12:42左右这台机器的网络问题:

hb 10.77.147.145

在这2个时间点业务发生了请求超时, 问题发生在以下链接中:

10.77.147.145 <-> 10.77.149.204
10.65.136.145 <-> 10.77.146.103
10.65.136.145 <-> 10.77.145.98
10.65.136.145 <-> 10.77.149.81
10.65.136.145 <-> 10.77.145.214

从天梁以看到这台机器在这2个时间点时带宽不大，但是有server ping loss:



------------------------------------------------

struct spdk_nvme_qpair *qpair = xxxxx;
struct nvme_pcie_qpair *pqpair = nvme_pcie_qpair(qpair);

pqpair->sq_tail: submission_queue tail: We insert command at &pqpair->cmd[pqpair->sq_tail], then ++pqpair->sq_tail.

We could retrieve completion queue head from pqpair->cq_head


--------------------

影响:
云盘io抖动2s左右

发布版本（可选）：
chunk-22.04.28-c835dc2-centos7-x86_64-haswell       7a1bf55aad5a38d26853039db9df622c
chunk-22.04.28-c835dc2-centos7-x86_64-skylake       9c3579f8378cf1655e51163b4e5d6b58

发布范围：

hb03: 5057 5058 5059 5060 5061 5062 5063 5064 5065 5066 5067 5068
hb04: 5022 5023 5024 5025 5026 5027 5028 5029 5030 5031 5032 5033
hb06: 5037 5038 5039 5040 5041 5042 5043 5044 5045 5046 5047 5048

配置变更：


变更步骤：
自动化变更：
进入要变更发布可用区的跳板机，进入/root/limax/udisk/tool/release/chunk_auto_release目录，参考readme.md文件.
cd /root/limax/udisk/tool/release/chunk_auto_release && /usr/bin/python chunk_auto_release.py


变更验证：
变更前记录云盘io，变更后检查云盘io是否恢复


回滚步骤：
  优先回退脚本：
python roolback_chunk.py hd03 5057 [chunk_roolback_uuid] [optional: conf_roolback_uuid]


上述回退失败后紧急回退：
pssh -l root -h /root/limax/udisk/region/hd03/5057/host_info -i "ln -sf /root/udisk/chunk/chunk-22.04.28-c835dc2-centos7-x86_64-skylake /root/udisk/chunk/chunk"
pssh -h /root/limax/udisk/region/hd03/5057/host_info -i "ps aux | grep chunk/conf | grep -v grep | awk '{print \$2}' | grep -v $$ | xargs kill -9"


回滚验证：
验证io是否恢复

周二凌晨:
hb03: 5057 5058 5059 5060
hb04: 5022 5023 5024 5025
hb06: 5037 5038 5039 5040

周三凌晨:
hb03: 5061 5062 5063 5064
hb04: 5026 5027 5028 5029
hb06: 5041 5042 5043 5044

周四凌晨:
hb03: 5065 5066 5067 5068
hb04: 5030 5031 5032 5033
hb06: 5045 5046 5047 5048

-----------

合并:
hb03: 5057 5058 5059 5060 5061 5062 5063 5064 5065 5066 5067 5068
hb04: 5022 5023 5024 5025 5026 5027 5028 5029 5030 5031 5032 5033
hb06: 5037 5038 5039 5040 5041 5042 5043 5044 5045 5046 5047 5048

6-21 ~ 6-13 发布计划

目标版本:
chunk-22.04.28-c835dc2-centos7-x86_64-haswell       7a1bf55aad5a38d26853039db9df622c
chunk-22.04.28-c835dc2-centos7-x86_64-skylake       9c3579f8378cf1655e51163b4e5d6b58

周二凌晨:
hb03: 5057 5058 5059 5060
hb04: 5022 5023 5024 5025
hb06: 5037 5038 5039 5040

周三凌晨:
hb03: 5061 5062 5063 5064
hb04: 5026 5027 5028 5029
hb06: 5041 5042 5043 5044

周四凌晨:
hb03: 5065 5066 5067 5068
hb04: 5030 5031 5032 5033
hb06: 5045 5046 5047 5048

发布轮班:
李务军-孙徽-李鹏







----------------------

未发布:

hb03: 5049 5050 5051 5052 5053 5054 5055 5056 5057 5058 5059 5060 5061 5062 5063 5064 5065 5066 5067 5068
hb04: 5011 5012 5014 5015 5016 5017 5018 5019 5020 5021 5022 5023 5024 5025 5026 5027 5028 5029 5030 5031 5032 5033 5034 5035 5036 5037 5038 5039 5040 5041 5042
hb06: 5028 5029 5030 5032 5033 5034 5035 5036 5037 5038 5039 5040 5041 5042 5043 5044 5045 5046 5047 5048 5049 5050 5051 5052 5053 5054 5055 5056 5057 5058 5059 5060 5061 5062 5063 5064 5065

----------

未发布(出去本周已计划):

hb03: 5057 5058 5059 5060 5061 5062 5063 5064 5065 5066 5067 5068
hb04: 5022 5023 5024 5025 5026 5027 5028 5029 5030 5031 5032 5033 5034 5035 5036 5037 5038 5039 5040 5041 5042
hb06: 5037 5038 5039 5040 5041 5042 5043 5044 5045 5046 5047 5048 5049 5050 5051 5052 5053 5054 5055 5056 5057 5058 5059 5060 5061 5062 5063 5064 5065

-----------









----------------------

未发布:

hb03: 5049 5050 5051 5052 5053 5054 5055 5056 5057 5058 5059 5060 5061 5062 5063 5064 5065 5066 5067 5068
hb04: 5011 5012 5014 5015 5016 5017 5018 5019 5020 5021 5022 5023 5024 5025 5026 5027 5028 5029 5030 5031 5032 5033 5034 5035 5036 5037 5038 5039 5040 5041 5042
hb06: 5028 5029 5030 5032 5033 5034 5035 5036 5037 5038 5039 5040 5041 5042 5043 5044 5045 5046 5047 5048 5049 5050 5051 5052 5053 5054 5055 5056 5057 5058 5059 5060 5061 5062 5063 5064 5065

----------

未发布(出去本周已计划):

hb03: 5057 5058 5059 5060 5061 5062 5063 5064 5065 5066 5067 5068
hb04: 5022 5023 5024 5025 5026 5027 5028 5029 5030 5031 5032 5033 5034 5035 5036 5037 5038 5039 5040 5041 5042
hb06: 5037 5038 5039 5040 5041 5042 5043 5044 5045 5046 5047 5048 5049 5050 5051 5052 5053 5054 5055 5056 5057 5058 5059 5060 5061 5062 5063 5064 5065

-----------

下周6月21日(周二)~6月23日(周四)的udisk变更计划见下表, 邮件已发送。

其中的"invisible"主机涉及以下产品线:
udb ues ukafka uredis utidb usdp

---------

5003 offline chunks:

----------

BW:
File: Chunk234.20220613-221750.test03-udisk-set5003-0.1551.log.gz
22:17:48.211566Z bw: 31272960 linenumber: 12589
22:17:53.293934Z bw: 5621760 linenumber: 50230
submits: line 1882 ~ 2665 = 784
22:17:53.398378Z bw: 148480 linenumber: 84650
submits: line 1883 ~ 3415 = 1533
22:17:53.498398Z bw: 741483520 linenumber: 118255

utils:
20220613 22:17:46.662189Z busy_util=0.209511144895, idle util=0.790488855105
20220613 22:17:53.291886Z busy_util-0.992957086728, idle-util=0.00704291327166

lcs with io:
28
8
22
26
24
4
7
16
30
3
31
9
17
21
20
2
32
29
13
11
12
33
27
10.72.141.20710.72.141.207均不是重定向盘

------------------

clone_ubs_request

128*8192+1024*4096+5120*4096+9216*4096+17408*4096+33792*4096+66560*1024+132096*1024+263168*1024+264192*7168+266240*512+10240*(4*8)+10000*(24*4)+(10*8)*10000

[4, 8, 6, 12, 16, 14, 10]
[1,6,3,2,5]
[1,3,2,6,5]
[1,2]
[2,1]
[1]
[]

----------
````````````````````
10.72.142.7 3202 10.72.142.7
10.72.142.7 5001 10.72.142.7
10.72.142.7 5003 10.72.142.7
10.72.142.7 1200

---------

5003	10.72.142.144	291	4283aec5-51a9-4882-be06-2cb63db9ddfd -> 5003	310	10.72.142.144 e8bf71df-b913-436e-a925-79ce1ae08533
5003	10.72.142.143	308	d73362a8-aef9-4133-91b8-d831ea8598ad -> 5003	309	10.72.142.143	6eb9349d-7846-4c4f-894c-0d1fd59142dd


------------------------------

{ "_id" : ObjectId("62c57349b3041c5e72225921"), "state" : 1, "ip" : "10.72.142.143", "nvme_pci_addr" : "0000:db:00.0", "id" : 290 }, 不在任何pg中
{ "_id" : ObjectId("62c57683a5e0d34f0a1b05d4"), "state" : 1, "id" : 291, "nvme_pci_addr" : "0000:d9:00.0", "ip" : "10.72.142.144" }, 不在任何pg中
{ "_id" : ObjectId("62cd6945a5e0d34f0a23f2b1"), "id" : 296, "nvme_pci_addr" : "0000:d8:00.0", "ip" : "10.72.142.143", "state" : 1 }, 不在任何pg中
{ "_id" : ObjectId("62ce3f7eb3041c5e722ef44f"), "state" : 1, "ip" : "10.72.142.144", "nvme_pci_addr" : "0000:3d:00.0", "id" : 299 }, 在pg中
{ "_id" : ObjectId("62d0ebe1b3041c5e7233fea5"), "id" : 308, "state" : 1, "ip" : "10.72.142.143", "nvme_pci_addr" : "0000:d9:00.0" }, 在pg中

----------------

m = 5

0, 1

-------------------

|     |   0    |   1    |  10   |   100    |     1000     |    10000     |   100000   |   1000000 |    10000000      | 

----------------

10.73.144.207 -> 10.73.144.19


------
14:25:00 ~ 14:30:00:

10.65.141.94 <-> 10.65.141.92
10.65.141.94 <-> 10.65.141.28


11:55:00 - 12:10:00:

10.65.141.94 <-> 10.65.140.219

----

1201
3202
5003

-----

4台机器没有trigger:

hd02 pc02 jakarta02 mumbai01


--------------------

7
[[1,0],[0,3],[0,2],[3,2],[2,5],[4,5],[5,6],[2,4]]


------------------------
rte_thread_set_affinity()

unsigned int 	rte_get_main_lcore (void)
unsigned int 	rte_lcore_to_socket_id (unsigned int lcore_id)

-----------------

rte_lcore.h -> rte_thread_set_affinity()

--------------

dpdk -m reserve内存: 优先会从master core所在numa进行reserve。如果把目标numa 上的内存reserve完了还不够, 那么剩下的内存会从其他numa node上reserve。

------

7
3              11
1    5    9     13
0 2 4 6 8 10 12 14

[7,3,11,1,5,9,13,0,2,4,6,8,10,12,14]


[7,3,11,1,5,9,13,0,2,4,6,8,10,12,14]
0
[7,3,11,1,5,9,13,0,2,4,6,8,10,12,14]
1
[7,3,11,1,5,9,13,0,2,4,6,8,10,12,14]
2
[7,3,11,1,5,9,13,0,2,4,6,8,10,12,14]
3
[7,3,11,1,5,9,13,0,2,4,6,8,10,12,14]
4
[7,3,11,1,5,9,13,0,2,4,6,8,10,12,14]
5
[7,3,11,1,5,9,13,0,2,4,6,8,10,12,14]
6
[7,3,11,1,5,9,13,0,2,4,6,8,10,12,14]
7
[7,3,11,1,5,9,13,0,2,4,6,8,10,12,14]
8
[7,3,11,1,5,9,13,0,2,4,6,8,10,12,14]
9
[7,3,11,1,5,9,13,0,2,4,6,8,10,12,14]
10
[7,3,11,1,5,9,13,0,2,4,6,8,10,12,14]
11
[7,3,11,1,5,9,13,0,2,4,6,8,10,12,14]
12
[7,3,11,1,5,9,13,0,2,4,6,8,10,12,14]
13
[7,3,11,1,5,9,13,0,2,4,6,8,10,12,14]
14




------------------

[1,2]
2
[1,2,3]
2
[1,2,3,4]
2
[1,2,3,4,5]
2
[1,2,3,4,5,6]
2

-----------
[1,2,3]
3
[1,2,3,4]
3
[1,2,3,4,5]
3
[1,2,3,4,5,6]
3
[1,2,3,4,5,6,7]
3
[1,2,3,4,5,6,7,8]
3
[1,2,3,4,5,6,7,8,9]
3


-------------------

38: 377 382 388
36: 388 382 377
31: 373 388 381
32: 388 373 381
37: 382 377 388
33: 373 388 382
34: 388 382 373

--------------
check_rate                         = 100M
check_high_water_mark_rate         = 1000M

------
pc meta:
is_used: 1
has_detection: 1
seq_no: 4894
pc_id: 313152
offset: 1314832236544
pg_id: 52
lc_id: 31
pc_no: 1902
lc_random_id: 2771744322
allocate_time: 1660777627
redirect_bits: 0

---------------------

hugepage_info_init(): 记录huge_page_dir对应的大页size, 以及其在各个numa node上的剩余大页数量。结果记录在 internal_conf->hugepage_info 中。

-------

/var/run/dpdk/spdkxxx/fbarrayxxx : 这些文件映射了spdk进程中的数组结构。每个这样的结构的首部是一组struct rte_memzone 的数组。在 

fbarray_memzone 中数组中元素的数量是  RTE_MAX_MEMZONE (2560), 其他的文件则需要单独计算。

紧随数组后的是一个 struct used_mask, 用做bitmap以标识这个数组中各元素是否正被使用。

准确来说, 这个结构所占用的内存来自一段private mmap。Private mmap成功后, spdk/dpdk将会创建文件 /var/run/dpdk/spdkxxx/fbarray_xxx 。

然后, 会把这个文件再shared mmap 到这段内存上。

这个结构的起始地址会被放入struct rte_fbarray里, 这个struct rte_fbarray

存在于多处, 包括 rte_eal_get_configuration()->mem_config->memzones， 以及每个 struct rte_memseg_list。

---------

memtype: dpdk用numa node id和huge page size的组合作为memtype。比如, 一个系统有2个numa节点, 2中不同size的大页, 那么这个系统上存在有2 * 2 = 4个memtype。

------------------

RTE_MAX_MEM_MB: 这个量是config生成的量, 代表整个spdk进程中, 大页所能够使用的最大内存量??? xstore编译环境生成的值为 524288, 即 512GiB。

-------------

Memseglist: struct rte_memseg_list, 作为一个数组存在于rte_mem_config中( internal_conf->memsegs)。数组元素数量为 RTE_MAX_MEMSEG_LISTS (128)。

每种memtype内都有数个rte_memseg_list。但他们都位于数组memsegs中, 也就是说, 不论系统中有多少种memtype, memseg list的总数都不会超过 RTE_MAX_MEMSEG_LISTS。

在我们的测试环境中, 每种memtype有2个memseglist, 每个memseglist负责管理32 G的虚拟地址空间。


---------------

eal_memseg_list_init(): 为 memseg初始化, 并创建其对应的rte_fbarray, 以及相应的文件。
eal_memseg_list_alloc(): 为 memseg所管理的大页保留虚拟地址空间。


--------------

在我们的编译环境中, 每个memseglist最多允许管理 32GiB 的大页地址空间。

-------------

rte_eal_memseg_init(): 初始化各个memseglist, 为他们创建对应的rte_fbarray以及相应的文件并map; 为各个memseglist所管理的大页保留虚拟地址空间。

----------------

fd_list: 定义为

static struct {
  int *fds; /**< dynamically allocated array of segment lock fd's */
  int memseg_list_fd; /**< memseg list fd */
  int len; /**< total length of the array */
  int count; /**< entries used in an array */
} fd_list[RTE_MAX_MEMSEG_LISTS];

这个结构体用于为各个memseglist保存对应的fd, 包括struct rte_fbarray所对应的fd(memseg_list_fd)和其内的大页所对应的fd(fds, 这是个数组,

因为一个memseg内会有多个大页, 未分配的大页在这个数组内的值是-1)。

-------------

eal_dynmem_calc_num_pages_per_socket():

从这个函数可以得出, dpdk在为dynamic mem mode的进程reserve大页内存时, 会优先从lcore所在的numa节点来reserve。

如果这个进程有多个lcore, 且它们位于不同的numa节点, 那么所预留的内存会根据各个numa上的lcore的数量来分配大页, 只要它们拥有足够的大页。

比如, 一个dpdk进程需要预留3G的大页。这个dpdk有3个lcore, 2个位于numa 0, 1个位于numa1。那么这预留的3G大页中, 2G将处于numa 0中, 1G将处于numa 1中。

----------------------

FaultPoint:
7 1
40 1
41 1
38 1
34 2
21 1
3 1
31 1
1 2
6 1
22 2
30 1
29 2
24 2
23 1
0 2
28 1
36 2
26 1
25 1
32 1
33 1


------------

FAULT_POINT_38, 问题: 没有mark replica received, 没有EndIORequest(), 可能会core。
需要改成 retcode = -1, extended_errcode = 0

FAULT_POINT_34, 问题: 不应该分成2个语句。

FAULT_POINT_1, 问题: 不应该分成2个语句。

FAULT_POINT_22, 问题: 不应该分成2个语句。

FAULT_POINT_24, 问题: 不应该分成2个语句。

FAULT_POINT_0, 问题: 不应该分成2个语句。

FAULT_POINT_11, 问题: 不应该分成2个语句。

FAULT_POINT_12, 问题: 不应该分成2个语句。

FAULT_POINT_14, 问题: 不应该分成2个语句。

----------------

FAULT_POINT_7: 问题: RawChunkStorage::AllocatePcCb(): 在done()执行后, 需要退出函数。


-----------------------

1. /etc/yum.repos.d: 删除其中所有的repo文件, 将manul_dev_env中的repo文件复制过来。
2. `yum clean all`, `yum makecache`。注意在执行上一步和这一步之前绝对不要使用yum安装或更新任何包。
3. `yum -y install which vim openssh-clients git`
4. `ssh-keygen`, 然后一路回车。
5. `cat ~/.ssh/id_rsa.pub`, 将cat出的字符串添加到gitlab账户的SSH keys中。
6. 手动拉取一次 `manul` ( git@git.ucloudadmin.com:Manul/manul.git )。这一步的目的是手动添加gitlab服务器的ssh key, 否则第7步拉取spdk时需要手动确认。
7. 回到`manul_dev_env`, 执行`./build_new_compile_env.sh 2>&1 | tee build_log.log`。 (脚本启动后需要敲一次回车键确认)
8. `vim ~/.bashrc`, 检查是否有重复的`export`语句。如果第7步执行了多次, 那么有可能会有重复出现, 需要将其删除。然后执行 source ~/.bashrc。
9. 执行 `gcc -v`和`g++ -v`, 查看gcc和g++版本是否为9.3.0。如果不是, 代表前述步骤出现错误。
10. 进入目录 `~` (`/root`), 拉取 wiwo (`git clone https://git.ucloudadmin.com/ucloud-base-rpc-framework/wiwo.git`)
11. 参考 `https://git.ucloudadmin.com/ucloud-base-rpc-framework/wiwo` 的readme, 编译`libwiwo_common`和`wiwo_aio`(不要再安装gcc g++和git; Makefile有问题, 不要使用`make -jn`, 只能单线程`make`)。
12. 进入目录 `~` (`/root`), 拉取message库 (`git clone https://git.ucloudadmin.com/ucloud-base-rpc-framework/message.git`)
13. 进入message库的目录, 执行`git checkout manul_dev`
14. 执行`./dep-rapid-make.sh`

-----------------

# 临时编译方案

## 1. 环境

1. 登陆 开发机 172.20.68.5 。
2. 建立docker container: `docker run -d --name ${contianer_name} --hostname ${host_name} -v ${host_data_dir}:${data_dir_in_container} -it bieti.compile.centos.7.4:tmp`
3. 进入容器`docker attach ${container_name}`
4. 重建sshkey, `ssh-keygen`, 并将新key添加至gitlab 账户ssh密钥列表。

## 2. 编译

1. 拉取 manul (`git clone https://git.ucloudadmin.com/Manul/manul.git`)
2. 进入 manul 目录, 确认处于`develop`分支（`git checkout develop`)
3. 拉取submodule (`git submodule update --init --recursive`)
4. 进入ustevent submodule目录, 切换到`manul_temp`分支, 并`git pull`
5. 执行ustevent目录中的`./do_cmake.sh`。
6. 进入build 目录, 执行`make` 编译ustevent。第一次`make`会失败, 失败后再执行一次`make`。`make`完成后, 执行`make install`安装ustevent。
7. 回到`manul`项目的根目录, 打开`CMakeLists.txt`, 注释掉`add_subdirectory(fio_client_plugin)`和`add_subdirectory(fio_portal_plugin)`。这两个项目的依赖问题没有解决, 目前无法编译。
8. 执行`./do_cmake.sh`, `cd build`, `make`。


---------------

la01 3203 10.70.19.12:

07-12 00:00 - 07-12 19:00 : 1.2% -> 1.63 % (时间跨度较长, 增长较缓)
07-12 19:00 - 07-12 22:00 : 1.63% -> 2.73%
07-13 02:00 - 07-13 05:00 : 2.7% -> 3.3%
07-13 19:00 - 07-13 23:00 : 3.3% -> 4.5%
07-14 08:00 - 07-14 11:00 : 4.5% -> 7%
07-18 09:00 - 07-18 18:00 : 7.5% -> 11.2%

且每段增长普遍伴随有CPU usage的升高

------------------

la01 3206 10.70.18.155:

07-14 10:00 - 07-14 14:00 : 0.7% -> 2.1%
07-18 09:00 - 07-18 16:00 : 2.1% -> 5.6%

-------------------

(gdb) p g_context->tcp_stack_->workers_
$2 = std::vector of length 5, capacity 8 = {0x328cdc0, 0x328d080, 0x328d340, 0x328d600, 0x328d8c0}

(gdb) p (('udisk::chunk::ChunkLoopHandle' *)(('uevent::EventWorker' *) 0x328cdc0)->eventloop_->loop_handle_)
$6 = (udisk::chunk::ChunkLoopHandle *) 0x3296160

(gdb) p (('udisk::chunk::ChunkLoopHandle' *)(('uevent::EventWorker' *) 0x328d080)->eventloop_->loop_handle_)
$7 = (udisk::chunk::ChunkLoopHandle *) 0x32962c0

(gdb) p (('udisk::chunk::ChunkLoopHandle' *)(('uevent::EventWorker' *) 0x328d340)->eventloop_->loop_handle_)
$8 = (udisk::chunk::ChunkLoopHandle *) 0x3296210

(gdb) p (('udisk::chunk::ChunkLoopHandle' *)(('uevent::EventWorker' *) 0x328d600)->eventloop_->loop_handle_)
$9 = (udisk::chunk::ChunkLoopHandle *) 0x3296370

(gdb) p (('udisk::chunk::ChunkLoopHandle' *)(('uevent::EventWorker' *) 0x328d8c0)->eventloop_->loop_handle_)
$10 = (udisk::chunk::ChunkLoopHandle *) 0x3296420

------------


(gdb) p ('udisk::chunk::IOHandleTcp' *)(('udisk::chunk::ChunkLoopHandle' *) 0x3296160)->io_handle_
$20 = (udisk::chunk::IOHandleTcp *) 0x1455d8000

(gdb) p ('udisk::chunk::IOHandleTcp' *)(('udisk::chunk::ChunkLoopHandle' *) 0x32962c0)->io_handle_
$21 = (udisk::chunk::IOHandleTcp *) 0x1455d8900

(gdb) p ('udisk::chunk::IOHandleTcp' *)(('udisk::chunk::ChunkLoopHandle' *) 0x3296210)->io_handle_
$22 = (udisk::chunk::IOHandleTcp *) 0x1455d8480

(gdb) p ('udisk::chunk::IOHandleTcp' *)(('udisk::chunk::ChunkLoopHandle' *) 0x3296370)->io_handle_
$23 = (udisk::chunk::IOHandleTcp *) 0x1455d8d80

(gdb) p ('udisk::chunk::IOHandleTcp' *)(('udisk::chunk::ChunkLoopHandle' *) 0x3296420)->io_handle_
$24 = (udisk::chunk::IOHandleTcp *) 0x1455d9200

------

(gdb) p ('udisk::chunk::RawChunkStorage' *)(('udisk::chunk::IOHandleTcp' *) 0x1455d8000)->chunk_storage_
$25 = (udisk::chunk::RawChunkStorage *) 0x32fcc60

(gdb) p ('udisk::chunk::RawChunkStorage' *)(('udisk::chunk::IOHandleTcp' *) 0x1455d8900)->chunk_storage_
$26 = (udisk::chunk::RawChunkStorage *) 0x14553d9e0

(gdb) p ('udisk::chunk::RawChunkStorage' *)(('udisk::chunk::IOHandleTcp' *) 0x1455d8480)->chunk_storage_
$27 = (udisk::chunk::RawChunkStorage *) 0x14553db00

(gdb) p ('udisk::chunk::RawChunkStorage' *)(('udisk::chunk::IOHandleTcp' *) 0x1455d8d80)->chunk_storage_
$28 = (udisk::chunk::RawChunkStorage *) 0x14553d8c0

(gdb) p ('udisk::chunk::RawChunkStorage' *)(('udisk::chunk::IOHandleTcp' *) 0x1455d9200)->chunk_storage_
$29 = (udisk::chunk::RawChunkStorage *) 0x14553d7a0


-----------

发起内部克隆

回收(要不要回收?)

---------------

spdk_gate配置文件添加如下项:

[tcp]
thread_num                 = 8

挂载脚本:

cd /root/udisk/scripts/nvmf/
./add_nvmf_udisk.py
./construct_nvme_udisk.sh

卸载脚本:

cd /root/udisk/scripts/nvmf/
./disconnect_nvme_udisk.sh
./remove_nvmf_udisk.py


--------------

issue2054-test0001 bsm-95o35iob50r 9643 1088545334
issue2054-test0002 bsm-95o50o81cmj 9653 3399458239
issue2054-test0003 bsm-95o5isjire3 9654 3088486495
issue2054-test0004 bsm-95o670m2l9n 9655 1605053630
issue2054-test0005 bsm-95o6m320sh7 9656 914250255
issue2054-test0006 bsm-95o73c0s2zv 9657 3957339110

------

宿主:

root@10.72.142.195
root@10.72.142.196
root@10.72.142.197
root@10.72.142.198
root@10.72.142.2
root@10.72.142.3

[root@test03-uhost-142-196 ~]# cp /root/udisk/spdk_gate/conf/spdk_gate.ini /root/udisk/spdk_gate/conf/spdk_gate.ini_bak20220910

cd /root/udisk/spdk_gate/conf/
vim spdk_gate.ini


------

fill_disk:

fio --direct=1 --ioengine=libaio --bs=128k --name=fill --iodepth=32 --numjobs=1 --rw=write --filename=/dev/nvme0n1

--------------

1657555200

1658246400


-----------------

1000: 机械盘落盘压力大, 老问题。

3000: 故障注入引起修复失败。

------------

香港 (ap04):


10.67.12.156
10.67.12.145
10.67.12.148
10.67.12.146
10.67.12.154
10.67.12.153
10.67.12.155
10.67.12.150
10.67.12.151







---------------

读请求起始:

SYSCALL_DEFINE3(read, .....); (fs/read_write.c)

SYSCALL_DEFINE3(read. ...) -> ksys_read()

ksys_read(): 获取fd, 管理f_pos。

ksys_read() -> vfs_read()

vfs_read(): 权限检查, 参数合法性检查

vfs_read() 调用 file->f_op->read()或new_sync_read(), fsnotify_access(), 更新current->ioac (struct task_io_accounting)。

------------------------

以ext2为例, ext2_file_operations::read 为空, 但是存在 .read_iter, 因此调用new_sync_read()。

---------------------

new_sync_read(): 初始化 iov (struct iovec) 和 kiocb (struct kiocb) 和 iter (struct iov_iter)。

随后调用 call_read_iter() , 并在其返回后更新ppos。

-------------------------

call_read_iter() 调用 file->f_op->read_iter, 在ext2中即 ext2_file_read_iter()

------------------

ext2_file_read_iter() 调用 generic_file_read_iter()。

-------------------------

generic_file_read_iter() 在不考虑 Direct IO 的情况下, 将调用filemap_read(), 其参数already_read将设置为0。

-------------------------

struct xarray、struct xa_node等, 是新的基础设施XARRAY的数据结构。XARRAY被用来替代旧有的radix tree。

------------------------

struct folio 用于表示一组连续的内存, 且内存的大小为2的幂。struct folio内只有一个union。如果struct folio指代
一段连续的内存page, 则struct folio 就是一个struct page的包装。

目前struct adress_space内的radix tree和struct page都已经被替换为了XArray和struct folio。

struct folio_batch就是此前的struct pagevec的平替。

-------------------------

mapping_writeably_mapped() 返回 一个address_space的i_mmap_writable是否大于0。如果其大于0, 就代表有进程shared map
了这个page cache, 并且这个map是writable的。

-----------------------

https://zhuanlan.zhihu.com/p/91338338 :

问：Linux的中断可以嵌套吗？

答：以前是可以嵌套的，现在不可以！

历史
早前的Linux内核版本，中断分为两种：

快中断，申请的时候带IRQF_DISABLED标记，在IRQ HANDLER里面不允许新的中断进来;

慢中断，申请的时候不带IRQF_DISABLED标记，在IRQ HANDLER里面允许新的其他中断嵌套进来。

老的Linux内核中，如果一个中断服务程序不想被别的中断打断，我们能看到这样的代码：

request_irq(FLOPPY_IRQ, floppy_interrupt,\
-                        IRQF_DISABLED, "floppy", NULL)
现在
在2010年如下的commit中，IRQF_DISABLED被作废了：

https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=e58aa3d2d0cc


-------------------

Linux 中定义, intterupt为内部定时器或IO设备所发出的、引起执行的信号, 这类信号通常引起执行流跳转至ISR;
 exception是指由程序错误引发的信号, 例如除0错误、缺页等, 对这些问题, kernel可能会向目标进程发送信号、执行异常对应的处理代码等。

中断分为maskable interrupts 和 nonmaskable interrupts (NMI)

异常分为
    Faults: 发生时栈上所保存的ip是引起此fault的指令。也就是说在异常处理完成后, 可以重新执行问题指令。Fault的代表是page fault。
    Traps: 发生时栈上所保存的ip是引起此trap的指令的下一条指令。典型的应用场景是debug程序时的trace功能。另外, 早期的x86系统调用也以trap的形式存在。用户态程序执行int指令陷入系统调用。
    Aborts: 发生时相应的control unit不能再继续执行, 因此其所保存的ip是不可执行的。此时内核将终止此control unit。

-------------------

Oops: Kernel在出错时所打印的寄存器状态、调用栈、堆栈信息等。
Panic: 内核崩溃, 指内核遇到严重错误必须停止内核自身的运行。

-----------------

为了充分利用locality，通常选择将处理hardirq的CPU作为该hardirq对应的workqueue底半部的执行CPU，
在早期Linux的实现中，每个CPU对应一个workqueue队列，并且每个CPU上只有一个worker线程来处理这个workqueue队列，
也就是说workqueue队列和worker线程都是per-CPU的，且一一对应。

cmwq

这种在一个CPU上运行多个worker线程的做法，就是 v2.6.36 版本引入的，也是现在Linux内核所采用的concurrency managed workqueue，简称cmwq。一个CPU上是不可能“同时”运行多个线程的，所以这里的名称是concurrency(并发)，而不是parallelism(并行)。

显然，设置合适的worker线程数目是很关键的，多了浪费资源，少了又不能充分利用CPU。大体的原则就是：如果现在一个CPU上的所有worker线程都进入了睡眠状态，但workqueue队列上还有未处理的work item，那么就再启动一个worker线程。

一个CPU上同一优先级的所有worker线程（优先级的概念见下文）共同构成了一个worker pool(此概念由内核v3.8引入)，我们可能比较熟悉memory pool，当需要内存时，就从空余的memory pool中去获取，同样地，当workqueue上有work item待处理时，我们就从worker pool里挑选一个空闲的worker线程来服务这个work item。


----------------------

15:22~15:24

10.67.157.143 <-> 10.76.28.74
10.67.157.143 <-> 10.67.176.87
10.67.157.143 <-> 10.67.177.24
10.67.157.143 <-> 10.67.176.209

------------

lc 58
pc 36

set 1000


----------

db.t_lc_info.aggregate([{$match:{"create_time": {"$gte":1664236800, "$lte": 1664266105}}},{$group:{"_id":null,"total capacity":{"$sum":"$size"}}}])

[8,4,12,2,6,10,14,1,3,5,7,9,11,13,15]
1
[8,4,12,2,6,10,14,1,3,5,7,9,11,13,15]
2
[8,4,12,2,6,10,14,1,3,5,7,9,11,13,15]
3
[8,4,12,2,6,10,14,1,3,5,7,9,11,13,15]
4
[8,4,12,2,6,10,14,1,3,5,7,9,11,13,15]
5
[8,4,12,2,6,10,14,1,3,5,7,9,11,13,15]
6
[8,4,12,2,6,10,14,1,3,5,7,9,11,13,15]
7
[8,4,12,2,6,10,14,1,3,5,7,9,11,13,15]
8
[8,4,12,2,6,10,14,1,3,5,7,9,11,13,15]
9
[8,4,12,2,6,10,14,1,3,5,7,9,11,13,15]
10
[8,4,12,2,6,10,14,1,3,5,7,9,11,13,15]
11
[8,4,12,2,6,10,14,1,3,5,7,9,11,13,15]
12
[8,4,12,2,6,10,14,1,3,5,7,9,11,13,15]
13
[8,4,12,2,6,10,14,1,3,5,7,9,11,13,15]
14
[8,4,12,2,6,10,14,1,3,5,7,9,11,13,15]
15
[1]
1


---------------

问题盘

173.30.161.211 nvme1 -> seoul02 3002 chunk-3
173.30.161.212 nvme0 -> seoul02 3002 chunk-4
172.21.1.164 nvme0 -> ap03 3000 chunk-10
172.30.161.214 nvme1 -> seoul02 3002 chunk-9


 172.20.132.194 槽位：d8:00.0，盘符：nvme0故障


----------------

下chunk:
172.20.132.194 nvme0 -> hd06 3005 chunk-12
172.20.132.71 nvme0 -> hd06 3005 chunk-46

下服务器上的所有chunk:
172.21.1.164 nvme0 -> ap03 3000 chunk-10

--------------------------

下服务器上的所有chunk, 但是先不操作:
172.30.161.211 nvme1 -> seoul02 3002 chunk-3
172.30.161.212 nvme0 -> seoul02 3002 chunk-4
172.30.161.214 nvme1 -> seoul02 3002 chunk-9


// 数据以小端序存储
struct index_on_file {
  u64 offset_in_segment : 38;
  u32 volume_id : 20,
  u32 flags : 6,
  u64 cdp_seq_no;
  u32 offset_in_manul_file;
  u32 length;
} __attribute__((packed));

flags:

| MSB <------------------------------------------------------------> LSB |
| Bit 5    | Bit 4    | Bit 3    | Bit 2    | Bit 1    | Bit 0           |
| Reserved | Reserved | Reserved | Reserved | Flag_CDP | Flag_Write_Trim |

----------------

struct index_file_head {
  u64 version;
  u32 segment_server_id_who_generates_this_file;
  u32 magic;
  u64 corresponding_data_file_index;
  u32 count; // 条目数量
    // 时间戳用u64 是考虑到u32 epoch溢出的问题。不过如果不从标准的1970-1-1来起始, 那也可以用u32
  u64 start_epoch; // data file中第一笔操作发生的时间戳。
  u64 last_epoch; // data file 中最后一笔操作发生的时间戳。
} __atrribute__((packed));

-----------------

struct log_in_data_file {
  u64 offset_in_segment : 38;
  u32 volume_id : 20;
  u32 flags : 6
  u64 cdp_seq_no;
  u32 length;
} __attribute__((packed));

-------------

Trim信息既记在index file里, 也记在data file 里。

一个index file对应一个data file。

Index file只有在对应的data file写完之后才会下发。

Index file写入前可以在内存中先进行去重。如果一笔数据在同一个index file里被写入了多次, 那只保存最新的一笔写入信息; 如果先被写入后被trim, 那index file里就完全不保存这笔数据的任何操作信息。(对于cdp数据, 不能去重)

Data file 定操作数量, 不定大小。(具体细节还要考虑, 比如操作数量包不包括trim)

Data file中, 每笔写入会生成一个data file index, 后面跟着对应的数据。如果是trim, 那就只有data file index, 没有对应的数据。

Volume Id, 每次打快照、启动cdp、关闭cdp, 都切换一次。每次创建新盘也会分配一个。Volume Id一共有20 bits, 共1048576个。

设计目标: 集群最大容量1 PB, 单盘最大容量1 PB。

---------------

Index 文件结构

----------------------------------     ---------
| Index  | Index | Index | Index |     | Index |
| File   |       |       |       | ... |       |
| Head   | 0     | 1     | 2     |     | n     |
----------------------------------     ---------

------

index_on_file中不单独存储volume_id

-------

1. 因为index file是在对应的data file写完之后才刷盘的, 所以刷盘的时候已经知道一个index file里有多少条索引了。需不需要在index file head或者tail里加上索引条目的数量，做校验用。

2. index_file_map_里每一个条目是sizeof(offset) + sizeof(IndexFileEntry)，是32B。这样128G的数据最好情况需要16M内存, 但是最差情况下需要1G。

如果IndexFileEntry只会被读取、写入、trim使用, 不牵涉到cdp恢复， 有3样东西是我觉得应该是用不着的:
  1. src_offset, 这个和index_file_map_的offset key重复
  2. flags: 只有trim有意义。如果发生trim, 直接将被trim的空间从index_file_map_里移除
  3. ctime: 读写用不着, 不需要存在全量索引里。
这样可以节省一半内存。

3. 只存在于内存里、不会通过网络发送出去、不会写入磁盘的数据结构，除非内存资源紧张, 尽量不要用__attribute__((packed))修饰, 会有性能问题。



-------------------------

1. 新segment的目录的创建: 在创盘时创建, 不在IO发生时动态创建。Metaserver先选出新segment对应的segmentserver, 然后向此segmentserver发送初始化此segment的pb。此时segmentserver创建对应的目录, 目录创建失败则创盘失败。这样在IO到来时可以确认对应的目录一定是存在的。
2. 删文件: 2个方案 1. 成对的文件由segmentserver负责删除; 只有index没有data的文件对由segmentserver删除; 只有data没有index的文件由GC删除; 最新的未写完的data file两边都不删除。2. Segmentserver发现文件不再需要后, 通过PB通知gc 删除。
3. CDP数据是否需要切换segment, 是否需要切换manul file。
a. 切换segment: 实现最简单, 但是IO的重定向链会很长。
b. 不换segment, 但是换manul file: GC会相对复杂一些。
c. 不换segment, 也不换manul file: 考虑是否需要和方舟一样, 提供秒级数据压缩成小时级、小时级压缩成天级, 或者类似的功能。如果需要提供类似的功能, 且这个功能由GC负责实现, 那么更换segment或manul file就是没有意义的, 不能给GC带来便利。


------------------

 mr_manager.cc: 100


---------------

删文件, 是新建一类任务类型, 还是

117+49+34+12+238+112

------------

debug3: fd 6 is not O_NONBLOCK
debug1: Forked child 340.
debug3: send_rexec_state: entering fd = 10 config len 713
debug3: ssh_msg_send: type 0
debug3: send_rexec_state: done
debug3: oom_adjust_restore
debug1: Set /proc/self/oom_score_adj to 0
debug1: rexec start in 6 out 6 newsock 6 pipe 9 sock 10
debug1: inetd sockets after dupping: 5, 5
Connection from 192.168.168.157 port 50659 on 172.17.0.44 port 22

wget -O Country.mmdb https://www.sub-speeder.com/client-download/Country.mmdb

docker run -d --name ss-libev --restart always -p 9000:9000 -p 9000:9000/udp -v /etc/shadowsocks-libev:/etc/shadowsocks-libev teddysun/shadowsocks-libev


37397

ProxyCommand /usr/bin/nc -X 5 -x 127.0.0.1:7777 %h %p

---------------------------

Github personal token: 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


---------------------
sourcetrail config -g /usr/local/include,/usr/include,/usr/local/include/c++/9.3.0,/usr/local/include/c++/9.3.0/backward,/usr/local/include/c++/9.3.0/x86_64-pc-linux-gnu,/usr/include/c++/4.8.2,/usr/include/c++/4.8.2/x86_64-redhat-linux,/usr/include/c++/4.8.2/backward

sourcetrail config -g /usr/local/lib/gcc/x86_64-pc-linux-gnu/9.3.0/include,/usr/local/include,/usr/local/lib/gcc/x86_64-pc-linux-gnu/9.3.0/include-fixed,/usr/include

----------

算法正确性(需具体考虑)

metaserver addr获取/变更

并发控制

添加各种成功时的打印。


允许生成极大的index file: 减少


-----------------


./segdump -segdump_content_file_path=./content -segdump_path_prefix=/bieti -manul_raft_conf=10.72.174.5:8550:0,10.72.174.7:8550:0,10.72.172.19:8550:0 -segdump_segment_id=0 -segdump_force_overwrite=true

----------------------

hn02

~~hb06~~

~~hb04~~

~~hb03~~

hd03  3201  3202  3203  3204
hd04  3007  3008 3009  3200  
hd06  3002  3004  3005  3007

-------------------


-----------------------

hn02:

5001
5002
5003
5009
5010
5011
5012
5013
5015
5016
5017
5018

共12个

---------------
hd03:

3201
3202
3203
3204
5003
5004
5005
5008
5009
5010
5011
5012
5014
5015
5016

共15个

--------------

hd04:

3007
3008
3009
3200

共4个

---------------

hd06:

3002
3004
3005
3007
3204

共5个

-------------------

hb06:

12001
1201
1202
1203
1204
3215
3216
3217
3218
3219
3221
3222
3224
3225
3226

共15个

--------------

hb04:

3211
3212
3213
3214
3215
3216
3217
3218
3219
3220
3221
3222
3224
3225
3229

共15个

-----------

hb03:

3228
3229
3230
3231
3232



-------------------------

周一:
hd06 3002 3004 3005 3007 3204
hn02 5001 5002 5003 5009 5010
hd03 3201 3202 3203 3204 5003
hb06 12001 1201 1202 1203 1204
hb04 3211 3212 3213 3214 3215
hd02 1000 3200

周二:
hn02 5011 5012 5013 5015 5016
hd03 5004 5005 5008 5009 5010
hd04 3007 3008 3009 3200
hb06 3215 3216 3217 3218 3219
hb04 3216 3217 3218 3219 3220

周四:
hn02 5017 5018
hd03 5011 5012 5014 5015 5016
hb06 3221 3222 3224 3225 3226
hb04 3221 3222 3224 3225 3229
hb03 3228 3229 3230 3231 3232

---------------------

下周的UDisk后端变更时间表已排出:
变更时间: 2022-12-20 - 2022-12-23, 每日00:00 ~ 03:00
变更服务: UDisk云硬盘后端服务
变更范围: 广州, 上海二A, 上海二B, 上海二C, 北京二B, 北京二C, 北京二E
维护影响: 维护期间，网络不会中断，涉及云盘会有一次3秒左右的IO抖动现象。

影响主要客户:
深圳市禅游科技股份有限公司
孩子王儿童用品股份有限公司
北京值得买科技股份有限公司
天天用车
colossal-tech.com
北京硬核聚视科技有限公司
鼎对科技(北京)有限公司

邮件已发, 具体受影响云盘见下表


变更期间, 对udb和ukafka的服务也有影响。受影响资源见下表, 也麻烦各位通知一下。


------------
1474560*4096=6039797760

----------------

last block start point: 131112, predecessor block = {.offset = 154, .start_point = 56, .length = 1520}, block_index = 4, block offset in index file: 131176 - meta_data_utils.cc:205

----------------------

影响范围：
云盘io抖动3s左右

发布版本（可选）：
chunk-22.11.16-f535f41-centos7-x86_64-haswell 4e3f1de84f54fb53e6cee78f5f18a3a5
chunk-22.11.16-f535f41-centos7-x86_64-skylake f76094b956ebee3d8f0f0490ea50e129

tools-22.11.11-c1576a3

发布范围：
12-20 00:00
hb04 3211 3212 3213 3214 3215
hb06 12001 1201 1202 1203 1204
hd06 3002 3004 3005 3007 3204
hd02 1000 3200
hd03 3201 3202 3203 3204 5003
hn02 5001 5002 5003 5009 5010
 
12-21 00:00
hb04 3216 3217 3218 3219 3220
hb06 3215 3216 3217 3218 3219
// hd03 5004 5005 5008 5009 5010
hd04 3007 3008 3009 3200
// hn02 5011 5012 5013 5015 5016

12-23 00:00
hb03 3228 3229 3230 3231 3232
hb04 3221 3222 3224 3225 3229
hb06 3221 3222 3224 3225 3226
// hd03 5011 5012 5014 5015 5016
// hn02 5017 5018
 

配置变更：


变更依赖（可选）：


变更步骤：
进入要变更发布可用区的跳板机，进入/root/limax/udisk/tool/release/chunk_auto_release目录，参考readme.md文件.
cd /root/limax/udisk/tool/release/chunk_auto_release && /usr/bin/python chunk_auto_release.py

变更验证：

回滚步骤：
python roolback_chunk.py hd06 3002 0 [chunk_roolback_uuid] [optional: conf_roolback_uuid]

pssh -l root -h /root/limax/udisk/region/hd06/3002/host_info -i "ln -sf /root/udisk/chunk/chunk-22.07.04-b5af675-centos7-x86_64-haswell /root/udisk/chunk/chunk"

pssh -h /root/limax/udisk/region/hd06/3002/host_info -i "ps aux | grep chunk/conf | grep -v grep | awk '{print $2}' | grep -v $$ | xargs kill -9"

-----------------

hb03 3228 3229 3230 3231 3232
hb04 3211 3212 3213 3214 3215 3216 3217 3218 3219 3220 3221 3222 3224 3225 3229
hb06 12001 1201 1202 1203 1204 3215 3216 3217 3218 3219 3221 3222 3224 3225 3226
hd02 1000 3200
hd03 3201 3202 3203 3204 5003 5004 5005 5008 5009 5010 5011 5012 5014 5015 5016
hd04 3007 3008 3009 3200
hd06 3002 3004 3005 3007 3204
hn02 5001 5002 5003 5009 5010 5011 5012 5013 5015 5016 5017 5018

--------------------

---------------

./reclaimer -manul_raft_conf=10.72.174.5:8550:0,10.72.174.7:8550:0,10.72.172.19:8550:0 -manul_client_IsPrintLog=true -reclaimer_ip=10.72.174.7 -reclaimer_port=32500 -bieti_raft_conf=10.72.174.7:39000:0 -reclaimer_log_path=/var/log/bieti/reclaimer -reclaimer_id=0 -segdump_concurrent_transactions_per_session=1

----------------

测试环境:
10.72.172.17
10.72.172.19

segment 300

前缀: /bieti

---------------------

reclaimer(10.72.172.17):

./reclaimer \
  -manul_raft_conf=10.72.174.5:8550:0,10.72.174.7:8550:0,10.72.172.19:8550:0 \
  -manul_client_IsPrintLog=true \
  -reclaimer_ip=10.72.172.17 \
  -reclaimer_port=32500 \
  -bieti_raft_conf=10.72.172.17:39000:0 \
  -reclaimer_log_path=/var/log/bieti/reclaimer \
  -reclaimer_id=0 \
  -reclaimer_path_prefix=/bieti \
  -reclaimer_log_level=debug \
  -reclaimer_concurrent_transactions_per_session=1

----------------

comet(10.72.172.17):

/root/bieti/comet/comet \
  -bieti_comet_ID=0 \
  -bieti_comet_IP=10.72.172.17 \
  -bieti_comet_ManagerPort=37001 \
  -bieti_comet_IOPort=37002 \
  -bieti_comet_IsForeground=false \
  -bieti_raft_conf=10.72.172.17:39000:0 \
  -bieti_raft_port=39000 \
  -manul_raft_conf=10.72.174.5:8550:0,10.72.174.7:8550:0,10.72.172.19:8550:0 \
  -manul_client_IsPrintLog=true

参数:
  -bieti_comet_DataFileSizeLimit 调整data file最大大小。

-------------------
./segdump \
  -segdump_layout_file_path=./layout-seg300.json \
  -segdump_path_prefix=/bieti \
  -manul_raft_conf=10.72.174.5:8550:0,10.72.174.7:8550:0,10.72.172.19:8550:0 \
  -segdump_segment_id=300 \
  -segdump_force_overwrite=true

  -segdump_content_file_path=./content \
--------

盘: 
10.72.172.19 sdn segment300
10.72.172.19 sdo segment301 

-----------

500870 行

-------------

29_0
28_0
27_0
26_0
25_0
24_0

---------

ll | grep -E '^d' | awk '{print $9}' | xargs -I{} sh -c 'echo {}; ssh `head -n 1 {}/host_info` realpath /root/udisk/chunk/chunk 2>&1 | grep -vE "Warning|Authenticated"'

-----------------

[root@test03-udisk-172-17 manul]# ./utility.sh file du /bieti/segment_300/
Conflicting CPU frequency values detected: 2400.120000 != 2329.459000. CPU Frequency is not max.
Conflicting CPU frequency values detected: 2400.109000 != 2331.095000. CPU Frequency is not max.
du /bieti/segment_300/

Path: /bieti/segment_300/
Total Dir  Num: 2
Total File Num: 549
Total Logical  Size:      43460408600
Total Physical Size:      51422167040

[root@test03-udisk-172-17 manul]# ./utility.sh file du /bieti/segment_300/
Conflicting CPU frequency values detected: 2509.041000 != 2400.996000. CPU Frequency is not max.
Conflicting CPU frequency values detected: 2459.842000 != 2553.628000. CPU Frequency is not max.
du /bieti/segment_300/

Path: /bieti/segment_300/
Total Dir  Num: 2
Total File Num: 51
Total Logical  Size:      21583712364
Total Physical Size:      22464692224

---------------

gc_session: weak_ptr*: 0x1b02c40

p (*(bieti::reclaimer::GcSession*)(0x7f6b103fe020)).segment_id_

----------------

p *((bieti::comet::IndexFileManager::LoadIndexFileContext *) ctx._M_ptr)->index_files._M_impl._M_start[9]

--------------

# 10.72.172.19 GC 测试盘, IO Error 后的处理方法

1. 首先停止 GC 启动脚本。在 testmanager 上执行 ps -aux | grep round_gc.sh 如果存在进程, kill 掉。

2. 按需重启reclaimer。一般情况下可以不重启reclaimer, 但如果GC对测试有影响, 就在 10.72.172.17 上 ps -aux | grep reclaimer, kill 掉, 然后执行 /root/bieti/reclaimer/start_reclaimer.sh 。

3. 按需重启comet。

4. 按需重启spdk_gate(在 10.72.172.19 kill 掉 spdk_gate, 然后执行 nohup /root/udisk/spdk_gate/spdk_gate -c /root/udisk/spdk_gate/conf/spdk_gate.ini & )。

5. 只要发生过IO Error, 就最好重新挂盘。在10.72.172.19 上按顺序执行下列4条命令:

  # 卸载iscsi块设备  
  iscsiadm --mode node -T iqn.2016-06.io.spdk:bruce --portal 10.72.172.19:13903 -u

  # host删除节点 
  iscsiadm -m discoverydb -t sendtargets -p 10.72.172.19:13903 -o delete

  # 探测iscsi块设备  
  iscsiadm -m discovery -t sendtargets -p 10.72.172.19:13903  

  # 登陆iscsi块设备  
  iscsiadm --mode node -T iqn.2016-06.io.spdk:bruce --portal 10.72.172.19:13903 --login

6. 挂盘成功后, 需要确定segment 300对应的盘符。此前segment 300的盘符是sdn, 但现在变成了sdo。为了防止写错盘, 最好确认一下。

在10.72.172.19上执行 dmesg -T | tail -n 50 可以看到如下打印:

> TODO 图片

可以看到, 有2个volume, UCLOUD_volume-0 和 UCLOUD_volume-1。其中volume-0是segment300, volume-1是segment301。

同时也可以看到, 有2个盘符 sdo 和 sdp。因为sdo 先于 sdp 出现, 因此sdo是volume-0对应的盘。

然后 lsblk 确认一下盘确实存在。

7. 执行 fio。下述命令会同时进行随机读写。将其中的盘符替换为第6步中确认的盘符。

fio -direct=1 -ioengine=libaio -group_reporting -bsrange=4k-64k -iodepth=16 -rw=randrw -randrepeat=0 -norandommap -time_based -runtime=7200 -name=fill_sdo -filename=/dev/sdo -numjobs=4 -time_based -runtime=7200

命令会持续读写 7200 秒, 可以修改命令中的`runtime`项来修改时长。

fio最好放在screen中运行, 方便随时登上去检查IO是否掉零。

8. 启动GC。

进入 testmanager 的 /data/bruce.li/tools/bieti-reclaimer-test

启动 GC 有2种方式, 一种是只启动一次GC, 执行 ./start_gc.py 。

还有一种是定期的启动GC, 则是执行 nohup ./round_gc.sh & 。默认的启动周期是半小时(1800秒) 一次, 可以修改。

9. 运行一段时间后, 需要确认是否发生了 IO Error。这里可以通过2种方法确认。

一种是观察screen中的fio有没有打印`Input/output error`。

另一种方法是 dmesg -T | tail -n 50, 观察最新的内核日志中是否有`I/O error`打印。

-----------

fio \
  -direct=1 \
  -ioengine=libaio \
  -group_reporting \
  -bsrange=4k-64k \
  -iodepth=16 \
  -rw=randrw \
  -randrepeat=0 \
  -norandommap \
  -time_based \
  -runtime=7200 \
  -name=fill_sdo2 \
  -filename=/dev/sdo2 \
  -numjobs=4 \
  -time_based \
  -runtime=172800

--------

bond0: 10.67.165.87
net2: 10.67.145.78

----------

洛杉矶/新加坡/台北/香港/hb/hd/hn

la01 singapore02 singapore03 taibei02 ap02 ap03

hb03 hb04 hb05 hb06
hd02 hd03 hd04 hd06
hn02 

--------

{
  "action":"CreateSystemUDisk",
    "level":"info",
    "msg":"create system udisk",
    "req":
    {
      "Action":"",
      "Backend":"",
      "top_organization_id":62935358,
      "organization_id":63365037,
      "region_id":7001,
      "zone_id":7001,
      "az_group":1000003,
      "Region":"cn-gd",
      "ProjectId":"org-ql5zhx",
      "request_uuid":"6c523644-25d5-44ac-9a98-47388e20b32f",
      "Zone":"cn-gd-02",
      "Size":100,
      "Name":"usmc-sys-disk1",
      "DiskType":"RSSDSystemDisk",
      "PodId":"",
      "ChargeType":"Month",
      "Quantity":0
    },
    "session":"1751170a-c9e9-4f80-99ed-c99db7f54cf6",
    "source":"/go/src/git.ucloudadmin.com/prj_smc/smc/src/manager/ucloud/udisk.go:61",
    "time":"2023-02-02T14:53:14.058871537+08:00"
}

{
  "err":"ucloud api param check failed",
    "file_path":"/go/src/git.ucloudadmin.com/prj_smc/smc/src/manager/manager/m_task_disk.go",
    "func_name":"manager.CreateSystemUDisk",
    "hostID":"",
    "level":"error",
    "msg":"create system udisk failed",
    "session":"1751170a-c9e9-4f80-99ed-c99db7f54cf6",
    "source":"/go/src/git.ucloudadmin.com/prj_smc/smc/src/manager/manager/m_task_disk.go:153",
    "time":"2023-02-02T14:53:14.058967853+08:00"
}

--------

Hi, 帮忙把这台主机 172.18.133.217 锁一下吧，这个不知道是哪个业务的，上面的udisk gate版本太老，io发不下去电话告警了。

--------

# 2023-02-07
1. 能否在index tail中加入cdp enable标志位。
2. 接收请求 -> 创建新盘 -> 通知comet创建新盘目录 -> 交换盘信息 -> 更新快照链以及快照链版本 -> 通知comet更新快照链 -> **通知gate重新挂载** -> 创建快照成功
3. 考虑是否支持不经过快照, 直接从盘进行克隆(被克隆的盘处于卸载状态下)。
4. 建立新的文件类型, 记录parent segment的数据索引?

# 2023-02-07
1. 能否在index tail中加入cdp enable标志位。
2. 接收请求 -> 创建新盘 -> 通知comet创建新盘目录 -> 交换盘信息 -> 更新快照链以及快照链版本 -> 通知comet更新快照链 -> **通知gate重新挂载** -> 创建快照成功
3. 考虑是否支持不经过快照, 直接从盘进行克隆(被克隆的盘处于卸载状态下)。
4. 建立新的文件类型, 记录parent segment的数据索引?

"leslie","leslie","leslie","clare","clare","clare","clare"

"13:00","13:20","14:00","18:00","18:51","19:30","19:49"


------------

1. 快照新索引, 方案。
2. 开关cdp切不切segment, GC的相关逻辑。
3. 先打快照, 再开cdp, 再cdp回滚。如果此时切了segment, 那还允不允许再回到原先的快照。

--------
1. cdp 随时间推移, 秒级->小时级->天级。
2. index tail中带上时间戳, 记录本文件内的最早的一次写入和最后的一次写入。
3. cdp开启期间, metaserver的GC请求需要给一个timestamp列表。这个列表会作为GC的barrier, 阻止跨时间节点的merge。
4. 重试IO, 时间戳需要变化。Comet需要保证时间的递增。
5. CDP回滚, 切segment。
6. 回滚后, CDP状态默认开启。这样用户开启一次cdp, 可能会产生多个segments。

-------------

1. 重定向IO加速: 因为工期问题, 暂时不做。设置每块盘的最大镜像数量。// 重定向读引起写(考虑做读写overlap限制)
2. 锁和排他性问题: 包括快照创建期间的挂载、路由无效问题等
3. DB修改: extern_id; 快照、镜像也存在volume表中, 以enable位区分; volume中加上用于表示原盘extern id的项, 方便从快照volume直接找到原盘volume。
4. 创建volume的 DB操作顺序和删除顺序。 //?
5. 用户不可用的extern_id。  // UUID
6. 重定向关系的保存: 是以volume为单位保存还是以segment为单位保存?
  I. 为了保证回滚逻辑的正确性, 回滚完成后volume ID必须改变。因此, 如果按volume为单位保存重定向关系, parent项必须是extern id
  II. 以volume为单位保存, 创建时的操作顺序不变设置: "创建segments", "插入VolumeRoutes", "插入Volumes", "修改volume间的映射关系"。
  III. 是否需要保证extern id可复用? 如果需要, 那么在删除一个volume时, 就需要修改其extern id为一个其他值。这样删除快照时, DB操作的原子性将无法保证。
7. 上传镜像怎么实现? // 暂时不考虑

-----------

## 1. 流程调整

I. 使用read重定向来解决快照/镜像的创建和回滚效率问题。后续使用其他手段来优化这种方案带来的网络和延时开销。

II. Read重定向指, 在读取一段数据时, 如果这段数据在当前segment中无法找到, 就前往这个segment的parent segment中寻找。如果在parent segment中也找不到, 就去parent segment的parent segment中找。依次递推, 直到找到所需数据, 或者在整条链的最后一个segment(即没有parent segment的segment)中仍然找不到数据, 就结束流程。

III. Read重定向信息记录在DB的segment表中。而快照和盘的对应关系则记录在DB volume表中。这样可以使得快照关系和IO重定向关系解耦, 方便后续优化。

IV. 现有实现中, 每个comet只需要知晓其自身所负责的segment的id就可以工作。现在, 因为重定向的加入, 每个comet都需要知道集群中所有segments所属的comets, 以及每个comet的地址。这样才能完成数据的转发。

V. 考虑comet 1, 2, 3, 4分别负责segment 1, 2, 3, 4。且有重定向关系segment 1 <- segment 2 <- segment 3 <- segment 4。

我们假设comet 4收到了一条对segment 4的读请求, 所读取的数据位于segment 1中。此时, 需要在comet间进行IO转发。

我们有2种方案可选:

方案一 逐层转发:

重定向链上各segment所在的comet向其parent segment所在的comet转发请求:

```mermaid
sequenceDiagram
  participant Client
  participant Comet 4
  participant Comet 3
  participant Comet 2
  participant Comet 1

  Client ->> + Comet 4 : 请求读取数据
  Note over Comet 4 : 发现本segment中数据不存在, 转发给parent segment所在的comet
  Comet 4 ->> + Comet 3 : 请求读取数据
  Note over Comet 3 : 发现本segment中数据不存在, 转发给parent segment所在的comet
  Comet 3 ->> + Comet 2 : 请求读取数据
  Note over Comet 2 : 发现本segment中数据不存在, 转发给parent segment所在的comet
  Comet 2 ->> + Comet 1 : 请求读取数据
  Note over Comet 1 : 在本segment中找到了数据
  Comet 1 -->> - Comet 2 : 返回数据
  Comet 2 -->> - Comet 3 : 返回数据
  Comet 3 -->> - Comet 4 : 返回数据
  Comet 4 -->> - Client : 返回数据
```

方案二 由单个comet负责转发:

爬链、转发都只有收到client请求的comet负责:

```mermaid
sequenceDiagram
  participant Client
  participant Comet 4
  participant Comet 3
  participant Comet 2
  participant Comet 1

  Client ->> + Comet 4 : 请求读取数据
  Note over Comet 4 : 发现本segment中数据不存在, 转发给parent segment所在的comet
  Comet 4 ->> + Comet 3 : 请求读取数据
  Note over Comet 3 : 发现本segment中数据不存在
  Comet 3 -->> - Comet 4: 返回数据不存在
  Note over Comet 4 : 找到parent segment的parent segment, 转发请求
  Comet 4 ->> + Comet 2 : 请求读取数据
  Note over Comet 2 : 发现本segment中数据不存在
  Comet 2 -->> - Comet 4 : 返回数据不存在
  Note over Comet 4 : 继续找更上一层的parent segment, 转发请求
  Comet 4 ->> + Comet 1 : 请求读取数据
  Note over Comet 1 : 在本segment中找到了数据
  Comet 1 -->> - Comet 4 : 返回数据
  Comet 4 -->> - Client: 返回数据
```

我们采取方案二。

VI. 因为上文中我们采取了方案二, 所以需要对读取请求进行分类。一类读取请求是client发向comet的读请求, 此类请求要求在数据不存在时重定向。另一类是comet向comet转发的请求, 这类数据要求数据不存在时不进行重定向。

## 2. 协议调整
### 2.1. Comet Heartbeat PB:

014b05a47938394e95af53b6343b5acf41713276

```diff
diff --git a/proto/bieti.341000.343000.proto b/proto/bieti.341000.343000.proto
index b2a52c3a6..757bc478e 100644
--- a/proto/bieti.341000.343000.proto
+++ b/proto/bieti.341000.343000.proto
@@ -332,6 +332,7 @@ message CometInfoPb {
 message SegmentInfoPb {
     required uint32 segment_id = 1;
     required uint32 comet_id = 2;
+    optional uint32 parent_segment_id = 3;
 };

 enum VolumeState {
@@ -560,14 +561,14 @@ message SegmentStatisticInfoPb {
 message CometHeartbeatRequest {
     required uint32 cluster_version = 10;
     required uint32 comet_id = 20;          // 由哪个comet上报
-    repeated SegmentStatisticInfoPb segment_stats = 30;
+    repeated uint32 segments = 40;          // 这里使用40. 原先的30项计划被用于GC调度的参数, 但后来决定不再参考此值, 因此删除。
 }

 message CometHeartbeatResponse {
     required ResponseCode rc = 10;
     optional uint32 cluster_version = 20;
     repeated CometInfoPb infos = 30;
-    repeated VolumeRouteInfoPb volume_route = 40;
+    repeated SegmentInfoPb segments = 50; // 这里使用50. 现行方案中comet不必知晓volume和segment的对应关系, 因此删除40项。
 }

 message ReclaimerHeartbeatRequest {
```

### 2.2. Relogin PB
TODO: 还需要client部分的人员参与设计

### 2.3. Read IO Cmd:

因为IO转发均收敛于收取了gate IO的comet, 因而需要在现有的gate IO请求中加入字段, 用于标记读请求是否需要重定向。

f1343f0afe85b5974758e5f94f1079ef05de2330

```diff
diff --git a/common/io_proto.h b/common/io_proto.h
index da7d36d..f623496 100644
--- a/common/io_proto.h
+++ b/common/io_proto.h
@@ -46,14 +46,20 @@ struct IORequestHead {
   uint32_t magic;
   uint64_t ctime;    // 用于cdp的场景
   uint64_t flow_no;
-  uint32_t lc_id;    // 用于快照的场景
   uint32_t route_version;
   uint8_t  cmd;
-  uint8_t  reserved[7];
+  uint8_t  flags;
+  uint8_t  reserved[6];
   char     data[0];
 } __attribute__((packed));
 constexpr size_t kIORequestHeadSize = sizeof(IORequestHead);
 
+constexpr size_t kRedrReadFlagOffset = 0u;
+constexpr uint8_t kRedrReadFlag = 1u << kRedrReadFlagOffset;
+constexpr inline static void SetRedrReadFlag(uint8_t *flag) { *flag |= kRedrReadFlag; }
+constexpr inline static void ClearRedrReadFlag(uint8_t *flag) { *flag &= ~kRedrReadFlag; }
+constexpr inline static bool IsRedrRead(uint8_t flag) { return flag & kRedrReadFlag; }
+
 struct IOResponseHead {
   uint32_t offset;
   uint32_t length;
@@ -61,7 +67,6 @@ struct IOResponseHead {
   uint32_t magic;
   uint64_t ctime;
   uint64_t flow_no;
-  uint32_t lc_id;
   int32_t retcode;
   uint8_t  cmd;
   uint8_t  reserved[7];
@@ -74,7 +79,7 @@ inline std::string DumpReqHead(const struct IORequestHead& head) {
   std::ostringstream oss;
   oss << "\nIORequestHead {" << "cmd:" << (uint32_t)head.cmd
       << " seg_id: " << head.seg_id << " flow_no: " << head.flow_no
-      << " lc_id:" << head.lc_id << " offset:" << head.offset
+      << " flags:" << head.flags << " offset:" << head.offset
       << " length:" << head.length << " ctime:" << head.ctime 
       << " route_version:" << head.route_version << " magic:" << head.magic << "}";
   return oss.str();
@@ -84,8 +89,7 @@ inline std::string DumpResHead(const struct IOResponseHead& head) {
   std::ostringstream oss;
   oss << "\nIOResponseHead {" << "retcode:" << head.retcode << " cmd:" << (uint32_t)head.cmd
       << " seg_id: " << head.seg_id << " flow_no: " << head.flow_no
-      << " lc_id:" << head.lc_id << " offset:" << head.offset
-      << " length:" << head.length << " ctime:" << head.ctime 
+      << " offset:" << head.offset << " length:" << head.length << " ctime:" << head.ctime
       << " magic:" << head.magic << "}";
   return oss.str();
 }
```

## 3. 控制流(metaserver)调整


----------

BdevNameIdPair (UUID->Vol ID的映射PB?)

---------

新增PB:

```protobuf
enum VolumeProperty {
    VOLUME_PROPERTY_MOUNTABLE = 0,
    VOLUME_PROPERTY_SNAPSHOT = 1,
    VOLUME_PROPERTY_IMAGE = 2,
};

// 用于从盘创建快照和从盘创建镜像
// 如果是从盘创建镜像, 那么创建后, 盘仍然可挂载
message DuplicateVolumeRequest {
    required string src_uuid = 10;
    required string name = 20; // 新volume的盘名, 由用户给定
    required string comment = 30; // 新volume的comment
    VolumeProperty type = 40; // 表明本次是创建快照还是创建镜像。只能是SNAPSHOT或IMAGE, 不能是MOUNTABLE
};

message DuplicateVolumeResponse {
    required ResponseCode rc = 10;
    optional string volume_uuid = 20;
}

message RollbackVolumeRequest {
    required string volume_uuid = 10;
    required string snapshot_uuid = 20;
}

message RollbackVolumeResponse {
    required ResponseCode rc = 10;
}

// 删除快照的功能放入DeleteVolumeRequest/DeleteVolumeResponse, 不另设pb

message ListSnapshotsRequest {
    required string volume_uuid;
}

message ListSnapshotsResponse {
    required ResponseCode rc = 10;
    repeated VolumeInfoPb snapshots = 20; // 用来存快照列表
}

message CloneVolumesRequest {
    required string src_uuid = 10;
    repeated CreateVolumeRequest new_volumes = 20;
}

message CloneVolumesResponse {
    required ResponseCode rc = 10;
    repeated string new_volume_uuids = 20;
}

// TODO: Re-login的PB
```


[17,6,13,19,8,15,22,9,1,3,7,1,9,18,16,22,9,8,7,6,5,3,4,4,4,1]
[17,6,13,19,8,15,22,9,1,3,7,1]
[17,6,13,19,8,15]

---------

1.
名称: 参与UDisk新迁移项目的开发
职责: 负责其中的迁移调度模块(trigger)的开发; 负责整个迁移项目的白盒测试
时间: 2021.07~2021.10
成果: 项目上线后, 使得云盘可以在可用区间跨RDMA Pod迁移
2.
名称: UDisk RSSD后端服务启动优化
职责: 研究并实现降低RSSD后端服务(xstore/chunk)启动耗时的手段
时间: 2021.10~2021.11
成果: 使启动耗时降低约0.6s, 研究并指出后续的优化方向
3. UDisk RSSD支持RDMA Read/Write
职责: 优化后端服务(XStore/chunk)代码, 测试性能
时间: 2021.11~2022.01
成果: 使得裸金属机器使用智能网卡接入UDisk服务并使用RSSD成为可能
4.
名称: UDisk 快速批量克隆功能
职责: 设计并实现IO路径
时间: 2022.01~2022.07
成果: 最终使得用户快速大批量创建并拉起虚机成为可能
5.
名称: UDisk 后端服务大页内存使用量过高问题的解决
职责: 协助研究、优化大页内存的使用量
时间: 2022.07~2022.08
成果: 使得每个RSSD后端进程的大页内存使用量稳定在5GiB左右
6.
名称: 私有化统一存储项目-块存储服务
职责: 参与系统设计、设计并实现GC、快照和克隆功能
时间: 2022.07至今
成果: 项目仍在开发阶段

---------

1.
名称: 《迁移模块接口说明文档》
介绍: 列出了允盘迁移管理模块(trigger)所提供的接口(PB)及其参数

2.
名称: 《SPDK/DPDK大页内存申请分配流程的研究》
介绍: 介绍了SPDK/DPDK框架对大页内存的申请和管理流程


--------------

1.
名称: 在组内会议中简介内部快照/批量克隆的IO流程
对象: 帮助组内同事了解内部快照/批量克隆的IO流程

-------------

1. 

---------

[2023-3-20 13:2:8.218174] [DEBUG] | IResizeUDisk | 483f07f9-cd62-4ee3-ba54-2f5d13a90369.4 |
Recv head:{version:2 magic_flag:305400199 random_num:521770498 flow_no:499722 session_no:"483f07f9-cd62-4ee3-ba54-2f5d13a90369.4" message_type:113017 worker_index:0 tint_flag:false source_entity:0 dest_entity:0 call_purpose:"" }
body:{[ucloud.ubs2.get_ubs_detail_info_response]:{rc:{retcode:0 error_message:"success" } lc:{id:"bsr-grcompi02lg" extern_id:"bsr-grcompi02lg" lc_name:"data_disk_UHost_rssd_resize_for_doblue_cores" create_time:1679288429 thrown_time:0 size:20 status:0 mount_status:30 mount_vm_id:"uhost-grcoltlarl0" mount_device_name:"vdb" account_id:180036 company_id:83 disk_type:4 version:3 utm_status:10 utm_mode:0 utm_modify_time:0 snapshot_count:0 cmk_id:"" data_key:"" isolation_group:"" disk_iops:2800 disk_bw:75 sub_disk_type:0 owner:"" multi_attach:false host_product:"" inner_udisk:false share_udisk:false channel:0 lc_id:30531 lc_random_id:398182378 across_pod_attach:false } logic_zone:"" pod_id:"666003001_25GE_D_R001" set_id:5000 region_id:"pre03" } }  - api_udisk/src/logic/context.PbRequestByIp:context/pb_request.go:91

---------

10.179.78.207 <--> 10.179.75.67
10.179.78.207 <--> 10.179.76.136
10.179.78.207 <--> 10.179.77.131
10.179.78.207 <--> 10.179.75.208
10.179.78.207 <--> 10.179.75.69

-------

core 190 191

------------

10.68.146.131
10.68.146.156
10.68.146.133
10.68.146.194
10.68.146.143
10.68.146.158
10.68.146.152
10.68.146.132
10.68.146.197
10.68.146.140
10.68.146.139
10.68.146.157
10.68.146.150
10.68.146.196
10.68.146.149
10.68.146.141
10.68.146.148
10.68.146.130
10.68.146.195
10.68.146.162
10.68.146.136
10.68.146.138
10.68.146.154
10.68.146.137
10.68.146.134
10.68.146.198
10.68.146.199
10.68.146.144
10.68.146.161
10.68.146.142
10.68.146.145
10.68.146.155
10.68.146.160
10.68.146.208
10.68.146.204
10.68.146.206
10.68.146.207
10.68.146.201
10.68.146.200
10.68.146.202
10.68.146.203
10.68.146.205
10.68.146.212
10.68.146.213
10.68.146.214
10.68.146.215
10.68.146.216
10.68.146.211
10.68.147.205
10.68.147.198
10.68.147.196
10.68.146.217
10.68.147.202
10.68.147.201
10.68.147.200
10.68.147.199
10.68.147.197
10.68.147.213
10.68.147.212
10.68.147.211
10.68.147.210
10.68.147.209
10.68.147.208
10.68.147.207
10.68.147.206
10.68.147.203
10.68.147.194
10.68.146.218
10.68.146.219
10.68.146.220
10.68.147.195
10.68.147.204
10.68.162.199
10.68.162.200
10.68.162.201
10.68.162.202
10.68.162.203
10.68.162.212
10.68.162.211
10.68.162.210
10.68.162.209
10.68.162.208
10.68.162.207
10.68.162.205
10.68.162.206
10.68.162.204
10.68.162.221
10.68.162.219
10.68.162.218
10.68.162.217
10.68.162.216
10.68.162.215
10.68.162.214
10.68.162.213
10.68.162.194
10.68.162.196
10.68.162.195
10.68.162.197
10.68.146.159
10.68.162.220
10.68.158.142
10.68.158.130
10.68.158.140
10.68.158.135
10.68.162.198
10.68.158.132
10.68.158.137
10.68.158.9
10.68.147.214
10.68.158.8
10.68.158.2
10.68.158.4
10.68.158.131
10.68.158.3
10.68.158.141
10.68.158.138
10.68.158.133
10.68.158.134
10.68.158.136
10.68.158.139

---------

resource_id: b94dda37-f671-458b-a2f7-09a6318191c9
volume_id: 0
segmentL 0

----------

resource id: b94dda37-f671-458b-a2f7-09a6318191c9
volume id: 1
segment id: 1


snapshot resource id: ecf30192-e8f4-4645-8b66-1cdc9a0da26f
volume id: 2
segment id: 0

---------

clone success: 863a2b42-8b9b-418b-b211-673a116ecc7f

----------
-----------

new volume: fb1f4676-eb02-4085-a427-81aaa2014dee
volume id: 12
segment id: 12 13

snapshot id: e7c459e9-714a-4426-bd4b-a84eeeac7b83
volume id: 13
segment id: 10 11

------
rollback:
new volume: fb1f4676-eb02-4085-a427-81aaa2014dee
volume id: 14
segment id: 14 15

snapshot id: e7c459e9-714a-4426-bd4b-a84eeeac7b83
volume id: 13
segment id: 10 11

------------
clone:

new volume: 35694654-df4f-4b2e-a90b-093f293e39fa
volume id: 15
segment id: 16 17

new volume: 2a9774af-88c3-44e3-9e94-dd04ca70d5a5
volume id: 16
segment id: 18 19

new volume: cf044692-b9c5-4617-8323-241e7e5a03e2
volume id: 17
segment id: 20 21

-----------------------
-----------------------

res id: 3e9556d3-d600-4de0-9f98-3dd7ce5c3dd4
vol id: 3
segment id: 2000003

----------
duplicate:

res id: 3e9556d3-d600-4de0-9f98-3dd7ce5c3dd4
vol id: 4
segment id: 2000004
 
snap res id: a7b8dc10-b49a-48ca-8194-bd820f8318c0
vol id: 5
segment id: 2000003

---------
rollback:

res id: 3e9556d3-d600-4de0-9f98-3dd7ce5c3dd4
vol id: 7
segment id: 2000006

snap res id: a7b8dc10-b49a-48ca-8194-bd820f8318c0
vol id: 5
segment id: 2000003

---------

resize:

res id: 3e9556d3-d600-4de0-9f98-3dd7ce5c3dd4
vol id: 7
segment id: 2000006(2000003) 2000008

----------

duplicate

res id: 3e9556d3-d600-4de0-9f98-3dd7ce5c3dd4
vol id: 9
segment id: 2000011(2000006) 2000012(2000008)


snap res id: 160cf051-96c1-4f97-add0-429dff464def
vol id: 10
segment id:  2000006(2000003) 2000008

----------

clone:

e65bd608-ce93-44b0-a218-276d17b429a5
26cee62a-6052-474e-bd17-57a8103d3970
98bf24e3-23e2-48f5-981d-b720d96af569

-----------

res id: fbae441f-a810-4996-a2dd-b4c6726c243e
vol id: 0
segment: 2000000

--------

duplicate:

res id: fbae441f-a810-4996-a2dd-b4c6726c243e
vol id: 1
segment: 2000001

snap res id: 30eaeb48-6c35-4475-bccc-156aeef4da04
vol id: 2
segment: 2000000

snap res id: 384dd6cb-913f-4580-a15e-6f5855a71b14
vol id:
segment: 

-----------

res id: 39da224f-d1a4-4c96-8ebf-d96a87b06bd2
vol id: 0
segment: 2000000

------------

duplicate

res id: 39da224f-d1a4-4c96-8ebf-d96a87b06bd2
vol id: 1
segment: 2000001

snap res id: c3a7f733-2df7-47de-9c70-e3ac3599adbc
vol id: 2
segment: 2000000

------

duplicate again
res id: 39da224f-d1a4-4c96-8ebf-d96a87b06bd2
vol id: 3
segment: 2000002

snap res id: fd7233bf-0a3e-4615-82ba-a6c1a7fd5576
vol id: 4
segment: 2000001

--------
rollback

res id: 39da224f-d1a4-4c96-8ebf-d96a87b06bd2
vol id: 5
segment: 2000003(2000001)

--------
clone

res id: 7ca81ae7-9d48-4a7b-9d9c-dafd5867dd41
res id: 1d603a03-13c6-465f-8af7-e7484d86251d



-----------
-----------

create vol:
res id: dcc8f653-1fb6-4a66-b4ec-f29e9902427a

--------

duplicate
snap res id: 473daf90-6617-439d-b152-86bd6956912b

---------

rollback

---------

clone:

new res id: 3f0d5b5d-d670-4965-9477-a43cb6e387b9
new res id: d279162d-bc92-4be3-8b96-72029f1f565f

---------

duplicate:

f0f2e902-b325-4ed3-94e5-33d8c955354f


--------
automatic
res id: dcc8f653-1fb6-4a66-b4ec-f29e9902427a
orig snap id: 8775aedf-1fe1-4f8a-9a9a-fbe8d0b03673



------------


res id: 41c07845-9030-44de-a93e-8376611a04ce

-----------

duplicate

snap res id: d9baaaa1-ba3e-42f6-addf-941a809bf503

--------

(gdb) p bieti::shuttle::g_context.cdp_stack_->eventloops_
$29 = std::vector of length 5, capacity 8 = {0x7f75b4001430, 0x7f75a8001430, 0x7f75ac001430, 0x7f75a0001430, 0x7f7598001430}
(gdb) p ((uevent::EventLoop *) 0x7f75b4001430)->loop_handle_
$30 = (bieti::shuttle::CdpLoopHandle *) 0x7f75b4020330
(gdb) p ((uevent::EventLoop *) 0x7f75a8001430)->loop_handle_
$31 = (bieti::shuttle::CdpLoopHandle *) 0x7f75a8020330
(gdb) p ((uevent::EventLoop *) 0x7f75ac001430)->loop_handle_
$32 = (bieti::shuttle::CdpLoopHandle *) 0x7f75ac020330
(gdb) p ((uevent::EventLoop *) 0x7f75a0001430)->loop_handle_
$33 = (bieti::shuttle::CdpLoopHandle *) 0x7f75a0020330
(gdb) p ((uevent::EventLoop *) 0x7f7598001430)->loop_handle_
$34 = (bieti::shuttle::CdpLoopHandle *) 0x7f7598020330


----------------

res id: 55840f70-c8bd-4cc8-8fa3-1c30ac859a4b


  Flags from /data/peng.li/Bieti/tools/ddump/ddump_gflags.cc:
    -ddump_data_file_path (Path to the data file in manul) type: string
      default: ""
    -ddump_force_write (Writes to the output file even if it exists, its
      original data will be removed) type: bool default: false
    -ddump_output_path (Path to store the output json result) type: string
      default: ""
    -ddump_pretty_write (Write the output json with indentation, make it
      human-readable) type: bool default: true


--------------

create: d5742391-18ed-4afd-93e9-131063d40c83

--------

duplicate

res id: d5742391-18ed-4afd-93e9-131063d40c83
vol id: 5
seg id: 2000004

snap res id: 35c8ad51-d919-47d5-986d-1fc09e5cc0cd
snap vol id:6
snap seg id: 2000003

----------------
----------------

res id: 8ac44cc1-8ad3-4fea-8321-b3968c3767a7

sdn
----------

duplicate

------

00813320


-----------

res id: 1ab1edd3-0a46-4867-b45d-c1cfbc9309c6

snap res id: da64a766-666a-4bbf-a79d-712d781de422

--------------

res id: a5c62947-340a-47d7-b5a7-3d7b51a7ac2b

----------

res id: 8a59d57d-d60f-4697-a76e-1e206e0a63a1
snap res id: 1903fdcc-fe78-4cba-b98a-1940d0cd822b


----

159 160 X

81919

1

---------------

0000000000000000000000000000000000001101001001000101001101010111
0000000000000000000000000000000000000010001011010110011110010101


-----------

134 clone test:

current seg id: seg_id: 1000057 1000058
current volume id: 57
current res id: 077d1554-8c58-477d-a6e1-33b2a2f0c1ce



------------

error sector: 128+74310656
error 4k unit: 9288848
segment: 1000059


----------

ssh-keygen -t rsa -f /etc/ssh/ssh_host_rsa_key
ssh-keygen -t ecdsa -f /etc/ssh/ssh_host_ecdsa_key
ssh-keygen -t ed25519 -f /etc/ssh/ssh_host_ed25519_key

----------

Host *
  ServerAliveInterval 30
  ServerAliveCountMax 60

Host bieti7.hui.sun
  HostName 172.20.68.5
  User root
  Port 29971
  ProxyCommand /usr/bin/nc -X 5 -x 127.0.0.1:16328 %h %p
  ControlMaster auto
  ControlPath   ~/.ssh/%h-%p-%r
  ControlPersist yes


~/.ssh/authorized_keys


-----------

docker run -d --name bieti7.hui.sun --hostname bieti7-hui-sun -p 29971:22 -v /data/hui.sun/data:/data --shm-size=1G -it bieti-hui-sun-v2




-------------


{
        "listen": ":16328",
        "remote": "172.20.68.5:37397",
        "password": "IdOy3/mfhyIXMAh2vJ5RDTqFgLAZmwoYMh7cSr+IBtB6dLlS40GZT/dZW61Mzrgqs3lqQmuo6V7g7AsbJXLltPNELYPvsVCa7f3aZ0g8ftfGgW4EK+b1Ptl4YpxVaedJ1MU1S7YTNoLHP8uUZPv/xFfN26t/J5JF3rdlYWD8qswjoywSbYkPc6F3Y+EUcPJAuwHqVvBOjZURfBrIR1Ok8TG+Jjm12PSQaAwWDql99ujWMzdcupdmB6yWcf7VA4tYitJUdYbiOK9v0QKMHEMpbACdH5PJEKXC+E3uX8rkKDT6FQl73Y9GmKCnos/DWi/B65EFHY6uPb0gJF0uO6bAhA=="
}



-----------

gate: 有读写间的check overlap

使用copy on read

空数据是否copy on read

多层重定向读, 不能每层都copy （修改协议）



----------
d27816f0-9e94-4c00-9625-fd9a204d5ac3

d670f8f8-01a9-4be0-a6d7-bbc15e62061c

-----------------

cd422709-70a2-4e1f-b08f-212c4f0ec580
33222c98-3701-4d9d-ad39-7397e78b73cf

------------------

3d0e1eab-e58d-4e22-81a2-e2d7e2a5eff6
iqn.2016-06.io.spdk:clone-resize-01

aa90e85a-9ce5-4bc3-b1d7-0e8c18172bf3

clone:
d5659134-f634-4e60-a15d-b8baffcff94f

f5db4948-73a0-48a9-bd21-085dc61292fa

a58655a1-810c-46e6-9d04-90fe47792552



--------

1. 保证不再会对此segment 发起GC
2. HaltGC
3. comet拉取任务列表时, 返回空
4. 通知comet停止任务删除。要求response返回后, 所有正在进行的删除都结束。


-----------

172.30.145.18
flow_no: 1098878309078401024
seg_id: 10191
:27006



-------

72057594037927936

-------

400509:20230423 19:02:14.515858Z 35215 ERROR Client Session is not Exist, maybe client hang or metaserver failover - manager_handle.cc:267

----------

create table if not exists t_reclaimer_info (reclaimer_id PRIMARY KEY, ip, portA

--------

失败原因是内存池里的对齐设置不正确。

RDMA memory region必须要2M对齐, 但是这里是默认值4k。
你可以尝试在100行之后加上 `option.mempool_option.align = 2 << 20;`, 看看还有没有问题。

这台机器上有些环境设置和我们这边的不大一样。

-------

-reclaimer_concurrent_transactions_per_merging_task=1024
-reclaimer_read_iops=50000
-reclaimer_read_bw_MBPS=200

----------

-reclaimer_gc_threads=5

11192


import os, re
segment_id = 11171
dir = '/var/log/bieti/reclaimer'
# regex_str = 'completed_segments: <' + str(segment_id) + '>'
# regex_str = 'completed_segments: ' + str(segment_id)
regex_str = 'completed_segments: ' + str(segment_id) + '$'
reg = re.compile(regex_str)

for root, dirs, files in os.walk(dir, topdown=False):
  for name in files:
    if name.endswith('gz'):
      continue
    path = os.path.join(dir, name)
    print "searching in " + path
    with open(path, 'r') as f:
      for line in f:
        if reg.search(line):
          print "Match!"
          print line


-----------

segment id: 10820

Miss Append File Locks

./comet-9/comet.20230425-102955.rg10-test-control003.ostack.hfb3.iflytek.net.3452.log:5:20230425 


./comet-11/comet.20230425-112848.rg10-test-control003.ostack.hfb3.iflytek.net.52460.log:450456:20230425 11:31:25.011866Z 52467 ERROR Client Session is not Exist, maybe client hang or metaserver failover - manager_handle.cc:267

Client Session Data: renew_interval: 20 ttl: 5 last_renew_time: 1682394106 session id : 5fc65997-114b-426e-b9d0-48e4be30162b current_time : 1682395207 - client_manager.cc:695

find . -name '*.log' -mmin -60 -exec grep -nw --with-filename 'ERROR' {} \; > comet_log.0425.log.txt

grep -vE 'Clear|file lock is lost|Peer|Miss|Duplicate' comet_log.0425.log.txt | less

Client Session is not Exist

10591

--------

383aa2f5-ee74-4059-b938-96f2ad2951c4
lc id:  746
segment id: 11195

-----------

segment

ps -aux | grep bieti_metaserver | grep -v 'grep' | awk '{print $2}' | xargs kill -9

 ps -aux | grep bieti_metaserver

ps -aux | grep comet | grep -v 'grep' | awk '{print $2}' | xargs kill -9

mv spdk_gate.dog.conf.bak spdk_gate.dog.conf

cat /sys/devices/system/clocksource/clocksource0/available_clocksource

echo hpet > /sys/devices/system/clocksource/clocksource0/current_clocksource

ps -aux | grep manul_metaserver | grep -v grep | awk '{print $2}' | xargs kill -9


-------------

原盘: f496004f-746a-439c-a14e-60f9e59abdff 762 11206
snap: 240d9d5c-d812-43ae-b1b6-563f0e741410 763 11205

628e1cd7-725c-40fe-993c-64b4a4f82624 765 11208
cb256ef2-bb3b-4487-acef-170741200f87 767 11210
72cf9bf6-53ad-4740-808c-c9a7d86bd293 764 11207
e46f304d-91e7-48c2-9a49-9ce05aa6f3fd 766 11209


-------------

公有云-批量克隆项目:

背景:
部分可用区偶发用户短时间内大批量创建虚拟机的场景, 并发数量可达上千台。每次虚机创建流程需要从镜像系统克隆一张系统盘到块存储系统, 大批量克隆的耗时高、负载大。因此需要开发新的批量克隆方案, 以支持短时间内的大批量克隆。


主要贡献:
采用redirect-on-write方案, 在尽量减少对虚机系统盘性能表现的影响的前提下, 设计后端服务中的读写流程, 并编码实现和测试。

项目成果:
成功达成设计目标, 系统可以在5分钟内完成1000台以上的虚拟机创建任务。且用户使用期间未能感知到性能下降。
使用此系统, 仅创建单台虚机, 耗时可以缩短至毫秒级。

-----------------

公有云-云盘迁移系统:

背景:
每个可用区拥有数个相互独立的块存储后端(set)。创建云硬盘时, 系统会根据用户提供的各类参数, 选择不同的后端set, 在其中创建云盘。随着可用区内各用户不断进行云盘创建、删除、扩容操作, 各set的负载也不再均衡。因此, 需要设计一套系统, 提供将云盘在set间迁移的功能。

主要贡献:
参与设计迁移功能的主要逻辑, 包括发起、监控、完成、中止、容灾等, 保证迁移流程无数据丢失风险, 失败后可重入。
参与迁移管理模块的编码工作。


项目成果:
成功完成项目设计目标。目前迁移系统已完成7万张盘的迁移工作。


------------------
私有云块存储系统:

背景:
该系统在兄弟部门的分布式追加写系统的基础上, 实现私有化分布式块存储系统。
项目目前仍处于早期开发阶段。

主要贡献：
1. 结合下层追加写系统的特点, 讨论设计系统的具体架构, 包括数据分区方案、索引设计等。参与各功能实现方案的讨论和研究, 包括IO流程, 快照、克隆功能, 连续数据保护功能等。并规划系统所需的功能模块, 以及互相间的交互流程。
2. 研究并设计GC逻辑, 包括GC的具体工作逻辑、与其余模块的交互、容灾流程等。并负责编码实现和测试。

项目成果:
1. 基本功能目前测试均通过。
2. GC模块在模拟环境中可以达到1000MiB/s的GC带宽。

































































10.67.13.226:13600
10.67.13.226:13600

wlcb 5012
10.77.97.157

chunk, cosmos, freyr idun, hela

515d5f3f-44bf-45f9-a799-71b23780cbbd


------------
1.
UpdateTaskSuspendedCb 里面:
```c++
  trigger_handle_->delete_migrate_task(master_extern_id_);
  trigger_handle_->migrate_udisk_task_erase(extern_id_);
```
把这2行移进: CbToResetAllGatesMigrate()

2.
ResetAllGatesMigrate(): 删除其中的migrate_udisk_task_erase()

----------

create_volume:

541a7070-b564-460e-bd93-20e3c4d84561

snapshot:
89d8bcbf-63b9-491e-9514-a818af01cc0f 1684812548
9add4065-62c0-414c-b98a-b0525561b11d 1684813484

----------

d769eec6-8255-43f3-8eff-47e85e7bd415

snapshot:
337a2c77-6783-497c-9a82-3cbcc144ab18 1684823827

-------------

34cb26ed-08f6-4bbf-adda-39bba92f07ab

snapshot:
c52f1d7b-0db0-4aa1-89d3-c74c8ed78c89 1684825274

----------

47a7d27a-b574-462d-9c15-df6e4e10c24f

snapshot:

afd988e8-0670-4f91-8bd3-8c3bceaf5e14
1684826348

-------------

dbebbea8-6e65-4d8e-849c-3d24a2b7ff4e

snapshot:
e6a432c0-4310-485b-b774-2968ca172ee6 1684827729

----------

991118bd-f23f-449f-9445-aa111fc3c2c4

snapshot:
2e2c9418-5ced-4249-868f-3f4521baf8b0 1684829394

------------

259e30db-a382-455b-b651-919fc2e9ad24

snapshot:
2b7660ee-9384-4543-af0c-0ef6f066ba5c 1684831876

------------

59b8b78f-15e9-42c1-bdb9-cdc0f662144f

snapshot:
1f819a02-073b-4d3f-85f4-bd4f84e3195a
1684898267

--------

194d36c1-9e47-4cf1-8f30-c5a0ce269238

snapshot:
1a3f7f49-14c5-48d2-90b8-987b2647c176
1684911473

------

bacef7ab-a04d-4897-b0d1-2db9d6d80614

snapshot:
b967763b-e06d-494a-ab32-1989934aeed4
1684917398

-----------

c8772af3-83ac-40a9-908a-10b6f8152c12

snapshot:
e8f2b095-3f34-455e-b3e8-373d71312efa
1684918016

-----------

53087847-f589-42c3-b351-d271bfe78035

snapshot:
b3714860-d7db-4031-8677-e410b616f630
1684930808

-------------
parent:
ee64eaa8-122e-42a9-becc-92197ca104f8
2000026
2000027

child:
566f78fd-c21a-4a3e-ad27-110d11f197aa
2000028
2000029

child snapshot:
9da2903a-4048-4605-be04-ace067f41d55
1685002230

-----------

783d7531-d476-40f1-9a7d-8527f5e358ed

11d47f92-2e82-4ea6-b0df-df6d9113ee85
1685071919

daf0c7c8-390e-4c84-8c04-b122eab88b81
1685073083

-----------

cb6c2186-8366-4def-af91-2342734b9dd2

image: 49a927e0-db9c-49cc-bf26-6a7a4e9788f1

------------

cdp 回滚开始时, 原vol的last tick需要设置, 新vol的start tick需要设置

-------------
reclaimer-23.06.14-55c9551-centos7-x86_64-haswell
371M -> 373M -> 373M -> 325M -> 330M -> 262M(266M) -> 6月15日早(226M) -> 246M

--------

58bb885b-4f04-4398-9ed5-c0d051bc8d80
target_node: iqn.2016-06.io.spdk:snapshot-1 sdn

before snapshot: 1687163522
snapshot id: ffc65f5d-87a9-44a1-b743-f8172d4ac5f4
after snapshot: 1687163575

before snapshot: 1687165734
snapshot id: 59d75d50-b620-4a09-811d-dbce10c72ee5
after snapshot: 1687165811


------------
795c6baf-f73a-4684-8927-752f06a54e39
target node: iqn.2016-06.io.spdk:snapshot-1

before snapshot: 1687168744
snapshot_id: e4515aff-6b62-4c8a-a097-5d2b3929a628
after snapshot: 1687169151

before snapshot: 1687169546
snapshot_id: 14c52ca0-1c88-4b9e-a4f3-fc7f4bb39d1d
after snapshot: 1687169620

------------------

a37d91fc-6af1-4e30-9d2e-0423dfef2509
target_node: iqn.2016-06.io.spdk:snapshot-1

sdn

before snapshot: 1687171961
snapshot_id: bafc9561-7dce-41b7-b9bc-6bdfce55f63d
after snapshot: 1687172021

before snapshot: 1687172282
snapshot_id: e76463dc-671d-45fa-a8a9-9095dd6214cd
after snapshot: 1687172309

------------

image_resource_id: efe2b82c-905c-4d93-bb3b-38343580ea09

new_resource: 745ca1c4-ba6a-400c-a656-b2379cf5b242
new_resource: eb0408fb-8dfd-4cce-9f57-4322526bda29

----------

image_id: 1d9b4347-8809-4cf3-82b4-e26eb8f91fc1
new_resource: 80cb02ce-6125-4e67-884d-c990ae97c690
new_resource: 42a398ed-449e-46c9-91ae-34503ac1627d

---------

image_id: b0c0eada-c551-4a2b-84c8-1974f04837f1
new_resource: e26c6075-ea8a-4374-b59b-c5846bf59277


==========

cdp:

disk: a37d91fc-6af1-4e30-9d2e-0423dfef2509
before: 1687259748
cdp rolback time: 1687259806
after: 1687259838



==========

resource: ae82428a-bdba-40e1-b3dc-5e85898c6f6f

before cdp: 1687264203
cdp rollback timestamp: 1687264580
after cdp: 1687264591

--------

image: 9d5372fd-d192-4c77-ad6a-cb791127c741
expected: 1687264203

wrong: 1687317341

cloned resource: c9d53319-5fec-4151-9190-747124eef312

--------------

image again: 2c7015e0-7476-4ccc-ac1b-1f3ca9ee65a4
expected: 1687264203

wrong: 1687319776

cloned resource: ebd6ddd4-054d-46c3-a808-519ef1871699

===========

200G resource: 33d164e5-6c76-4794-9dc0-61169d225b10

image resource id: 2c5af3c6-5ccc-42bb-8849-cb920d3739fa


auto_big_disk_test_cdp.py
bieti_cdp_auto_hot_test.py

--------

disk: 5ef8b27c-478f-42a3-9dca-9982f417d4b9

11:18:12~28

---------

create volume: 80dac308-7e1f-4e7a-a084-88f7a96d8016
before snap: 1687682509
snap: f20d3da5-0a94-4f55-95d3-02a8b51575ef
after snap: 1687682574

---------

cdp: 2a6be845-09e2-4b18-810a-063e0bb2c491


--------------

volume: 267d21af-832a-4757-b44e-5a581502e10a

snap: e3ab1078-d29f-4e26-bcfa-f689ccb469ee


---------
create image:

from snap:
  image: 07c782f1-24a4-4dcf-9936-97983054a824
  cloned volume: 93dbd436-11cc-4f16-a53a-147b9f102538

from volume:
  image: f9782f2d-9e72-4b16-979e-9549c1961ec6
  cloned volume: e58acae5-bce1-40e3-b7b3-1ad5a8afd122 e8310629-0708-41c6-9518-317517e556a0

from redirected volume:
  image: 99d43b34-60c7-4b77-aa16-7711345c0256
  cloned volume: df9549d2-bb83-48bb-b88b-4f72cb940512

------

TODO:
  01: udisk handle vec 改为 unordered_map 完成(是否需要改为基数树)
  02: pc handle vec 启用2段索引
  03: LocalAioRequest2
  04: IOHandleRdma::OnMessageRead2 完成
  05: IoProxy
  06: 全部结束后, 引入OpRequest2
  07: ResourceContext2的对象池需要放在ChunkLoopHandle 完成
  08: IOHandleRdma::FreeResourceContext2 完成
  09: CHUNK IO REQ处理流程
  10: 池化ResourceContext2 完成
  11: 取消 max_udisk_num 上限
  12: FillDataByZero2
  13: SubmitReadLocal2 完成
  14: SubmitWriteLocal2
  15: UDiskHandle::LocalAioCb2
  16: PCRead2
  17: 内存池, 加入以kPaddingSize为key的部分


---------

1. 能够承担具有挑战性的工作: 例如在新私有云Bieti项目中，参与了方案制定，承担了GC模块的开发任务等。
2. 主动发现提出灰色工作且乐意主动承担: 本人在Bieti项目的开发过程中, 主动编写了多个校验数据正确性、一致性的工具，且成功使用此工具发现过bug。其也可以在后续的一致性验证和长期测试中用于校验和调试。
3. 在Bieti开发过程中，完成了对ustevent中的rdma/spdk的依赖的剥离, 搭建了编译所需的docker image。保证了开发过程的顺利进行。
4. 在进行 ustevent 框架大页内存优化的过程中，和玉广一同对dpdk的大页管理进行了相对深入的研究, 并输出文档 (https://git.ucloudadmin.com/block/udisk_doc/-/issues/1641)。<D-x>


----------
业绩:
1. Bieti私有化块存储项目开发:  参与讨论设计数据删除方案, 参与讨论设计了新快照/镜像/CDP方案, 并参与编码实现和测试。
2. Bieti次优化块存储-讯飞测试项目: 主动参与各个模块(metaserver, reclaimer, gate, shuttle)的debug工作, 积极协助定位测试期间发现的问题, 并直接修改或给出修改意见。
3. 值班期间, 能够积极响应并解决spt所提出的问题和需求, 能够积极解决线上问题。
4. 积极为组内新人答疑解惑, 包括介绍公有云的云盘迁移逻辑、介绍Bieti reclaimer的工作逻辑, 协助搭建编译环境等。


------------

do {
  if (((padding_ctx)->list_entry_.tqe_next) != __null)
    (padding_ctx)->list_entry_.tqe_next->list_entry_.tqe_prev = (padding_ctx)->list_entry_.tqe_prev;
  else
    (paddings)->tqh_last = (padding_ctx)->list_entry_.tqe_prev;
  *(padding_ctx)->list_entry_.tqe_prev = (padding_ctx)->list_entry_.tqe_next;
} while ( 0)

----------

2:

TODO:
  01: udisk handle vec 改为 unordered_map  完成
  02: pc handle vec 启用2段索引 通过配置项: [common] pc_max_size_in_vec调整
  03: LocalAioRequest2 不需要
  04: IOHandleRdma::OnMessageRead2 不需要
  05: IoProxy 完成
  06: 全部结束后, 引入OpRequest2 不需要
  07: ResourceContext2的对象池需要放在ChunkLoopHandle 不需要
  08: IOHandleRdma::FreeResourceContext2 不需要
  09: CHUNK IO REQ处理流程
  10: 池化ResourceContext2
  11: 取消 max_udisk_num 上限
  12: FillDataByZero2
  13: SubmitReadLocal 完成
  14: SubmitWriteLocal 完成
  15: UDiskHandle::LocalAioCb2
  16: PCRead
  17: 内存池, 加入以kPaddingSize为key的部分

-----------
/root/hao/mongodb-3.4.5/bin/mongod -f /root/hao/mongodb-3.4.5/mongod.conHanld
10.72.142.145:

mongo:
  iR/root/hao/mongodb-3.4.5/bin/mongod -f /root/hao/mongodb-3.4.5/mongod.conf

umongo:
  /root/hao/bin/umongo-2023.04.19-ae782cb -c /root/hao/umongo/umongo.conf

access:
  /root/hao/bin/access-23.04.19-a6f58bb-centos7-x86_64 -c /root/hao/access/access.conf

抹盘:

#!/bin/bash

./raw_chunk_storage_tool format /dev/nvme0 0 true false 4194304 true &
./raw_chunk_storage_tool format /dev/nvme1 1 true false 4194304 true &
./raw_chunk_storage_tool format /dev/nvme2 2 true false 4194304 true &

wait

0000:3d:00.0
0000:3e:00.0
0000:d8:00.0

-----------

盘的可用空间
盘的dev name
盘的uuid
盘的pcie地址
盘的chunk_id

---------

devops -> 块存储 -> 业务运维 -> 单盘qos调整, 改total_bytes_sec

先改这个, 改好再继续下一步

----------

fs/open.c:1298
SYSCALL_DEFINE3(open, const char __user *, filename, int, flags, umode_t, mode)

open()对O_CREAT:的处理: 会在 fs/namei.c的lookup_open()中, 调用 dir_inode->i_op->create, 因而fuse_lowlevel_operations中的open接口不需要处理 O_CREAT。

SYSCALL_DEFINE3(open ...)(fs/open.c) -> do_sys_open() -> do_sys_openat2() -> do_filp_open() -> path_openat() -> open_last_lookups() -> lookup_open()

------

O_TRUNC 应该也不需要fuse的open函数处理, 从逻辑看kernel会调用inode的setattr函数, 这和观察到的现象不同

----

24890 root      20   0  135.0g 657720 151220 S  0.0  0.2   0:00.47 pfs_thread
24934 root      20   0  135.0g 657720 151220 S  0.0  0.2   0:00.46 pfs_thread
24935 root      20   0  135.0g 657720 151220 S  0.0  0.2   0:00.47 pfs_thread
 3648 root      20   0  135.0g 657720 151220 S  0.0  0.2   0:00.06 pfs_thread
 3649 root      20   0  135.0g 657720 151220 S  0.0  0.2   0:00.06 pfs_thread

-----------

lc_id: 1689071944
pc_no: 118
lc_random_id: 847228023
pg_id: 3

pc meta:
is_used: 1
has_detection: 1
seq_no: 1206
pc_id: 51264
offset: 215324356608
pg_id: 3
lc_id: 1689071944
pc_no: 118
lc_random_id: 847228023
allocate_time: 1689133583
redirect_bits: 0

--------

20230712 14:19:30.088532Z 15947 DEBUG HandleGateMessage receive gate io request: [GateIORequestHeadV3]{  crc32: 0 size: 512 flowno: 87381475614871 flowno_56: 87381475614871 offset: 2048 length: 512 lc_id: 1689071945 lc_random_id: 478034945 lc_size: 1000 pc_no: 0 pg_id: 7 route_version: 3 pg_version: 0 pc_size: 4194304 cluster_uuid: 0 magic: 4045641850 fragno: 0 data_type: 2 sgl_size: 0 chunk_0_id: 0 chunk_1_id: 1 chunk_2_id: 2 cmd: 1 } - message_handle_rdma.cc:315

./test.sh /tmp/fuse/iotest_071401 1 iotest_01.list
./test.sh /tmp/fuse/iotest_071402 8 iotest_02.list
./test.sh /tmp/fuse/iotest_071403 32 iotest_03.list
./test.sh /tmp/fuse/iotest_071404 1 iotest_04.list

iotest_03: file id: 668504758993398? 703689131082230?
last flow_no: 703689131082230

iotest01 magic: 7250
iotest02 magic: 8061
iotest03 magic: 7690
iotest04 magic: 9023

-------

./spdk_gate-23.07.11-103667b-centos7-x86_64-native.20230714-141117.test03-uhost-142-3.32544.log:197420:20230714 14:11:19.294294Z 47019 DEBUG construct_udisk_ios_from_buff fuse allocate udisk io, flowno:77078217219206, offset_blocks:32, num_blocks32, share_idx:0, type:1, io_len:16384, file offset:16384 - pfs.cc:1411
./spdk_gate-23.07.11-103667b-centos7-x86_64-native.20230714-141138.test03-uhost-142-3.32544.log:87882:20230714 14:11:39.326032Z 47212 DEBUG construct_udisk_ios_from_buff fuse allocate udisk io, flowno:279574856935692, offset_blocks:32, num_blocks32, share_idx:0, type:0, io_len:16384, file offset:16384 - pfs.cc:1411

目标write req flow no: 77078217219206

对应chunk2:
Chunk2.20230714-141118.test03-udisk-142-145.25259.log.gz:161789:20230714 14:11:19.294340Z 25277 DEBUG HandleGateMessage receive gate io request: [GateIORequestHeadV3]{  crc32: 0 size: 16384 flowno: 36105875236183174 flowno_56: 77078217219206 offset: 16384 length: 16384 lc_id: 1689252027 lc_random_id: 612203759 lc_size: 1000 pc_no: 0 pg_id: 4 route_version: 3 pg_version: 0 pc_size: 4194304 cluster_uuid: 0 magic: 4045641850 fragno: 0 data_type: 2 sgl_size: 0 chunk_0_id: 2 chunk_1_id: 0 chunk_2_id: 1 cmd: 1 } - message_handle_rdma.cc:315

----------

spdk_gate-23.07.14-8ef840d-centos7-x86_64-skylake.20230714-153226.test03-uhost-142-3.47469.log:195398

Chunk2.20230714-153226.test03-udisk-142-145.3451.log.gz:248870

chunk0:
pc meta:
is_used: 1
has_detection: 1
seq_no: 5426
pc_id: 10
offset: 139563008
pg_id: 4
lc_id: 1689252027
pc_no: 0
lc_random_id: 612203759
allocate_time: 1689315079
redirect_bits: 0

chunk1:
pc meta:
is_used: 1
has_detection: 1
seq_no: 5429
pc_id: 13
offset: 152158208
pg_id: 4
lc_id: 1689252027
pc_no: 0
lc_random_id: 612203759
allocate_time: 1689315079
redirect_bits: 0

chunk2:
pc meta:
is_used: 1
has_detection: 1
seq_no: 5426
pc_id: 10
offset: 139563008
pg_id: 4
lc_id: 1689252027
pc_no: 0
lc_random_id: 612203759
allocate_time: 1689315079
redirect_bits: 0


--------

iotest_03:

      specdata {
        specdata1: 1689252027
        specdata2: 612203759
      }

      specdata {
        specdata1: 1689252027
        specdata2: 612203759
      }


---------

lc_id: 1689331396
lc_random_id: 1328104339


-----------

The return-value of the await_resume() method call becomes the result of the co_await expression. The await_resume() method can also throw an exception in which case the exception propagates out of the co_await expression.

Note that if an exception propagates out of the await_suspend() call then the coroutine is automatically resumed and the exception propagates out of the co_await expression without calling await_resume().

----------------

off cpu 火焰图抓取io_depth_limitDecQueueDepth:

perf record -F 99 -p <目标pid> -e sched:sched_stat_sleep -e sched:sched_switch -e sched:sched_process_exit -g -o perf.data sleep 120

perf script -i perf.data &> perf.unfold

FlameGraph/stackcollapse-perf.pl perf.unfold &> perf.folded

FlameGraph/flamegraph.pl --countname=ms --title="Off-CPU Time Flame Graph" --colors=io perf.folded > offcpu.svg


----------

on cpu 火焰图抓取:

perf record -a -F 99 -g -p <目标pid> sleep 10

perf script -i perf.data &> perf.unfold

FlameGraph/stackcollapse-perf.pl perf.unfold &> perf.folded

FlameGraph/flamegraph.pl --countname=ms --title="On-CPU Time Flame Graph" perf.folded > oncpu.svg

---------

iops低的原因推测:

对gate抓取 on cpu 和 off cpu 火焰图后, 发现 fuse 线程繁忙和空闲时都是主要卡在`mmap()`和`munmap()`中。

查阅内核代码后发现每个linux进程中同时只能有一个线程对进程的内存空间进行修改。因而当多个fuse线程同时进行`mmap()`和`munmap()`时, 只有一个可以进行, 其它必须等待。

这个问题直到目前最新的正式内核版本 (v6.4.4) 仍没有解决。


把page改回来?


------------

libaio/spdk io suspend/resume

CoWakeup();
CoLaunch();
CoScheduleOut();
CoSleepMs(); 单位毫秒
ThisCoContext();
CoSpmcQueue();

CoPromise
CoContext


CoEvent 组合 (必须为一个Awaiter)

PromiseType 需要实现:
get_return_object()
initial_suspend()
return_void()或return_value()
unhandled_exception()可选
final_suspend()可选

Awaiter

--------

STL六大组件的交互关系:
1. 容器通过空间配置器取得数据存储空间。
2. 算法通过迭代器存储容器中的内容。
3. 仿函数可以协助算法完成不同的策略的变化。
4. 适配器可以修饰仿函数。

---------

std::deque 组织buffer的结构也是一大段连续内存。

-----------

条件变量: 与wait queue类似, 在条件满足后, 由引发条件变化的线程通过 条件变量 来通知等待这个条件满足的线程。减少锁争用的开销。
但条件变量必须结合锁 一同使用, 目的是防止 条件变量的 触发事件丢失。

PV 原语是对信号量的操作，一次 P 操作使信号量减1, 一次 V 操作使信号量加1。

TIME_WAIT: 2MSL

慢启动: 当发送方每收到一个 ACK，拥塞窗口 cwnd 的大小就会加1。

慢启动算法，发包的个数是指数性的增⻓。1->2->4->8->2的次方。
有一个慢启动门限 ssthresh 状态变量:
1. 当 cwnd < ssthresh 时，使用慢启动算法。
2. 当 cwnd >= ssthresh 时，就会使用拥塞避免算法。

一般来说 ssthresh 的大小是 65535 字节。超过后会进入拥塞避免算法。那么进入拥塞避免算法后，它的规则是:每当收到一个 ACK 时，cwnd 增加 1 / cwnd 。

拥塞避免算法就是将原本慢启动算法的指数增⻓变成了线性增⻓，还是增⻓阶段，但是增⻓速度缓慢一些。

网络就会慢慢进入了拥塞的状况了，于是就会出现丢包现象，这时就需要对丢失的数据包进行重传。

当触发了重传机制，也就进入了拥塞发生算法A
-----------------

InnerUDisk-o5mc0ss4
3	wlcb01	10027_100GE_D_R001	5001	wlcb01	目标pod: 10027_25GE_D_R004	目标set: 5017	已暂停	0.00%	20	无需迁移	无需迁移	已暂停	hui.sun	2023-09-04 16:40:11	2023-09-04 16:45:40

22/12/23：控制台已标记售罄。可推荐用户使用RSSD资源，价格一样。 正常情况下海外除非有客户报备需求，否则不补充。 23/09/07: 临时打开供KUN业务创建，现已关闭  23/09/08: 临时打开供KUN业务创建，现已关闭

发生拥塞，进行数据重传:
1. 超时重传
2. 快速重传



-------------------

State Idle:
  when this state enters:
    must ensure:
      reading_buf == nullptr;
      writing_buf == nullptr;
      last_full_size_write == nullptr;
      waiting list is empty;
      in_flying list is empty;
    append this local_write_proxy to global idle local_write_proxy list; (unless this local_write_proxy just get initialized)

  when write comes:
    if this local_write_proxy is in global idle local_write_proxy list:
      move this local_write_proxy out;
    if write's size < padding_size:
      allocate reading_buf;
      if allocation failed:
        set this writing failed;
        enter State Idle again;
        return;
      append this write to waiting list;
      enter State Reading;
    else if write's size == padding_size:
      set writing_buf with this write's buf;
      set this write's is_buf_used_as_writing_buf_ with true;
      append this write to in_flying list;
      set last_full_size_write with this write;
      enter State Writing;
    else:
      abort;

State Reading:
  when this state enters:
    must ensure:
      reading_buf is not nullptr;
      writing_buf == nullptr;
      last_full_size_write == nullptr;
      waiting list is not empty;
      in_flying list is empty;
    start reading to reading_buf from disk;


  when write comes:
    if write's size < padding_size:
      append this write to waiting list;
    else if write's size == padding_size:
      append this write to waiting list;
      if last_full_size_write is not nullptr:
        set last_full_size_write's is_buf_used_as_writing_buf to false;
      set last_full_size_write with this write;
      set this write's is_buf_used_as_writing_buf with true;
    else:
      abort;

  when disk reading succeeded:
    define: LocalWriteTransaction cur;
    if last_full_size_write_ != nullptr:
      cur = last_full_size_write's next;
      set last_full_size_write with nullptr;
      set writing_buf with cur's buf;
      free reading_buf;
      set reading_buf with nullptr;
    else:
      cur = first element of waiting list;
      set writing_buf with reading_buf;
      set reading_buf with nullptr;
    while cur does not pass the end of waiting list:
      memcpy cur's buf into writing_buf;
      cur = cur->next;
    move elements of waiting list into in_flying list;
    enter State Writing;
    start writing writing_buf to disk;

  when disk reading failed:
    iterate all writes in the waiting list:
      set this write failed;
      if this write's is_buf_used_as_writing_buf == false:
        free this write's buf
      free this write;
    free reading_buf;
    set reading_buf with nullptr;
    set last_full_size_write with nullptr;
    append this local_write_proxy to global idle local_write_proxy list;
    enter State Idle;

State Writing:
  when this state enters:
    must ensure:
      reading_buf is nullptr;
      writing_buf is not nullptr;
      last_full_size_write is nullptr;
      waiting list is empty;
      in_flying list is not empty;
    start writing writing_buf into disk;
  
  when write comes:
    if this write's size < padding_size:
      append this write to waiting list;
    else if this write's size == padding_size:
      append this write to waiting list;
      if last_full_size_write != nullptr:
        set last_full_size_write's is_buf_used_as_writing_buf_ with false;
      set last_full_size_write with this write;
    else:
      abort;

  when disk write failed:
    iterate all writes in in_flying list:
      set this write failed;
      if this write's is_buf_used_as_writing_buf_ is false:
        free this write's buf;
      free this write;
    iterate all writes in waiting list:
      set this write failed;
      if this write's is_buf_used_as_writing_buf_ is false:
        free this write's buf;
      free this write;
    free writing_buf;
    set writing_buf with nullptr;
    set last_full_size_write_ with false;
    enter State Idle;

  when disk write succeeded:
    iterate all writes in in_flying list:
      set this write succeeded;
      if this write's is_buf_used_as_writing_buf is not true:
        free this write's buf
      free this write
    if waiting list is empty:
      free writing_buf;
      set writing_buf with nullptr;
      enter State Idle;
      return;
    define: LocalWriteTransaction cur;
    if last_full_size_write_ != nullptr:
      free writing_buf;
      cur = last_full_size_write's next;
      set last_full_size_write with nullptr;
      set writing_buf with cur's buf;
    else:
      cur = first element of waiting list;
    while cur does not pass the end of waiting list:
      memcpy cur's buf into writing_buf;
      cur = cur->next;
    move elements of waiting list into in_flying list;
    enter State Writing;
    start writing writing_buf to disk;

----------------------

1. 已完成 OpenPC, Redirect IO 去除
2. 已完成 Gate Read 逻辑的修改
3. 具体设计Write逻辑

----------------

21个

5103 timeout

-----------
Chunk1.20231103-143110.test03-nebula-174-11.13284.log: 25
20231103 14:31:10.683513Z 13290 DEBUG OnMessageRead MessageHeaderV2's version: 3, msg_type: 1, data_len: 128 - message_handle_rdma.cc:33



------------
chunk0:
pcie addr: 0000:d8:00.0

lc_id: 10788982627508304
pc_no: 0
offset: 4393453006848

lc_id: 10789073750864256
pc_no: 0
offset: 468713472

------------

Chunk2.20231107-165358.test03-nebula-174-12.12840.log +156467

问题IO起始:
Chunk2.20231107-165358.test03-nebula-174-12.12840.log +156695

-----------

class Employee(object):
    def __init__(self, name, salary, address):
        self.name = name
        self.salary = salary
        self.address = address
    def __iter__(self):
        yield 'name', self.name
        yield 'salary', self.salary
        yield 'address', dict(self.address)

class Address(object):
    def __init__(self, city, street, pin):
        self.city = city
        self.street = street
        self.pin = pin
    def __iter__(self):
        yield 'city', self.city
        yield 'street', self.street
        yield 'pin', self.pin

--------------

vim -R /var/log/upfs/chunk-0/Chunk0.20231108-202956.test03-nebula-174-10.20191.log +360440

buf frags:
0x6BD6670 local_write_elevator: <pc_id_: 2293204, index_in_pc_: 0, disk_offset_: 2407326412800, state_:kInit>
0x6BDE230 local_write_elevator: <pc_id_: 2293204, index_in_pc_: 1, disk_offset_: 2407326478336, state_:kInit>

-----------------

for file in `ls .`; do echo $file; grep "my_id" $file; grep "nvme_disk_addr" $file; done
10.80.143.9:
0: 0000:3b:00.0
1: 0000:3c:00.0
2: 0000:3d:00.0
3: 0000:3e:00.0
4: 0000:d8:00.0
5: 0000:d9:00.0
6: 0000:da:00.0
7: 0000:db:00.0

10.80.143.6:
8: 0000:3b:00.0
9: 0000:3c:00.0
10: 0000:3d:00.0
11: 0000:3e:00.0
12: 0000:d8:00.0
13: 0000:d9:00.0
14: 0000:da:00.0
15: 0000:db:00.0

10.80.143.8:
16: 0000:3b:00.0
17: 0000:3c:00.0
18: 0000:3d:00.0
19: 0000:3e:00.0
20: 0000:d8:00.0
21: 0000:d9:00.0
22: 0000:da:00.0
23: 0000:db:00.0

--------------

# /bin/bash
set -e

cd /root/upfs/chunk/tools/
nohup ./raw_chunk_storage_tool format <chunk_id>-<pc_id> <chunk_id> true true 1048576 true 0x1 &
nohup ./raw_chunk_storage_tool format <chunk_id>-<pc_id> <chunk_id> true true 1048576 true 0x2 &
nohup ./raw_chunk_storage_tool format <chunk_id>-<pc_id> <chunk_id> true true 1048576 true 0x4 &
nohup ./raw_chunk_storage_tool format <chunk_id>-<pc_id> <chunk_id> true true 1048576 true 0x8 &
nohup ./raw_chunk_storage_tool format <chunk_id>-<pc_id> <chunk_id> true true 1048576 true 0x10 &
nohup ./raw_chunk_storage_tool format <chunk_id>-<pc_id> <chunk_id> true true 1048576 true 0x20 &
nohup ./raw_chunk_storage_tool format <chunk_id>-<pc_id> <chunk_id> true true 1048576 true 0x40 &
nohup ./raw_chunk_storage_tool format <chunk_id>-<pc_id> <chunk_id> true true 1048576 true 0x80 &

wait
# 在 /root/upfs/format_log/ 查看抹盘进度

--------------

10.80.143.9:
0 41218a8c-41b4-4ae8-b568-d58aea75be19
1 a46fa247-199a-41e2-8855-9dbd04fcb1e8
2 ae128fec-bd88-47f5-b69c-c174dd90154c
3 814fb4c5-7b0a-4330-9816-3b5d7d3a2fdc
4 18a3a076-3563-40cc-b295-779d404c1a78
5 6ab8a494-249a-4254-868a-abed01af7ae0
6 de6c2b7e-b01b-439c-b4c3-eb61616ea855
7 e849de1e-dda9-4a56-a598-472451f63d1c

10.80.143.6:
10 869094f9-7bb0-4679-9c14-83033639e4f7
11 234d2b55-e832-46f9-bef9-50f62482f301
12 df10ffa2-c80c-4d4e-aa78-2facbf2434ff
13 405a172f-6651-4465-9e62-e10937fe6257
14 0b9b5e59-74ca-483b-9134-5d58fcbe5b40
15 90ce8846-c804-4978-bab7-774ed23f51dd
8 2c2fee50-dee2-404e-ba78-eb890d041e52
9 46dfe957-139a-4ede-b334-8b31a7b590c5

10.80.143.8:
16 7d2693b7-0913-4ea5-b092-adf8da400d01
17 af56ad2a-1810-4d8b-9d1b-0428987e9654
18 daba106b-f641-492c-979a-0ff608575ca1
19 e914c4b5-2699-416b-9d92-7d10bc2e4516
20 1ede6b59-94dc-4356-ba39-5741897cfa5b
21 f95e39a9-371a-43fc-a0f3-ea8051138990
22 95a25dce-c79a-4144-8ffa-2766b971571b
23 4f6c8f91-25fa-456d-bf88-e8fdfd1ffd66

--------------

An Awaiter type is a type that implements the three special methods that are called as part of a co_await expression:
  await_ready()
  await_suspend()
  await_resume()

---------------

template<typename P, typename T>
decltype(auto) get_awaitable(P& promise, T&& expr)
{
  if constexpr (has_any_await_transform_member_v<P>)
    return promise.await_transform(static_cast<T&&>(expr));
  else
    return static_cast<T&&>(expr);
}

template<typename Awaitable>
decltype(auto) get_awaiter(Awaitable&& awaitable)
{
  if constexpr (has_member_operator_co_await_v<Awaitable>)
    return static_cast<Awaitable&&>(awaitable).operator co_await();
  else if constexpr (has_non_member_operator_co_await_v<Awaitable&&>)
    return operator co_await(static_cast<Awaitable&&>(awaitable));
  else
    return static_cast<Awaitable&&>(awaitable);
}

"co_await <expr>" 会被大致翻译成如下代码:
{
  // 当前处于coroutine function中

  auto&& value = <expr>;
  // 如果当前promise有
  auto&& awaitable = get_awaitable(promise, static_cast<decltype(value)>(value));
  auto&& awaiter = get_awaiter(static_cast<decltype(awaitable)>(awaitable));
  if (!awaiter.await_ready())
  {
    using handle_t = std::experimental::coroutine_handle<P>;

    using await_suspend_result_t =
      decltype(awaiter.await_suspend(handle_t::from_promise(p)));

    // 在这里保存当前coroutine的状态, 包括保存coroutine frame和设置 coroutine 恢复时需要执行
    // 的指令的地址(也就是下方的<resume-point>的位置)。
    // 真正挂起当前coroutine的位置是下方的<return-to-caller-or-resumer>
    <suspend-coroutine>

    // 无返回值的await_suspend()一定会挂起当前coroutine
    // bool 返回值版本的 await_suspend() 则只会在返回值为true时挂起。
    // 因此可以使用bool版本的await_suspend()来处理 "等待的条件在事实挂起前就已满足"的情况。
    //
    // 如果 await_suspend()抛异常, 则会直接退回到上级函数, 后续的 await_resume() 将不会执行。
    // 当前coroutine在<return-to-caller-or-resumer>处挂起, 此时会返回到当前coroutine的上级函数。

    if constexpr (std::is_void_v<await_suspend_result_t>) {
      awaiter.await_suspend(handle_t::from_promise(p));
      <return-to-caller-or-resumer>
    } else {
      static_assert(
         std::is_same_v<await_suspend_result_t, bool>,
         "await_suspend() must return 'void' or 'bool'.");

      if (awaiter.await_suspend(handle_t::from_promise(p))) {
        <return-to-caller-or-resumer>
      }
    }

    // 如上方所属, 当前coroutine的恢复点在这里。
    <resume-point>
  }

  // 根据上方的代码可以看出, 事实上 co_await <expr> 不一定会挂起 (await_ready()返回true时不挂起)。
  // 不论是否挂起, 只要 await_suspend() 不抛异常, 下方的 await_resume()就一定会执行。
  // await_resume()的返回值就是本次 co_await <expr> 操作给当前coroutine 的返回值。
  // 如果await_resume()抛异常, 则也会向上方的 await_suspend()一样, 被直接抛给当前coroutine。
  return awaiter.await_resume();
}

----------

协程标准支持 添加了三个新的关键字：co_await、co_yield 和 co_return。

每当你在函数体中使用这些协程关键字之一时，这会触发编译器将此函数编译为协程，而不是正常的函数。

对于每次协程函数的调用，都会在协程帧内构造一个 promise 对象的实例。

在协程执行的关键点，编译器会生成对 promise 对象上某些方法的调用。

在以下示例中，假设在协程帧中为协程的特定调用创建的 promise 对象是 `p`。

当你编写一个协程函数，其主体为，并且包含协程关键字（co_return、co_await、co_yield）之一时，协程的主体将被转换为类似以下的内容（大致）：

{
  co_await p.initial_suspend();
  try
  {
    <body-statements>
  }
  catch (...)
  {
    p.unhandled_exception();
  }
FinalSuspend:
  co_await p.final_suspend();
}

--------------

--------------

--------------

(以下翻译自 cppreference):

## 限制

协程不能使用可变参数、普通的 `return` 语句或占位符返回类型（`auto` 或 `Concept`）。

`consteval` 函数、`constexpr` 函数、`构造函数`、`析构函数`和 `main 函数`不能是协程。

## 执行

每个协程都与以下内容相关联：

promise 对象，从协程内部操作。协程通过此对象提交其结果或异常。

coroutine handle (`std::coroutine_handle<XXX>` 和 `std::coroutine_handle<>`)，从协程外部操作。这是一个非拥有的句柄，用于恢复协程的执行或销毁协程帧。

协程状态 (coroutine state, 或协程帧 coroutine frame)，这是一个内部的 (和lambda一样, 具体类型信息不对编码人暴露)、
动态分配的存储对象（除非优化掉了分配, 优化的方法通常是将多个协程帧打包在一起一同分配, 这样多个协程帧只需要分配一次），包含以下内容：
+ promise 对象
+ 参数（全部通过值复制）
+ 当前挂起点的某种表示，以便恢复知道从哪里继续，销毁知道哪些局部变量在作用域内
+ 生命周期跨越当前挂起点的局部变量和临时变量。

当协程开始执行时，它会执行以下操作：

使用 `operator new` 分配协程状态对象。

1. 将所有函数参数复制到协程状态：按值参数被移动或复制，按引用参数保持引用(因此，如果在引用对象的生命周期结束后恢复协程，可能会变成dangling reference)。

2. 调用 promise 对象的构造函数。如果 promise 类型有一个接受所有协程参数的构造函数，那么将调用该构造函数，参数为复制后的协程参数。否则调用默认构造函数。

3. 调用 `promise.get_return_object()` 并将结果保留在局部变量中, 该调用的结果将在协程首次挂起时返回给调用者。在此步骤及之前抛出的任何异常都会向调用者传播，而不是放在 promise 中。(`promise.get_return_object()` 的返回值通常是/包含 一个 `std::coroutine_handle<Promise>`, 指向当前coroutine, 这样其他逻辑可以在条件合适时使用`coroutine_handle<Promise>::resume()`来恢复此协程)。

4. 调用 `promise.initial_suspend()` 并 `co_await` 其结果。标准库提供了2个预先定义的 Awaiter `std::suspend_always`和`std::suspend_never`。典型的 Promise 类型要么返回 `std::suspend_always`，用于延迟启动的协程，要么返回 `std::suspend_never`，用于立即启动的协程。

5. 当 `co_await promise.initial_suspend()` 恢复时，开始执行协程的主体。


当协程达到一个挂起点时，先前获取的返回对象(此前上方的`promise.get_return_object()`返回值) 在必要时隐式转换为协程的返回类型后，将返回给调用者/恢复者。

当协程达到 `co_return` 语句时，它会执行以下操作：

+ 调"用return"函数:
  + 对于 `co_return` 或 `co_return expr`（其中 `expr` 的类型为 `void`），调用 `promise.return_void()`
  + 对于 `co_return expr`（其中 `expr` 的类型为非 `void`），调用 `promise.return_value(expr)`
+ 按照创建它们的反序销毁所有具有自动存储期的变量。
+ 调用 `promise.final_suspend()` 并 `co_await` 其结果。

协程的结束等同于 `co_return`;，但是如果 `Promise` 类型没有 `Promise::return_void()` 成员函数，那么行为是未定义的。

如果函数体中没有协程相关的关键字出现(`co_await`, `co_yield` 或 `co_return`)，那么无论其返回类型如何，都不是协程，并且如果返回类型不是 `cv void` (我也不知道什么是`cv void`, 我理解和"函数定义中有返回值但函数体中没有return任何值"是一回事)，那么结束时的结果将是未定义的。

``` c++
// assuming that task is some coroutine task type
task<void> f()
{
    // not a coroutine, undefined behavior
}
 
task<void> g()
{
    co_return;  // OK
}
 
task<void> h()
{
    co_await g();
    // OK, implicit co_return;
}
```

如果协程以未捕获的异常结束，它将执行以下操作：

+ 在 catch-block 中捕获异常并调用 `promise.unhandled_exception()`
+ 调用 `promise.final_suspend()` 并 `co_await` 其结果 (例如, 恢复一个延续或发布一个结果)。从这一点恢复协程是未定义的行为。

当协程状态因通过 `co_return` 或未捕获的异常终止，或通过其句柄被销毁时，它会执行以下操作：

+ 调用 `promise` 对象的析构函数。
+ 调用函数参数副本的析构函数。
+ 调用 `operator delete` 释放协程状态使用的内存。
+ 将执行权转回给调用者/恢复者。

## Dynamic allocation

协程状态通过non-array的 `operator new` 动态分配。

如果 `Promise` 类型定义了类级替换，它将被使用，否则将使用全局的 `operator new`。

如果 `Promise` 类型定义了一个接受额外参数的 `operator new` 的放置形式，并且它们匹配一个参数列表，其中第一个参数是请求的大小（类型为 `std::size_t`），其余的是协程函数参数，那么这些参数将被传递给 `operator new` (这使得可以使用领先的分配器约定用于协程)。

如果满足以下条件，可以优化掉对 `operator new` 的调用 (即使使用了自定义分配器):

协程状态的生命周期严格嵌套在调用者的生命周期内，以及在调用点知道协程帧的大小。 在这种情况下，协程状态被嵌入到调用者的堆栈帧中 (如果调用者是一个普通函数) 或协程状态中 (如果调用者是一个协程)。
如果分配失败，协程会抛出 `std::bad_alloc`，除非 `Promise` 类型定义了成员函数 `Promise::get_return_object_on_allocation_failure()`。如果定义了该成员函数，分配使用 `operator new` 的 `nothrow` 形式，在分配失败时，协程立即将从 `Promise::get_return_object_on_allocation_failure()` 获取的对象返回给调用者，例如:

``` c++
struct Coroutine::promise_type
{
  /* ... */

  // ensure the use of non-throwing operator-new
  static Coroutine get_return_object_on_allocation_failure()
  {
    std::cerr << __func__ << '\n';
    throw std::bad_alloc(); // or, return Coroutine(nullptr);
  }

  // custom non-throwing overload of new
  void* operator new(std::size_t n) noexcept
  {
    if (void* mem = std::malloc(n))
      return mem;
    return nullptr; // allocation failure
  }
};
```

## Promise

`Promise` 类型是由编译器通过使用 std::coroutine_traits 从协程的返回类型中确定的。

正式地说，让 `R` 和 `Args...` 分别表示协程的返回类型和参数类型列表，`ClassT` 和 `cv-qual`（如果有的话）分别表示协程所属的类类型及其 `cv-qualification`，如果它被定义为非静态成员函数，那么它的 `Promise` 类型由以下方式确定：

+ 如果协程不是定义为非静态成员函数，那么由 `std::coroutine_traits<R, Args...>::promise_type` 确定，
+ 如果协程被定义为非静态成员函数且没有 `rvalue-reference-qualified`，那么由 `std::coroutine_traits<R, ClassT /*cv-qual*/&, Args...>::promise_type` 确定，
+ 如果协程被定义为非静态成员函数且是 `rvalue-reference-qualified`，那么由 `std::coroutine_traits<R, ClassT /*cv-qual*/&&, Args...>::promise_type` 确定。

For example:

| If the coroutine is defined as ... | then its Promise type is ... |
|------------------------------------|------------------------------|
| task<void> foo(int x); | std::coroutine_traits<task<void>, int>::promise_type |
| task<void> Bar::foo(int x) const; | std::coroutine_traits<task<void>, const Bar&, int>::promise_type |
| task<void> Bar::foo(int x) &&; | std::coroutine_traits<task<void>, Bar&&, int>::promise_type |

## co_await

一元运算符 `co_await` 挂起协程并将控制权返回给调用者。它的操作数是一个表达式，该表达式要么 (1) 是定义了成员运算符 `co_await` 的类类型，或者可以传递给非成员运算符 `co_await`，要么 (2) 可以通过当前协程的 `Promise::await_transform()` 转换为这样的类类型。

`co_await <expr>`

`co_await` 表达式只能出现在常规函数体内的可被评估表达式 (Potentially-evaluated expressions)中，不能出现在:

+ exception handler 中，
+ 声明语句中，除非它出现在该声明语句的initializer中 (也就是在声明变量时可以出现在等号右边, 作为右值使用)，
+ init-statement 的simple declaration中（参见 if，switch，for 和 range-for），除非它出现在该 init-statement 的初始化器中，
+ 函数的默认参数中，或者
+ 具有静态或线程级 storage duration 的 block-scoped 变量的initializer中。












-------------------

Some examples of a parameter becoming dangling:

``` cpp
#include <coroutine>
#include <iostream>
 
struct promise;
 
struct coroutine : std::coroutine_handle<promise>
{
    using promise_type = ::promise;
};
 
struct promise
{
    coroutine get_return_object() { return {coroutine::from_promise(*this)}; }
    std::suspend_always initial_suspend() noexcept { return {}; }
    std::suspend_always final_suspend() noexcept { return {}; }
    void return_void() {}
    void unhandled_exception() {}
};
 
struct S
{
    int i;
    coroutine f()
    {
        std::cout << i;
        co_return;
    }
};
 
void bad1()
{
    coroutine h = S{0}.f();
    // S{0} destroyed
    h.resume(); // resumed coroutine executes std::cout << i, uses S::i after free
    h.destroy();
}
 
coroutine bad2()
{
    S s{0};
    return s.f(); // returned coroutine can't be resumed without committing use after free
}
 
void bad3()
{
    coroutine h = [i = 0]() -> coroutine // a lambda that's also a coroutine
    {
        std::cout << i;
        co_return;
    }(); // immediately invoked
    // lambda destroyed
    h.resume(); // uses (anonymous lambda type)::i after free
    h.destroy();
}
 
void good()
{
    coroutine h = [](int i) -> coroutine // make i a coroutine parameter
    {
        std::cout << i;
        co_return;
    }(0);
    // lambda destroyed
    h.resume(); // no problem, i has been copied to the coroutine
                // frame as a by-value parameter
    h.destroy();
}
```


-------------
这是一篇翻译了一半的markdown文章，

请参考上下文，将此段按照原markdown格式翻译成中文, 以markdown语法的源码的形式展示
请参考上下文，将此段翻译成中文。结果需要保留原markdown 语法元素和缩进不变，以markdown语法的源码的形式展示结果

-----------

lc_id:
10791097491001840

lc_random_id:
183999781

--------------

lc_id: 10791530703893136
lc_random_id: 1410425157

db.t_remove_info.insertOne({
	"file_id" : NumberLong("10791530703893136"),
	"set_id" : NumberInt("5500"),
	"file_name" : "remove_exclusiveness_test",
	"create_time" : NumberInt("1701671010"),
	"file_rand_id" : NumberInt("663664653"),
	"file_size" : NumberInt("10485760"),
	"fs_id" : NumberInt("1"),
	"modify_time" : NumberInt("1701671010"),
	"file_state" : NumberInt("20"),
	"delete_time" : NumberInt("1701671011")
})

--------

fake one:

db.t_remove_info.insertOne({
	"file_id" : NumberLong("791530703893137"),
	"set_id" : NumberInt("5500"),
	"file_name" : "remove_exclusiveness_test",
	"create_time" : NumberInt("1701671010"),
	"file_rand_id" : NumberInt("663664653"),
	"file_size" : NumberInt("0"),
	"fs_id" : NumberInt("1048576"),
	"modify_time" : NumberInt("1701671010"),
	"file_state" : NumberInt("20"),
	"delete_time" : NumberInt("1701671011")
})

udisk::chunk::RawChunkPool::PCEntryPCMap::find



------------------

1. 人工修改uint32 lc_id/pc_no -> u64
2. 清除编译错误: 假设当前模块仅有一个IO线程，模仿RDMA文件，修改TCP文件
3. 支持内核态盘的IO
4. 支持多IO线程: PC -> Thread 哈希函数
5. 支持多IO线程: 收包,回包,哈希
6. 支持多IO线程: ResCtx/OpRequest 分配/回收
7. 支持多IO线程: 回收/修复流程

10.80.143.4
10.80.143.5
10.80.143.6
10.80.143.8
10.80.143.9
10.80.143.10
10.80.143.131
10.80.143.130
10.80.143.132
10.80.143.135
10.80.143.133
10.80.143.134

----------

10.80.143.6
10.80.143.8

-----

10791883017440080

---------

dbab183c4878a4fc94186c58331c102a5509701f

帮忙排查一下北京二C 以下机器在今天5:30~6:00间的RDMA网络情况吧，业务发现间歇性发包失败的情况:

源:
10.68.150.225

目的:
10.68.150.144
10.68.152.139
10.179.76.70
10.179.75.130
10.68.148.133
10.68.148.200
10.68.152.12
10.68.150.146
10.68.148.135

-----------

10.68.152.22
10.68.148.10
10.68.149.66
10.68.149.87
10.68.148.210
10.68.148.69
10.68.149.24

-----------

lcs_ 还要不要

LokiRepairChunkHandle::GetUDiskInfoFromMeta() LokiRepairChunkHandle::GetUDiskInfoFromMetaTimeout() LokiRepairChunkHandle::GetUDiskInfoFromMetaResponse() 还要不要
LcRepairHandle还要不要

DegradeWarningAndFinishRepair()
FinishRepairChunkRequest()
FinishRepairChunkResponse()
FinishRepairChunkTimeout()
UpdateRepairTaskStatus()
UpdateRepairTaskStatusResponse()
UpdateRepairTaskStatusTimeout()
DeleteRepairTaskJobs()


你如果布thor的话，我代码已经push到pfs分支了。配置文件你可以参考 10.77.9.77 /root/upfs/thor/conf/thor.conf

我看了崩掉的chunk-38, 里面有很多副本超时。
假设问题IO的收包loop是loop1, IO loop是loop2
如果副本超时，会在loop2调用ResourceContex::Destruct()。Destruct()会RunInLoop(), 回到loop1，在loop1调用BufFrag::TryFree()
同时，本地IO完成，在loop2中, elevator也会调用TryFree()。
这2个loop同时分别设置了 buf_frag->in_res_ctx_ 和 buf_frag->in_local_write_proxy_，然后都认为释放条件已经满足，所以两边都会调用BufFrag::Free()。








    
  
















-----------

Return the offset of the first free inode in the record. If the inode is sparsely allocated, we convert the record holemask to inode granul and mask off the unallocated regions from the inode free mask.




-------------

[common]
my_id                           = 0
my_set                          = 3500
my_name                         = /NS/upfs_tcp/set3500/thor
metaserver_timeout              = 5
is_auto_recycle                 = 1
pending_queue_size_limit        = 2000
heartbeat_metaserver_interval   = 5
recycle_concurrency_pc          = 100
recycle_concurrency_lc          = 20

[names]
my_set                          = 3500
metaserver                      = /NS/upfs_tcp/set3500/metaserver
global_umongo                   = /NS/upfs_tcp/umongo_gateway

[network]
listen_ip                       = 10.72.141.218
listen_port                     = 22500

[zookeeper]
server                          = 10.72.136.107:2181,10.72.136.108:2181,10.72.136.109:2181,10.72.136.110:2181,10.72.136.111:2181
global_server                   = 10.72.136.107:2181,10.72.136.108:2181,10.72.136.109:2181,10.72.136.110:2181,10.72.136.111:2181
timeout                         = 10

[log]
log_level                       = trace
log_path                        = /var/log/upfs/thor/thor-

-----------------------

1. 抹盘时 PC Size 可配(可以先写死成1M, 反正不能写4M)。
2. 删除"资源查询"接口，因为现在没有extern-id, 新的查询功能需要参考ufs的查询功能，需要和张柯/郭伟讨论
3. 监控查询界面: 调整资源监控界面，可能需要参考ufs的监控功能，需要和张柯/郭伟讨论；删除 “脏数据”项；删除“黑名单云盘”项；删除“挂载状态异常云盘”项，因为现在没有挂载操作；删除“白名单用户”项
4. 迁移系统暂时不用适配，这也是因为extern-id不存在，新的迁移模块还没有开发
5. 容量管理界面中的 超卖/超售 概念需要取消，因为lc的分配不是由access/metaserver决定的。同样，客户磁盘分布统计，单集群用户磁盘统计，用户存量信息 都需要删除。
6. 业务运维界面: 删除"快照操作"项；删除“挂载信息”项；删除 ”集群超卖“项；删除“业务组”项(?)；删除"单盘qos调整"项；调整“磁盘强制回收”项；现在只能通过set id + lc id的方式回收；删除“云盘备份”项。

----------

# 1. 入向限流

入向限流策略会限制应用程序能够收到的包的带宽，不过注意这不能限制这台*机器*所实际收到的带宽。

假设我们需要将入向端口为12345和12346、入向iface为bond0的流量总和限制为500MBps, 可以这么做:

``` bash
modprobe ifb

ip link set dev ifb0 up

tc qdisc add dev bond0 handle ffff: ingress

tc filter add dev bond0 parent ffff: protocol ip u32 match ip dport 12345 0xffff action mirred egress redirect dev ifb0

tc filter add dev bond0 parent ffff: protocol ip u32 match ip dport 12346 0xffff action mirred egress redirect dev ifb0

tc qdisc add dev ifb0 root handle 1: htb default 10

tc class add dev ifb0 parent 1: classid 1:10 htb rate 500mbps
```

下面介绍下各命令的作用:

## 1.1  加载ifb模块

``` bash
# 这条命令每台机器重启后只用跑一遍
modprobe ifb
```

## 1.2 设置ifb0设备

根据tc相关文档描述，使用tc ingress限速，功能有限，似乎只能选择丢弃，并且也不支持分类。实际应用中，我们可以将业务流重定向到ifb设备上，业务流从这个ifb设备中出去，再又相应的端口接收，那我们就可以像正常使用tc对egress限速一样，来对ifb设备进行egress限速，就可以达到对接收方向的限速了。具体原理可以参考最下面列出的文档。

``` bash
# 执行前最好先ip link show看一下ifb0是否已经up,
# 如果已经up就不用再执行一次了
ip link set dev ifb0 up
```

## 1.3 为需要限流的if创建入向队列规则

```bash
# 这里假设需要限流的iface为eth0, 根据实际需要修改
# 如果有多个iface要限制, 可以把1.3和1.4多做几次, 每次把eth0换成一个你需要的iface
tc qdisc add dev eth0 handle ffff: ingress
```

## 1.4 过滤需要限流的包
现在需要创建过滤规则, 将目标iface上的相应包重定向到ifb0, 然后再在ifb0上限流

我们可以同时创建多条过滤规则, 各条过滤策略之间则为"或"关系

每条过滤规则内部可以有一个或多个过滤条件，这些条件之间为"与"关系。

下方演示各种过滤策略

``` bash
# 将来自10.98.12.11和10.98.12.12的流量重定向到ifb0
tc filter add dev eth0 parent ffff: protocol ip u32 match ip src 10.98.12.11 action mirred egress redirect dev ifb0
tc filter add dev eth0 parent ffff: protocol ip u32 match ip src 10.98.12.12 action mirred egress redirect dev ifb0

# 将发送至本地端口 12345 的流量重定向到 ifb0
tc filter add dev eth0 parent ffff: protocol ip u32 match ip dport 12345 0xffff action mirred egress redirect dev ifb0
# 将源ip为10.59.248.136, 且目标端口为本地12345的流量重定向到ifb0
tc filter add dev eth0 parent ffff: protocol ip u32 match ip src 10.59.248.136/32 match ip dport 12345 0xffff action mirred egress redirect dev ifb0
```

这里的过滤规则基本遵循这种语法:

> `tc filter add dev <iface> parent ffff: protocal ip u32 <match xxx> <match xxx> action mirred egress redirect dev ifb0`

如上所述, 一条过滤规则中可以有一个或多个过滤条件, 也就是 `<match xxx>`。下面`<match xxx>`的语法示例:

| 语法 | 解释 |
|------|------|
| match ip src 10.98.12.11 | 源ip为 10.98.12.11 |
| match ip dst 10.98.12.12 | 目标ip为 10.98.12.12 |
| match ip dst 10.98.12.0/24 | 目标ip前缀为 10.98.12 |
| match ip sport 12345 0xffff | 源端口为 12345, 其中的0xffff是掩码, 只有掩码中对应位为1的位置需要匹配。因为端口只有16位, 所以用0xffff |
| match ip sport 8192 0xfff0 | 源端口为 8192~8207, 因为掩码的低4bit为0, 所以只匹配端口号的高12bit。这种策略一般很少使用 |
| match ip dport 12345 0xffff | 目标端口为 12345 |
| match u32 0 0 | 匹配所有包 |

## 1.5 在ifb0设备上设置限流规则
``` bash
tc qdisc add dev ifb0 root handle 1: htb default 10
tc class add dev ifb0 parent 1: classid 1:10 htb rate 200mbps
```

示例将符合过滤条件的流量总带宽限制到 200mbps。除了mbps外，你也可以使用如下单位:

| 单位 | 解释 |
|------|------|
| bit | Bits per second |
| kbit | Kilobits per second (1000进制) |
| mbit | Megabits per second (1000进制) |
| gbit | Gigabits per second (1000进制) |
| tbit | Terabits per second (1000进制) |
| kibit | Kilobits per second (1024进制) |
| mibit | Megabits per second (1024进制) |
| gibit | Gigabits per second (1024进制) |
| tibit | Terabits per second (1024进制) |
| bps | Bytes per second |
| kbps | Kilobytes per second (1000进制) |
| mbps | Megabytes per second (1000进制) |
| gbps | Gigabytes per second (1000进制) |
| tbps | Terabypes per second (1000进制) |
| kibps | Kilobytes per second (1024进制) |
| mibps | Megabytes per second (1024进制) |
| gibps | Gigabytes per second (1024进制) |
| tibps | Terabypes per second (1024进制) |

使用单位时，注意单位钱的数字不能超过`UINT32_MAX`。当数字过大时你需要换一个更大的单位。



# 2. 解除入向限流

``` bash
# 删除ifb0设备上的队列规则
tc qdisc del dev ifb0 root

# 删除eth0设备上的ingress队列规则
tc qdisc del dev eth0 handle ffff: ingress

# 关闭ifb0设备 (可选)
ip link set dev ifb0 down
```

# 3. 出向限流
出向限流的写法相对简A, 下方示例展示如何将eth0上源端口为13600的流量限制到50mbit

```bash
# 创建队列规则
tc qdisc add dev eth0 root handle 1: htb

# 创建类，并设置带宽限制为50mbps
tc class add dev eth0 parent 1: classid 1:10 htb rate 50mbit

# 添加过滤器，只匹配源端口为13600的流量
tc filter add dev eth0 protocol ip parent 1:0 prio 1 u32 match ip sport 13600 0xffff flowid 1:10
```

限流大小的调整、过滤器的设置可以参考上方出向限流部分的介绍。

# 4. 解除出向限流

下方示例展示如何解除eth0上的出向限流

```bash
# 删除队列规则和类
tc qdisc del dev eth0 root
```

# 5. 流量监控
可以使用iftop工具查看

```bash
iftop -i eth0 -B
```

其中 `-i etho`表示检查eth0上的流量, `-B`指以byte为单位而非bit为单位来显示流量。

另外, iftop 也可以添加筛选条件, 语法比 `tc filter` 要简单的多, 比如下方示例展示筛选出向ip为 192.168.1.100 且目标端口为 13600 或 13601 的流量:

``` bash
sudo iftop -i eth0 -B -f "dst host 192.168.1.100 and (dst port 13600 or dst port 13601)"
```

----------------

正在进行:
  智谱华章产品交付:
    1. 存储后端稳定性验证，调参

计划进行:
  存储后端优化:
    1. 构建并实现更高效的后端数据回收方案
    2. 尝试降低后端数据分片大小，同时尽量减少后端进程的内存占用，以提高存储空间利用率
   Set间文件迁移:
    1. 考虑trigger/trans的修改方案
   小文件优化:
    1. 考虑系统内存在大量小文件时，如何提升空间利用率


-------------

128*300000+256*1024+512*1024+(1+4+8+16+32+48+64)*1024*1024+(80+96+128+160+192+256)*1024*512+(384+512+768+1024)*1024*256
6.54GiB

256*5000+(4+8+16+32+64+128+256+512+1024)*1024*1024
9.98GiB

---------------

# 从epoch 获取可读时间
date -d @<epoch> # 获取可读时间

# 批量拉取chunk内存占用和chunk启动时间点
pssh -h chunks.ip -i "ps -aux | grep chunk | grep -v grep | awk '{print \$2}' | xargs -I{} ps -p {} -o pid,pmem,rss,lstart,comm,cmd --no-headers | awk '{print \"PID: \" \$1 \";\", \"MemUsage: \" \$2 \"%;\", \"RSS: \" \$3/1024 \"MB;\", \"StartTime: \"\$4,\$5,\$6,\$7,\$8 \";\", \"Comm: \" \$9 \";\", \"Cmd: \" substr(\$0, index(\$0,\$10))}'"

# find and grep
find /var/log/upfs -regextype egrep -regex '/var/log/upfs/chunk.*log' -exec grep -n 'Free list out of memory' {} \;


抹盘

复制/修改配置文件，创建日志目录

insert chunk

watch dog

prepare repair

start repair


loki拉取的函数在: https://git.ucloudadmin.com/ucloud-pfs/loki/-/blob/pfs/loki_repair_chunk_handle.cc LokiRepairChunkHandle::LoadChunkTaskJobFromDB()

------------

BOOT_IMAGE=/vmlinuz-5.10.0-19.el7.ucloud.x86_64 root=UUID=491819e5-cd54-48f5-be03-90592db4492c ro crashkernel=512M-2G:64M,2G-4G:128M,4G-8G:192M,8G-:256M net.ifnames=0 biosdevname=0 intel_iommu=off rhgb quiet spectre_v2=off l1tf=off intel_iommu=on default_hugepagesz=1G hugepagesz=1G hugepages=50 LANG=en_US.UTF-8
BOOT_IMAGE=/vmlinuz-5.10.0-16.el7.ucloud.x86_64 root=UUID=f3d41254-c7a7-4a20-8b95-fa516fb64561 ro crashkernel=512M-2G:64M,2G-4G:128M,4G-8G:192M,8G-:256M biosdevname=0 net.ifnames=0 rhgb quiet console=tty0 LANG=en_US.UTF-8

------------

4k_4: 2997180
4k_6: 4040964
4k_8: 4901397
4k_12: 5859124
4k_16: 6314085
4k_24: 6784803
4k_32: 7008002
4k_48: 7230434
4k_64: 7351926
4k_64: 7351926
4k_80: 7465111
4k_96: 7450423
4k_112: 7331017
4k_128: 7443166

-----------

(这里搞错了，其实是iops, 不是bw)

/upfs_data/fio_log/64k_split/writebw_128k_12: 68499.00385787725
/upfs_data/fio_log/64k_split/writebw_128k_16: 68091.79182098767
/upfs_data/fio_log/64k_split/writebw_128k_4: 70726.46655844155
/upfs_data/fio_log/64k_split/writebw_128k_6: 70893.51497694953
/upfs_data/fio_log/64k_split/writebw_128k_8: 70585.36249999999
/upfs_data/fio_log/64k_split/writebw_16k_12: 302639.9868670886
/upfs_data/fio_log/64k_split/writebw_16k_16: 305138.16250000003
/upfs_data/fio_log/64k_split/writebw_16k_4: 298686.6813291141
/upfs_data/fio_log/64k_split/writebw_16k_6: 295146.05000000016
/upfs_data/fio_log/64k_split/writebw_16k_8: 298326.2408227849
/upfs_data/fio_log/64k_split/writebw_256k_12: 33595.22820002365
/upfs_data/fio_log/64k_split/writebw_256k_16: 31369.321043782496
/upfs_data/fio_log/64k_split/writebw_256k_4: 35822.816613924035
/upfs_data/fio_log/64k_split/writebw_256k_6: 35154.83750000001
/upfs_data/fio_log/64k_split/writebw_256k_8: 34663.49470441407
/upfs_data/fio_log/64k_split/writebw_32k_12: 198232.60727848092
/upfs_data/fio_log/64k_split/writebw_32k_16: 199264.83750000008
/upfs_data/fio_log/64k_split/writebw_32k_4: 190078.23212025306
/upfs_data/fio_log/64k_split/writebw_32k_6: 192728.05110759503
/upfs_data/fio_log/64k_split/writebw_32k_8: 195179.10870253164
/upfs_data/fio_log/64k_split/writebw_64k_12: 135465.95000000007
/upfs_data/fio_log/64k_split/writebw_64k_16: 136367.28749999995
/upfs_data/fio_log/64k_split/writebw_64k_4: 132501.65949367094
/upfs_data/fio_log/64k_split/writebw_64k_6: 134385.1792721519
/upfs_data/fio_log/64k_split/writebw_64k_8: 135757.5628164557

8955.704153481009M

-------------

/upfs_data/fio_log/32ksplit_writebw_128k_12: 9585.024234400036M
/upfs_data/fio_log/32ksplit_writebw_128k_16: 7612.670331751682M
/upfs_data/fio_log/32ksplit_writebw_128k_4: 9999.75029171675M
/upfs_data/fio_log/32ksplit_writebw_128k_6: 9989.331470352563M
/upfs_data/fio_log/32ksplit_writebw_128k_8: 9861.22971191407M
/upfs_data/fio_log/32ksplit_writebw_256k_12: 8739.28326811581M
/upfs_data/fio_log/32ksplit_writebw_256k_16: 8145.100514079101M
/upfs_data/fio_log/32ksplit_writebw_256k_4: 9696.006520432691M
/upfs_data/fio_log/32ksplit_writebw_256k_6: 9512.684082031254M
/upfs_data/fio_log/32ksplit_writebw_256k_8: 9352.804687499998M
/upfs_data/fio_log/32ksplit_writebw_32k_12: 9446.855289713549M
/upfs_data/fio_log/32ksplit_writebw_32k_16: 9440.454323167069M
/upfs_data/fio_log/32ksplit_writebw_32k_4: 9771.88232985277M
/upfs_data/fio_log/32ksplit_writebw_32k_6: 9594.748214643432M
/upfs_data/fio_log/32ksplit_writebw_32k_8: 9480.85129895332M
/upfs_data/fio_log/32ksplit_writebw_64k_12: 9710.327294921875M
/upfs_data/fio_log/32ksplit_writebw_64k_16: 9640.681323242188M
/upfs_data/fio_log/32ksplit_writebw_64k_4: 9608.489389272838M
/upfs_data/fio_log/32ksplit_writebw_64k_6: 9622.152539062497M
/upfs_data/fio_log/32ksplit_writebw_64k_8: 9693.379078400445M

--------------




## 1. 发包改为积极发包:

这种策略被称为"优先写"或"写入优先"。你首先尝试发送数据，如果发送失败（通常是因为发送缓冲区已满），那么你就注册一个EV_WRITE事件，并在该事件触发时重试发送。

这种策略的优点是，如果发送缓冲区有足够的空间，你可以立即发送数据，而无需等待EV_WRITE事件。这可以减少事件处理的开销，提高数据发送的效率。

以下是这种策略的一个基本示例：

ssize_t n;
n = send(fd, data, len, 0);
if (n < 0) {
    if (errno == EAGAIN || errno == EWOULDBLOCK) {
        // 注册EV_WRITE事件
        event_set(&ev_write, fd, EV_WRITE, on_write, NULL);
        event_add(&ev_write, NULL);
    } else {
        // 处理其他错误
    }
} else if (n < len) {
    // 保存未发送的数据，等待EV_WRITE事件后重试
}
在这个示例中，我们首先尝试发送数据。如果send返回EAGAIN或EWOULDBLOCK（这表示发送缓冲区已满），我们就注册一个EV_WRITE事件，并在该事件触发时重试发送。如果send发送了部分数据，我们保存未发送的数据，等待EV_WRITE事件后重试。

## 2. 流控

-----

lc: 1789598450581536

-----------------

pssh -h clients.ip -i "nohup fio /upfs_data/pressure/pressure.fio  >/dev/null  2>&1 &"


----------

chunk-24.01.30-3be93ed-centos7-x86_64-native 2.4G/2.7G
chunk-24.01.31-drep-aggsend-01-centos7-x86_64-haswell : 2.6G/2.7G

------------

chunk-24.01.30-ae6f6420-centos7-x86_64-haswell
chunk-24.02.01-fb0b541f-centos7-x86_64-haswell

---------

# BBR:
echo 'net.core.default_qdisc=fq' | tee -a /etc/sysctl.conf

echo 'net.ipv4.tcp_congestion_control=bbr' | tee -a /etc/sysctl.conf

sysctl -A


--------

生成一个函数，其使用get_opt读取进程参数: -s 后接一组用逗号隔开的ip:port, 表示一组需要监听的地址; -p 后接一个整数，表示并发线程数量; 请给解析的结果自定一个类型，返回给上层函数；此外，还需要在用户传入-h时输出帮助文档

--------------

抓包:
10.78.161.77:35868 <--> 10.78.161.79:13603

timeout 60 tcpdump -i bond0 -nn -s 128 'tcp and ((src host 10.78.161.77 and src port 35868 and dst host 10.78.161.79 and dst port 13603) or (dst host 10.78.161.77 and dst port 35868 and src host 10.78.161.79 and src port 13603))' -w /root/hui.sun/client_cap.pcap
timeout 60 tcpdump -i bond0 -nn -s 128 'tcp and ((src host 10.78.161.77 and src port 35868 and dst host 10.78.161.79 and dst port 13603) or (dst host 10.78.161.77 and dst port 35868 and src host 10.78.161.79 and src port 13603))' -w /root/hui.sun/chunk_cap.pcap

------------

128*150000+(256+512)*1024+(1+4+8+16+32+48+64+80+96)*1024*1024+(128+160+192+256)*1024*512+(384+512+768+1024)*1024*256

(128*150000+(256+512)*1024+(1+4+8+16+32+48+64+80+96)*512*1024+(128+160+192+256)*1024*256+(384+512+1024)*1024*256)/1024/1024/1024

(128*8192+1024*4096+(4+8+16+32+64+128+256)*1024*512+512*1024*128+768*1024*64+1024*1024*256)/1024/1024/1024


tcps = {
        128: 300000,     256: 1024,       512: 1024,
        1 * 1024: 512,   4 * 1024: 512,   8 * 1024: 512,
        16 * 1024: 512,  32 * 1024: 512,  48 * 1024: 512,
        64 * 1024: 512,  80 * 1024: 512,  96 * 1024: 512,
        128 * 1024: 256, 160 * 1024: 256, 192 * 1024: 256,
        256 * 1024: 256, 512 * 1024: 256,
        1024 * 1024: 256
}

tcps[32768] = (tcps[32768] + 5000 * (256/32) + 1024) if 32768 in tcps else (5000 * (256/32) + 1024)

tcp_sums = 0
for k,v in tcps.items():
        tcp_sums += k * v

tcp_final_mem = tcp_sums * 2
print("tcp total mem:", tcp_final_mem)

rdmas = {
        128: 300000,     1024: 4096,      4 * 1024: 512,
        8 * 1024: 512,   16 * 1024: 512,  32 * 1024: 512,
        64 * 1024: 512,  128 * 1024: 512, 256 * 1024: 512,
        512 * 1024: 128, 768 * 1024: 64,  1024 * 1024: 256
};

rdmas[32768] = (tcps[32768] + 5000 * (256/32) + 1024) if 32768 in rdmas else (5000 * (256/32) + 1024)

rdmas[258 * 1024] = (rdmas[258 * 1024] + 7168) if (258 * 1024) in rdmas else 7168

rdma_sums = 0
for k,v in tcps.items():
        rdma_sums += k * v

rdma_arenas = rdma_sums // (1000 * 1024 * 1024)
rdma_final_mem = rdma_arenas * 1024 * 1024 * 1024 + (rdma_sums - rdma_arenas * 1000 * 1024 * 1024)

print("rdma final_mem:", rdma_final_mem)

print("total final_mem:", rdma_final_mem + tcp_final_mem)

-----------------------

-----------------------

tcps = {
        128: 300000,      256: 1024,        512: 1024,
        1 * 1024: 512,   4 * 1024: 512,   8 * 1024: 512,
        16 * 1024: 512,  32 * 1024: 512,  48 * 1024: 512,
        64 * 1024: 512,  96 * 1024: 256,
        128 * 1024: 256, 192 * 1024: 256,
        256 * 1024: 256, 1024 * 1024: 256
}

tcps[32768] = (tcps[32768] + 5000 * (256/32) + 1024) if 32768 in tcps else (5000 * (256/32) + 1024)

tcp_sums = 0
for k,v in tcps.items():
        tcp_sums += k * v

tcp_final_mem = tcp_sums * 3

print("tcp total mem:", tcp_final_mem)

rdmas = {
        128: 300000,      256: 1024,        512: 1024,
        1 * 1024: 1024,   4 * 1024: 1024,   8 * 1024: 1024,
        16 * 1024: 1024,  32 * 1024: 1024,  48 * 1024: 512,
        64 * 1024: 512,  96 * 1024: 256,
        128 * 1024: 256, 192 * 1024: 128,
        256 * 1024: 128, 1024 * 1024: 128
}

rdmas[32768] = (tcps[32768] + 5000 * (256/32) + 1024) if 32768 in rdmas else (5000 * (256/32) + 1024)

rdmas[258 * 1024] = (rdmas[258 * 1024] + 7168) if (258 * 1024) in rdmas else 7168

rdma_sums = 0
for k,v in tcps.items():
        rdma_sums += k * v

rdma_arenas = rdma_sums // (1000 * 1024 * 1024)
rdma_final_mem = rdma_arenas * 1024 * 1024 * 1024 + (rdma_sums - rdma_arenas * 1000 * 1024 * 1024)

print("rdma final_mem:", rdma_final_mem)

print("total final_mem:", tcp_final_mem + rdma_final_mem)

------

/root/mongodb-linux-x86_64-3.4.5_37021/bin/mongod -f /root/mongodb-linux-x86_64-3.4.5_37021/mongo.conf



--------------

这是一组从mongodb中拉取到的信息。我需要你根据这些信息和我的描述帮我写一组自动化脚本。

下面我会用尖括号表示对key取值，比如如果我提到 `<id>`, 则表示第一行中的 `0`, 第一行的 `1`等等；如果我提到 `<man_port>`, 就代表第一行的13500，第二行的13501等等。


这里的每一行都有一个对应的配置文件, 文件名为 `chunk-<id>.conf.rdma`，比如对应于第一行的配置文件名就是 "chunk-0.conf.rdma", 对应第二行的是 "chunk-1.conf.bak"

你需要写一个shell脚本来完成以下功能:

修改每行对应的配置文件，在其中包含有"device_prefix"字样的行的下方，添加一行: 

`nvme_disk_addr                     = <nvme_pci_addr>`

-----

我有一个文本，名为 `data.txt`，每一行都形似这种:

```
{ "_id" : ObjectId("65d8a34688d02e6f20d1c2c5"), "capacity" : NumberLong("6393444696064"), "uuid" : "8ffc4a81-24f4-4baf-a5d5-139da0677ef3", "repair_port" : 13700, "ip" : "10.77.69.136", "gate_io_port" : 13600, "state" : 0, "man_port" : 13500, "rack" : "rack-A2F11-A05", "chunk_io_port" : 13620, "host" : "host-10.77.69.136", "hela_io_port" : 13660, "nvme_pci_addr" : "0000:17:00.0", "io_port" : 13600, "create_tick" : 1708696390, "ark_io_port" : 13640, "id" : 0, "heartbeat_capacity" : NumberLong("5972777500672") }
```

可以看出，这是一组键值对，语法类似于在javascript中定义一个字典。
我现在需要你帮我写一个linux shell脚本。伪代码如下:

```
脚本打开这个文本;
while 遍历每一行数据:
  从中解析出 "id"和"nvme_pci_addr"的值，记作 val_id和val_nvme_pci_addr //注意"id"不是"_id"
  打开文件 chunk-${val_id}.conf.rdma, 记作conf_file # 注意替换文件名中的 ${val_id}为上方解析得到的 val_id
  在文件 conf_file中，找到字符串 device_prefix ,在它所处的行的后面再插入一行: "nvme_disk_addr                     = ${val_nvme_pci_addr}" # 注意双引号中的空格数量和位置，这个需要严格模仿

```

----

你的脚本执行结果符合预期，现在参照你自己的脚本和一下伪代码，再编写一个脚本，进一步修改配置文件:

```
脚本打开这个文本;
while 遍历每一行数据:
  从中解析出 "id"的值，记作 val_id //注意"id"不是"_id"
  解析出 "nvme_disk_addr"的值，记作 val_nvme_disk_addr
  打开文件 chunk-${val_id}.conf.rdma, 记作conf_file # 注意替换文件名中的 ${val_id}为上方解析得到的 val_id
  // 注意下方各双引号中的空格数量和位置，这个需要严格匹配
  在文件 conf_file中，找到字符串 "device_prefix" ,在它所处的行的后面再插入一行: "nvme_disk_addr                     = ${val_nvme_pci_addr}" # 注意
  在刚才插入的行后，再插入一行: "polling_core_list                  = 0x1"
  在刚才插入的行后，再插入一行: "huge_mem_size                      = 8192"
  在刚才插入的行后，再插入一行: "huge_dir                           = /dev/hugepages-${val_id}" // 注意替换 ${val_id}为上方解析得到的 val_id
  在文件 conf_file中，找到字符串 "chunk_io_listen_port" ,在它所处的行的后面再插入一行: "rdma_addr                          = mlx5_bond_0"
  在刚才插入的行后, 再插入一行: "rdma_port                          = 1"

  在conf_file的末尾，插入一行: "[options]"
  在刚才插入的行后，再插入一行: "rdma_qp_max_send_wr                = 2048"
```

-------

很好，再实现这段:

```
脚本打开这个文本;
while 遍历每一行数据:
  从中解析出 "id"的值，记作 val_id //注意"id"不是"_id"
  解析出 "nvme_disk_addr"的值，记作 val_nvme_disk_addr
  打开文件 chunk-${val_id}.conf.rdma, 记作conf_file # 注意替换文件名中的 ${val_id}为上方解析得到的 val_id
  // 注意下方各双引号中的空格数量和位置，这个需要严格匹配
  在文件 conf_file中，找到字符串 "device_prefix" ,在它所处的行的后面再插入一行: "nvme_disk_addr                     = ${val_nvme_pci_addr}" # 注意 ${val_nvme_pci_addr}要替换为上方解析得到的 val_nvme_pci_addr
  在刚才插入的行后，再插入一行: "polling_core_list                  = 0x1"
  在刚才插入的行后，再插入一行: "huge_mem_size                      = 8192"
  在刚才插入的行后，再插入一行: "huge_dir                           = /dev/hugepages-${val_id}" // 注意替换 ${val_id}为上方解析得到的 val_id
  在文件 conf_file中，找到字符串 "chunk_io_listen_port" ,在它所处的行的后面再插入一行: "rdma_addr                          = mlx5_bond_0"
  在刚才插入的行后, 再插入一行: "rdma_port                          = 1"

  在conf_file的末尾，插入一行: "[options]"
  在刚才插入的行后，再插入一行: "rdma_qp_max_send_wr                = 2048"
  在刚才插入的行后，再插入一个空行

  在conf_file中，找到文本"chunk_transport_mode               = TCP", 将其替换为 "chunk_transport_mode               = RDMA"

```

#!/bin/bash

# 设置 data.txt 文件的路径
DATA_FILE="data.txt"

# 遍历 data.txt 中的每一行
while IFS= read -r line
do
  # 使用 grep 和 awk 解析 id 的值
  val_id=$(echo "$line" | grep -o '"id" : [^,]*' | awk '{print $3}')
  # 使用 grep 和 awk 解析 nvme_pci_addr 的值
  val_nvme_pci_addr=$(echo "$line" | grep -o '"nvme_pci_addr" : "[^"]*"' | awk '{print $3}' | tr -d '"')

  # 构建目标配置文件名
  conf_file="chunk-${val_id}.conf.rdma"

  # 检查目标文件是否存在
  if [ -f "$conf_file" ]; then
    # 在 device_prefix 所在行的下一行插入新的配置
    sed -i "/device_prefix/a nvme_disk_addr                     = ${val_nvme_pci_addr}\n\
polling_core_list                  = 0x1\n\
huge_mem_size                      = 8192\n\
huge_dir                           = /dev/hugepages-${val_id}" "$conf_file"

    # 在 chunk_io_listen_port 所在行的下一行插入新的配置
    sed -i "/chunk_io_listen_port/a rdma_addr                          = mlx5_bond_0\n\
rdma_port                          = 1" "$conf_file"

    # 在文件末尾添加 [options] 部分、配置和一个空行
    echo -e "[options]\n\
rdma_qp_max_send_wr                = 2048\n" >> "$conf_file"

    # 替换 chunk_transport_mode 的值为 RDMA
    sed -i 's/chunk_transport_mode               = TCP/chunk_transport_mode               = RDMA/' "$conf_file"
  else
    echo "File $conf_file not found."
  fi
done < "$DATA_FILE"

-------------

MLNX_OFED_LINUX-23.10-1.1.9.0-rhel7.9-x86_64

./mlnxofedinstall --user-space-only

可能会要求你装依赖

装不上就再加上 --force再跑一遍

然后, echo '/usr/lib64/libibverbs' > /etc/ld.so.conf.d/libibverbs.conf; ldconfig


----------------

20240228 16:14:55.222700Z 22192 INFO  resource context pool free size: 8,resource context pool size: 5000,resource context pool lowest waterline: 0,resource context pool malloc_succeed_cnt: 4958228,resource context pool malloc_failed_cnt: 13 - io_handle_tcp.cc:114
20240228 16:14:55.223524Z 22204 INFO  resource context pool free size: 2689,resource context pool size: 5000,resource context pool lowest waterline: 2680,resource context pool malloc_succeed_cnt: 20429337,resource context pool malloc_failed_cnt: 0 - io_handle_tcp.cc:114
20240228 16:14:55.227699Z 22205 INFO  resource context pool free size: 1951,resource context pool size: 5000,resource context pool lowest waterline: 1943,resource context pool malloc_succeed_cnt: 28138026,resource context pool malloc_failed_cnt: 0 - io_handle_tcp.cc:114
20240228 16:14:55.240948Z 22183 INFO  resource context pool free size: 713,resource context pool size: 5000,resource context pool lowest waterline: 705,resource context pool malloc_succeed_cnt: 30870118,resource context pool malloc_failed_cnt: 0 - io_handle_tcp.cc:114

----------


bad_chunk_uuid: 229d6882-e0be-4da4-9028-295ff8533227
bad_chunk_ip: 10.72.174.10
bad_chunk_id: 6


new_chunk_ip: 10.72.174.10
new_chunk_man_port: 18000
new_chunk_gate_port: 18010
new_chunk_chunk_port: 18030
new_chunk_uuid: e93f7794-2c82-4aff-b27e-9de430af8bdd
6217569140736

-----

256G 内存中元数据需要 1300/4179774*24389056*20/1024*1.2 = 177 G 内存

----------------

thor_recycle_1340.log

lc_ids = set()
lc_infos = list()

for pc in j:
  if pc['lc_id'] in lc_ids:
    continue
  lc_ids.add(pc['lc_id'])
  lc_infos.append({'lc_id':pc['lc_id'], 'lc_random_id':pc['lc_random_id']})
  if len(lc_ids) >= 200:
    break

-------------

[{'lc_id': 7495782081833943552, 'lc_random_id': 953283635}, {'lc_id': 7495782079798649728, 'lc_random_id': 2049042674}, {'lc_id': 7495782079798649760, 'lc_random_id': 585247560}, {'lc_id': 7495782079798649776, 'lc_random_id': 43158579}, {'lc_id': 7495782079798650048, 'lc_random_id': 2004103212}, {'lc_id': 7495782079798649792, 'lc_random_id': 1043760525}, {'lc_id': 7495782079798649824, 'lc_random_id': 1009896687}, {'lc_id': 7495782079798649840, 'lc_random_id': 1196082726}, {'lc_id': 7495782079798649856, 'lc_random_id': 1562132985}, {'lc_id': 7495782079798649920, 'lc_random_id': 1039343091}, {'lc_id': 7495782079798650064, 'lc_random_id': 932679201}, {'lc_id': 7495782079798649936, 'lc_random_id': 690461675}, {'lc_id': 7495782079798649968, 'lc_random_id': 1274818066}, {'lc_id': 7495782079797600688, 'lc_random_id': 1186419250}, {'lc_id': 7495782079797600720, 'lc_random_id': 1468666757}, {'lc_id': 7495782079797600752, 'lc_random_id': 1549831135}, {'lc_id': 7495782079798649984, 'lc_random_id': 977906566}, {'lc_id': 7495782079797600768, 'lc_random_id': 1576731461}, {'lc_id': 7495782079798650000, 'lc_random_id': 1500930005}, {'lc_id': 7495782079798649392, 'lc_random_id': 625111211}, {'lc_id': 7495782079798649472, 'lc_random_id': 1688734284}, {'lc_id': 7495782079798650016, 'lc_random_id': 912334833}, {'lc_id': 7495782079798649536, 'lc_random_id': 354591868}, {'lc_id': 7495782079798649552, 'lc_random_id': 1973813579}, {'lc_id': 7495782079798649584, 'lc_random_id': 899536818}, {'lc_id': 7495782079798649600, 'lc_random_id': 636786296}, {'lc_id': 7495782079798649616, 'lc_random_id': 1324945053}, {'lc_id': 7495782079798649632, 'lc_random_id': 2032562494}, {'lc_id': 7495782079798649648, 'lc_random_id': 732717247}, {'lc_id': 7495782079798649680, 'lc_random_id': 714885796}, {'lc_id': 7495782079797599696, 'lc_random_id': 643351429}, {'lc_id': 7495782079797599712, 'lc_random_id': 275653800}, {'lc_id': 7495782079797599728, 'lc_random_id': 163648554}, {'lc_id': 7495782079797599744, 'lc_random_id': 976352694}, {'lc_id': 7495782079797599792, 'lc_random_id': 476379074}, {'lc_id': 7495782079797599808, 'lc_random_id': 1650532937}, {'lc_id': 7495782079797599840, 'lc_random_id': 598493303}, {'lc_id': 7495782079797599856, 'lc_random_id': 1691343545}, {'lc_id': 7495782079797599872, 'lc_random_id': 1055308442}, {'lc_id': 7495782079797599888, 'lc_random_id': 632766624}, {'lc_id': 7495782079797599904, 'lc_random_id': 911988921}, {'lc_id': 7495782079797599920, 'lc_random_id': 2027398655}, {'lc_id': 7495782079797599936, 'lc_random_id': 415108048}, {'lc_id': 7495782079797599952, 'lc_random_id': 752792813}, {'lc_id': 7495782079797599984, 'lc_random_id': 446629288}, {'lc_id': 7495782079797600016, 'lc_random_id': 90412656}, {'lc_id': 7495782079797600048, 'lc_random_id': 1885613264}, {'lc_id': 7495782079797600064, 'lc_random_id': 967668983}, {'lc_id': 7495782079797600080, 'lc_random_id': 429976319}, {'lc_id': 7495782079797600096, 'lc_random_id': 875890587}, {'lc_id': 7495782079797600112, 'lc_random_id': 1936425930}, {'lc_id': 7495782079797600128, 'lc_random_id': 1712577157}, {'lc_id': 7495782079797600144, 'lc_random_id': 1925645134}, {'lc_id': 7495782079797600176, 'lc_random_id': 564676451}, {'lc_id': 7495782079797600192, 'lc_random_id': 1302176947}, {'lc_id': 7495782079797600208, 'lc_random_id': 281345536}, {'lc_id': 7495782079797600224, 'lc_random_id': 1292024501}, {'lc_id': 7495782079797600240, 'lc_random_id': 1785976228}, {'lc_id': 7495782079797600256, 'lc_random_id': 1173094794}, {'lc_id': 7495782079797600272, 'lc_random_id': 669954284}, {'lc_id': 7495782079797600288, 'lc_random_id': 2132325450}, {'lc_id': 7495782079797600320, 'lc_random_id': 402931962}, {'lc_id': 7495782079797600336, 'lc_random_id': 326637048}, {'lc_id': 7495782079797600368, 'lc_random_id': 1216226687}, {'lc_id': 7495782079797600384, 'lc_random_id': 787018838}, {'lc_id': 7495782079797600400, 'lc_random_id': 1703065372}, {'lc_id': 7495782079797600416, 'lc_random_id': 1791397862}, {'lc_id': 7495782079797600432, 'lc_random_id': 667681583}, {'lc_id': 7495782079797600464, 'lc_random_id': 1315586843}, {'lc_id': 7495782079797600480, 'lc_random_id': 1673423851}, {'lc_id': 7495782079797600512, 'lc_random_id': 2076257018}, {'lc_id': 7495782079797600544, 'lc_random_id': 520318212}, {'lc_id': 7495782079797600560, 'lc_random_id': 756919348}, {'lc_id': 7495782079797600592, 'lc_random_id': 1914649477}, {'lc_id': 7495782079797600624, 'lc_random_id': 449929045}, {'lc_id': 7495782079797600640, 'lc_random_id': 1249072955}, {'lc_id': 7495782079797600656, 'lc_random_id': 947413286}, {'lc_id': 7495782079797600672, 'lc_random_id': 344365121}, {'lc_id': 7495782079796550144, 'lc_random_id': 876498508}, {'lc_id': 7495782079796550176, 'lc_random_id': 37689270}, {'lc_id': 7495782079796550192, 'lc_random_id': 186725404}, {'lc_id': 7495782079796550224, 'lc_random_id': 643138681}, {'lc_id': 7495782079796550240, 'lc_random_id': 1233185268}, {'lc_id': 7495782079796550272, 'lc_random_id': 314451387}, {'lc_id': 7495782079796550320, 'lc_random_id': 1638798235}, {'lc_id': 7495782079796550336, 'lc_random_id': 1589010669}, {'lc_id': 7495782079796550352, 'lc_random_id': 1101637384}, {'lc_id': 7495782079796550384, 'lc_random_id': 530967212}, {'lc_id': 7495782079796550432, 'lc_random_id': 1671912198}, {'lc_id': 7495782079796550448, 'lc_random_id': 1176154726}, {'lc_id': 7495782079796550480, 'lc_random_id': 1023898171}, {'lc_id': 7495782079796550496, 'lc_random_id': 1534256577}, {'lc_id': 7495782079796550512, 'lc_random_id': 487511576}, {'lc_id': 7495782079796550544, 'lc_random_id': 1492587180}, {'lc_id': 7495782079796550560, 'lc_random_id': 308763238}, {'lc_id': 7495782079796550576, 'lc_random_id': 1301646869}, {'lc_id': 7495782079796550608, 'lc_random_id': 1953881874}, {'lc_id': 7495782079796550640, 'lc_random_id': 291475110}, {'lc_id': 7495782079796550656, 'lc_random_id': 2145106836}, {'lc_id': 7495782079796550672, 'lc_random_id': 1892172044}, {'lc_id': 7495782079796550688, 'lc_random_id': 2144350163}, {'lc_id': 7495782079796550720, 'lc_random_id': 551988229}, {'lc_id': 7495782079796550768, 'lc_random_id': 1411414452}, {'lc_id': 7495782079796550784, 'lc_random_id': 333243526}, {'lc_id': 7495782079796550800, 'lc_random_id': 250317641}, {'lc_id': 7495782079796550832, 'lc_random_id': 591859434}, {'lc_id': 7495782079796550848, 'lc_random_id': 1030633646}, {'lc_id': 7495782079796550864, 'lc_random_id': 1371346160}, {'lc_id': 7495782079796550912, 'lc_random_id': 960502637}, {'lc_id': 7495782079796550928, 'lc_random_id': 902039637}, {'lc_id': 7495782079796550960, 'lc_random_id': 33104333}, {'lc_id': 7495782079796550992, 'lc_random_id': 614847973}, {'lc_id': 7495782079796551008, 'lc_random_id': 1974273204}, {'lc_id': 7495782079796551024, 'lc_random_id': 983354654}, {'lc_id': 7495782079796551040, 'lc_random_id': 1667555997}, {'lc_id': 7495782079796551072, 'lc_random_id': 2122347725}, {'lc_id': 7495782079797599680, 'lc_random_id': 1286562512}, {'lc_id': 7495782079796550064, 'lc_random_id': 496941904}, {'lc_id': 7495782079796550080, 'lc_random_id': 1063028602}, {'lc_id': 7495782079796550096, 'lc_random_id': 482141586}, {'lc_id': 7495782079794451872, 'lc_random_id': 180733025}, {'lc_id': 7495782079794451888, 'lc_random_id': 1904242573}, {'lc_id': 7495782079794451904, 'lc_random_id': 1541893517}, {'lc_id': 7495782079795500512, 'lc_random_id': 1318165093}, {'lc_id': 7495782079795500544, 'lc_random_id': 479142748}, {'lc_id': 7495782079795500592, 'lc_random_id': 1716495048}, {'lc_id': 7495782079795500640, 'lc_random_id': 238479073}, {'lc_id': 7495782079795500688, 'lc_random_id': 16446867}, {'lc_id': 7495782079795500704, 'lc_random_id': 774048441}, {'lc_id': 7495782079795500720, 'lc_random_id': 218515800}, {'lc_id': 7495782079795500736, 'lc_random_id': 864624313}, {'lc_id': 7495782079795500752, 'lc_random_id': 671923594}, {'lc_id': 7495782079795500784, 'lc_random_id': 2133512081}, {'lc_id': 7495782079795500800, 'lc_random_id': 1856439592}, {'lc_id': 7495782079795500832, 'lc_random_id': 1315269745}, {'lc_id': 7495782079795500848, 'lc_random_id': 135250720}, {'lc_id': 7495782079795500864, 'lc_random_id': 1650743289}, {'lc_id': 7495782079795500880, 'lc_random_id': 945926709}, {'lc_id': 7495782079795500896, 'lc_random_id': 1178430749}, {'lc_id': 7495782079795500912, 'lc_random_id': 953313919}, {'lc_id': 7495782079795500944, 'lc_random_id': 39364148}, {'lc_id': 7495782079795500960, 'lc_random_id': 2027686728}, {'lc_id': 7495782079795500976, 'lc_random_id': 1989489985}, {'lc_id': 7495782079795500992, 'lc_random_id': 137613171}, {'lc_id': 7495782079795501008, 'lc_random_id': 1799218968}, {'lc_id': 7495782079795501024, 'lc_random_id': 1416949693}, {'lc_id': 7495782079795501040, 'lc_random_id': 631490718}, {'lc_id': 7495782079795501088, 'lc_random_id': 1322098885}, {'lc_id': 7495782079795501120, 'lc_random_id': 1090853375}, {'lc_id': 7495782079795501152, 'lc_random_id': 573091491}, {'lc_id': 7495782079795501168, 'lc_random_id': 1199972146}, {'lc_id': 7495782079795501184, 'lc_random_id': 694340469}, {'lc_id': 7495782079795501200, 'lc_random_id': 1802498142}, {'lc_id': 7495782079795501232, 'lc_random_id': 2032627829}, {'lc_id': 7495782079795501248, 'lc_random_id': 818525112}, {'lc_id': 7495782079795501296, 'lc_random_id': 644898246}, {'lc_id': 7495782079795501312, 'lc_random_id': 210118144}, {'lc_id': 7495782079795501328, 'lc_random_id': 282876714}, {'lc_id': 7495782079795501360, 'lc_random_id': 344239670}, {'lc_id': 7495782079795501376, 'lc_random_id': 1551878696}, {'lc_id': 7495782079795501408, 'lc_random_id': 187171411}, {'lc_id': 7495782079795501424, 'lc_random_id': 110841341}, {'lc_id': 7495782079795501440, 'lc_random_id': 1528812370}, {'lc_id': 7495782079795501456, 'lc_random_id': 147111110}, {'lc_id': 7495782079796550048, 'lc_random_id': 1273661711}, {'lc_id': 7495782079793401936, 'lc_random_id': 72259805}, {'lc_id': 7495782079793401952, 'lc_random_id': 1016962084}, {'lc_id': 7495782079793401968, 'lc_random_id': 997188661}, {'lc_id': 7495782079793401984, 'lc_random_id': 881988181}, {'lc_id': 7495782079793402032, 'lc_random_id': 1950744754}, {'lc_id': 7495782079793402048, 'lc_random_id': 1421419761}, {'lc_id': 7495782079793402080, 'lc_random_id': 1606058791}, {'lc_id': 7495782079794450704, 'lc_random_id': 1052435774}, {'lc_id': 7495782079794450736, 'lc_random_id': 1607657124}, {'lc_id': 7495782079794450752, 'lc_random_id': 366405470}, {'lc_id': 7495782079794450768, 'lc_random_id': 1933994700}, {'lc_id': 7495782079794450784, 'lc_random_id': 1332606827}, {'lc_id': 7495782079794450848, 'lc_random_id': 292382590}, {'lc_id': 7495782079794450864, 'lc_random_id': 750579519}, {'lc_id': 7495782079794450928, 'lc_random_id': 181053036}, {'lc_id': 7495782079794450960, 'lc_random_id': 890773007}, {'lc_id': 7495782079794450976, 'lc_random_id': 70586554}, {'lc_id': 7495782079794450992, 'lc_random_id': 705971339}, {'lc_id': 7495782079794451008, 'lc_random_id': 1604475962}, {'lc_id': 7495782079794451024, 'lc_random_id': 1308140206}, {'lc_id': 7495782079794451040, 'lc_random_id': 58991284}, {'lc_id': 7495782079794451072, 'lc_random_id': 97259426}, {'lc_id': 7495782079794451088, 'lc_random_id': 1847920660}, {'lc_id': 7495782079794451104, 'lc_random_id': 1853208642}, {'lc_id': 7495782079794451120, 'lc_random_id': 354014662}, {'lc_id': 7495782079794451136, 'lc_random_id': 733953271}, {'lc_id': 7495782079794451152, 'lc_random_id': 1215676588}, {'lc_id': 7495782079794451184, 'lc_random_id': 1979654444}, {'lc_id': 7495782079794451200, 'lc_random_id': 762109382}, {'lc_id': 7495782079794451216, 'lc_random_id': 881212965}, {'lc_id': 7495782079794451232, 'lc_random_id': 1157373227}, {'lc_id': 7495782079794451248, 'lc_random_id': 850817333}, {'lc_id': 7495782079794451296, 'lc_random_id': 1883379}, {'lc_id': 7495782079794451328, 'lc_random_id': 1336596617}, {'lc_id': 7495782079794451344, 'lc_random_id': 453374692}]




------

7495782081833943552
7495782079798649728
7495782079798649760
7495782079798649776
7495782079798650048
7495782079798649792
7495782079798649824
7495782079798649840
7495782079798649856
7495782079798649920
7495782079798650064
7495782079798649936
7495782079798649968
7495782079797600688
7495782079797600720
7495782079797600752
7495782079798649984
7495782079797600768
7495782079798650000
7495782079798649392
7495782079798649472
7495782079798650016
7495782079798649536
7495782079798649552
7495782079798649584
7495782079798649600
7495782079798649616
7495782079798649632
7495782079798649648
7495782079798649680
7495782079797599696
7495782079797599712
7495782079797599728
7495782079797599744
7495782079797599792
7495782079797599808
7495782079797599840
7495782079797599856
7495782079797599872
7495782079797599888
7495782079797599904
7495782079797599920
7495782079797599936
7495782079797599952
7495782079797599984
7495782079797600016
7495782079797600048
7495782079797600064
7495782079797600080
7495782079797600096
7495782079797600112
7495782079797600128
7495782079797600144
7495782079797600176
7495782079797600192
7495782079797600208
7495782079797600224
7495782079797600240
7495782079797600256
7495782079797600272
7495782079797600288
7495782079797600320
7495782079797600336
7495782079797600368
7495782079797600384
7495782079797600400
7495782079797600416
7495782079797600432
7495782079797600464
7495782079797600480
7495782079797600512
7495782079797600544
7495782079797600560
7495782079797600592
7495782079797600624
7495782079797600640
7495782079797600656
7495782079797600672
7495782079796550144
7495782079796550176
7495782079796550192
7495782079796550224
7495782079796550240
7495782079796550272
7495782079796550320
7495782079796550336
7495782079796550352
7495782079796550384
7495782079796550432
7495782079796550448
7495782079796550480
7495782079796550496
7495782079796550512
7495782079796550544
7495782079796550560
7495782079796550576
7495782079796550608
7495782079796550640
7495782079796550656
7495782079796550672
7495782079796550688
7495782079796550720
7495782079796550768
7495782079796550784
7495782079796550800
7495782079796550832
7495782079796550848
7495782079796550864
7495782079796550912
7495782079796550928
7495782079796550960
7495782079796550992
7495782079796551008
7495782079796551024
7495782079796551040
7495782079796551072
7495782079797599680
7495782079796550064
7495782079796550080
7495782079796550096
7495782079794451872
7495782079794451888
7495782079794451904
7495782079795500512
7495782079795500544
7495782079795500592
7495782079795500640
7495782079795500688
7495782079795500704
7495782079795500720
7495782079795500736
7495782079795500752
7495782079795500784
7495782079795500800
7495782079795500832
7495782079795500848
7495782079795500864
7495782079795500880
7495782079795500896
7495782079795500912
7495782079795500944
7495782079795500960
7495782079795500976
7495782079795500992
7495782079795501008
7495782079795501024
7495782079795501040
7495782079795501088
7495782079795501120
7495782079795501152
7495782079795501168
7495782079795501184
7495782079795501200
7495782079795501232
7495782079795501248
7495782079795501296
7495782079795501312
7495782079795501328
7495782079795501360
7495782079795501376
7495782079795501408
7495782079795501424
7495782079795501440
7495782079795501456
7495782079796550048
7495782079793401936
7495782079793401952
7495782079793401968
7495782079793401984
7495782079793402032
7495782079793402048
7495782079793402080
7495782079794450704
7495782079794450736
7495782079794450752
7495782079794450768
7495782079794450784
7495782079794450848
7495782079794450864
7495782079794450928
7495782079794450960
7495782079794450976
7495782079794450992
7495782079794451008
7495782079794451024
7495782079794451040
7495782079794451072
7495782079794451088
7495782079794451104
7495782079794451120
7495782079794451136
7495782079794451152
7495782079794451184
7495782079794451200
7495782079794451216
7495782079794451232
7495782079794451248
7495782079794451296
7495782079794451328
7495782079794451344


---------------

7495782081833943552|7495782079798649728|7495782079798649760|7495782079798649776|7495782079798650048|7495782079798649792|7495782079798649824|7495782079798649840|7495782079798649856|7495782079798649920|7495782079798650064|7495782079798649936|7495782079798649968|7495782079797600688|7495782079797600720|7495782079797600752|7495782079798649984|7495782079797600768|7495782079798650000|7495782079798649392|7495782079798649472|7495782079798650016|7495782079798649536|7495782079798649552|7495782079798649584|7495782079798649600|7495782079798649616|7495782079798649632|7495782079798649648|7495782079798649680|7495782079797599696|7495782079797599712|7495782079797599728|7495782079797599744|7495782079797599792|7495782079797599808|7495782079797599840|7495782079797599856|7495782079797599872|7495782079797599888|7495782079797599904|7495782079797599920|7495782079797599936|7495782079797599952|7495782079797599984|7495782079797600016|7495782079797600048|7495782079797600064|7495782079797600080|7495782079797600096|7495782079797600112|7495782079797600128|7495782079797600144|7495782079797600176|7495782079797600192|7495782079797600208|7495782079797600224|7495782079797600240|7495782079797600256|7495782079797600272|7495782079797600288|7495782079797600320|7495782079797600336|7495782079797600368|7495782079797600384|7495782079797600400|7495782079797600416|7495782079797600432|7495782079797600464|7495782079797600480|7495782079797600512|7495782079797600544|7495782079797600560|7495782079797600592|7495782079797600624|7495782079797600640|7495782079797600656|7495782079797600672|7495782079796550144|7495782079796550176|7495782079796550192|7495782079796550224|7495782079796550240|7495782079796550272|7495782079796550320|7495782079796550336|7495782079796550352|7495782079796550384|7495782079796550432|7495782079796550448|7495782079796550480|7495782079796550496|7495782079796550512|7495782079796550544|7495782079796550560|7495782079796550576|7495782079796550608|7495782079796550640|7495782079796550656|7495782079796550672|7495782079796550688|7495782079796550720|7495782079796550768|7495782079796550784|7495782079796550800|7495782079796550832|7495782079796550848|7495782079796550864|7495782079796550912|7495782079796550928|7495782079796550960|7495782079796550992|7495782079796551008|7495782079796551024|7495782079796551040|7495782079796551072|7495782079797599680|7495782079796550064|7495782079796550080|7495782079796550096|7495782079794451872|7495782079794451888|7495782079794451904|7495782079795500512|7495782079795500544|7495782079795500592|7495782079795500640|7495782079795500688|7495782079795500704|7495782079795500720|7495782079795500736|7495782079795500752|7495782079795500784|7495782079795500800|7495782079795500832|7495782079795500848|7495782079795500864|7495782079795500880|7495782079795500896|7495782079795500912|7495782079795500944|7495782079795500960|7495782079795500976|7495782079795500992|7495782079795501008|7495782079795501024|7495782079795501040|7495782079795501088|7495782079795501120|7495782079795501152|7495782079795501168|7495782079795501184|7495782079795501200|7495782079795501232|7495782079795501248|7495782079795501296|7495782079795501312|7495782079795501328|7495782079795501360|7495782079795501376|7495782079795501408|7495782079795501424|7495782079795501440|7495782079795501456|7495782079796550048|7495782079793401936|7495782079793401952|7495782079793401968|7495782079793401984|7495782079793402032|7495782079793402048|7495782079793402080|7495782079794450704|7495782079794450736|7495782079794450752|7495782079794450768|7495782079794450784|7495782079794450848|7495782079794450864|7495782079794450928|7495782079794450960|7495782079794450976|7495782079794450992|7495782079794451008|7495782079794451024|7495782079794451040|7495782079794451072|7495782079794451088|7495782079794451104|7495782079794451120|7495782079794451136|7495782079794451152|7495782079794451184|7495782079794451200|7495782079794451216|7495782079794451232|7495782079794451248|7495782079794451296|7495782079794451328|7495782079794451344


-------

stats = {0:0, 1:0, 2:0, 3:0, 5:0, 10:0, 20:0, 50:0,100:0,200:0,500:0,1000:0}

for pc in j:
  pc_no = pc['pc_no']
  if pc_no <= 0:
    stats[0] = stats[0] + 1
  if pc_no <= 1:
    stats[1] = stats[1] + 1
  if pc_no <= 2:
    stats[2] = stats[2] + 1
  if pc_no <= 3:
    stats[3] = stats[3] + 1
  if pc_no <= 5:
    stats[5] = stats[5] + 1
  if pc_no <= 10:
    stats[10] = stats[10] + 1
  if pc_no <= 20:
    stats[20] = stats[20] + 1
  if pc_no <= 50:
    stats[50] = stats[50] + 1
  if pc_no <= 100:
    stats[100] = stats[100] + 1
  if pc_no <= 20:
    stats[200] = stats[200] + 1
  if pc_no <= 500:
    stats[500] = stats[500] + 1
  if pc_no <= 1000:
    stats[1000] = stats[1000] + 1

-------

epoch_floor = 1709179200
meta_file = '/root/chunk10_pcmeta_1641.json'
out_file_prefix = '/root/chunk10_to_recycle/target_lcs_chunk10_1641.json'
split_count = 500

import json

j = json.load(open(meta_file))

pcs = []
for pc in j:
  if pc['is_used'] == 0:
    continue
  pcs.append(pc)

lcs = set()
lc_infos = []

for pc in pcs:
  if pc['allocate_time'] < epoch_floor:
    continue
  lc_id = pc['lc_id']
  lc_random_id = pc['lc_random_id']
  if lc_id in lcs:
    continue
  lcs.add(lc_id)
  lc_infos.append({'lc_id':lc_id,'lc_random_id':lc_random_id})

out_file_id = 0
lc_index = 0
while lc_index < len(lc_infos):
  target_len = lc_index + split_count if lc_index + split_count < len(lc_infos) else len(lc_infos)
  with open(f"out_file_prefix.{out_file_id}", 'w') as outf:
    json.dump(lc_infos[lc_index:target_len], outf)
  out_file_id = out_file_id + 1
  lc_index = target_len


-----------------

import concurrent.futures
shlex, subprocess

def run_cmd(cmd: str, shlex_posix_mode: bool, run_in_new_shell: bool):
    logger.debug(f'Run CMD: "{cmd}"')
    cmd_arr = shlex.split(cmd, posix=shlex_posix_mode)
    ret = subprocess.run(
        cmd_arr,
        shell=run_in_new_shell,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        encoding="utf-8",
    )
    if ret.returncode != 0:
        print(
            f'Run CMD: "{cmd}" failed,\nreturncode: {ret.returncode}\nstdout:\n{ret.stdout}\nstderr:\n{ret.stderr}'
        )
    else:
        print(
            f'Run CMD: "{cmd}" success,\nreturncode: {ret.returncode}\nstdout:\n{ret.stdout}\nstderr:\n{ret.stderr}'
        )
    return ret


with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
        futures = []
        for filename in os.listdir(directory):
            f = os.path.join(directory, filename)
            future = executor.submit(run_cmd,f"你的命令",True)
            futures.append(future)
        concurrent.futures.wait(futures, return_when=concurrent.futures.ALL_COMPLETED)

--------------------

{"uuid" : "2b3f16aa-04fa-45bb-8ade-4fc1d2f2d4b1", "id" : 8 }
{"uuid" : "7ab56e82-5789-4316-8e36-f4ff55af70cd", "id" : 9 }
{"uuid" : "34314047-3348-47a3-92f4-1be0be586d3b", "id" : 10 }
{"uuid" : "b426fb4c-7172-4f49-83c3-893e7e05897e", "id" : 11 }
{"uuid" : "c72959d2-3686-4d20-b4b1-bc2aca05c243", "id" : 12 }
{"uuid" : "20d065d7-bb68-4681-a0cf-1f5662388f03", "id" : 13 }
{"uuid" : "5c097cc8-281f-4e4a-ac54-cd4987cfcfea", "id" : 14 }
{"uuid" : "bfb7dff7-564e-491d-99d5-09df99cf0643", "id" : 15 }


----------------

pc_meta_total_size = super_block_.pc_num * super_block_.pc_meta_size
super_block_.pc_zone_offset

Dump:
pc_zone_offset
pc_num
pc_size

------------

shixin 3203

chunk 3: 3203_1: /dev/nvme3n1
3 -> 16



  

./raw_chunk_storage_tool format /dev/nvme3n1 16 true false 1048576 true

-----------------

fio --direct=1 --bs=1M --rw=randwrite --ioengine=libaio --name=fill --directory=. --iodepth=16 --numjobs=16 --size=10G

-----------------

10.13.1.68   /root/limax/udisk/tool/get_set_capacity.py 


-----------

import asyncio

class A:
  async def init(self):
    await asyncio.sleep(1)
    print(self)

class B(A):
  pass

a = [B() for i in range(0, 4)]

a_init = [aa.init() for aa in a]
async def call_all():
  await asyncio.gather(*a_init)

loop = asyncio.get_event_loop()
loop.run_until_complete(call_all())


-----------------

lsblk | grep -oE 'nvme[0-9]n[0-9]' | xargs -I{} readlink -f /sys/block/{}/device

10.72.141.218:
0000:3b:00.0 8
0000:3e:00.0 11
0000:da:00.0 14
0000:3d:00.0 10
0000:d9:00.0 13
0000:3c:00.0 9
0000:d8:00.0 12
0000:db:00.0 28

10.72.142.140:
0000:3d:00.0 15
0000:d9:00.0 18
0000:d8:00.0 17
0000:db:00.0 20
0000:3e:00.0 16
0000:da:00.0 19

------------------------
sed -i -E 's/tcp_thread_num                     = 5/tcp_thread_num                     = 3/g' *.conf
sed -i -E 's/rdma_thread_num                    = 0/rdma_thread_num                    = 1/g' *.conf
sed -i -E 's/recycle_rate                       = 50M/recycle_rate                       = 100M/g' *.conf
sed -i -E 's/io_timeout                         = 2/io_timeout                         = 5/g' *.conf
# 添加polling core 设置: polling_core_list
sed -i "/device_prefix/a nvme_disk_addr                     = ${val_nvme_pci_addr}\n\
polling_core_list                  = 0x1\n\
huge_mem_size                      = 8192\n\
huge_dir                           = /dev/hugepages-${val_id}" "$conf_file"


--------------

find . |grep 20240308 | xargs grep -n 'connect to chunk closed'

.1:
./chunk-2/Chunk2.20240308-145854.hk-kl-17-01.10292.log:481605:20240308 15:00:20.887461Z 10470 INFO  connect to chunk closed, peer addr:10.0.17.8:13622 - io_handle.cc:284
./chunk-7/Chunk7.20240308-145833.hk-kl-17-01.10297.log:619283:20240308 15:00:21.338239Z 10489 INFO  connect to chunk closed, peer addr:10.0.17.8:13627 - io_handle.cc:284
./chunk-12/Chunk12.20240308-142907.hk-kl-17-01.10283.log:23498:20240308 14:34:43.929172Z 10463 INFO  connect to chunk closed, peer addr:10.0.17.9:13632 - io_handle.cc:284
./chunk-9/Chunk9.20240308-145852.hk-kl-17-01.10299.log:502080:20240308 15:00:21.477289Z 10473 INFO  connect to chunk closed, peer addr:10.0.17.8:13629 - io_handle.cc:284

.2:
./upfs/chunk-361/Chunk361.20240308-143108.hk-kl-17-02.51660.log:103469:20240308 14:54:20.268691Z 51789 INFO  connect to chunk closed, peer addr:10.0.17.1:13621 - io_handle.cc:284



----------------

db.t_chunk_repair_task_job.count({"tgt_chunk":{"$ne":41}})
2724585

----------------

SPDK: 添加新的bdev类型: SPDK_BDEV_MODULE_REGISTER

这个宏定义一个static函数，以 __attribute__(constructor)修饰:

```
/*
 *  Macro used to register module for later initialization.
 */
#define SPDK_BDEV_MODULE_REGISTER(name, module) \
static void __attribute__((constructor)) _spdk_bdev_module_register_##name(void) \
{ \
  spdk_bdev_module_list_add(module); \
} \

```

其中的module:

```
static struct spdk_bdev_module udisk_if = {
  .name = "udisk",
  .module_init = bdev_udisk_initialize,
  .module_fini = bdev_udisk_finish,
  .config_text = bdev_udisk_get_spdk_running_config,
  .get_ctx_size = bdev_udisk_get_ctx_size,
};

SPDK_BDEV_MODULE_REGISTER(udisk, &udisk_if)
```

-------------

bdev subsystem注册:

```
static struct spdk_subsystem g_spdk_subsystem_bdev = {
  .name = "bdev",
  .init = bdev_subsystem_initialize,
  .fini = bdev_subsystem_finish,
  .write_config_json = bdev_subsystem_config_json,
};

SPDK_SUBSYSTEM_REGISTER(g_spdk_subsystem_bdev);
SPDK_SUBSYSTEM_DEPEND(bdev, accel)
SPDK_SUBSYSTEM_DEPEND(bdev, vmd)
SPDK_SUBSYSTEM_DEPEND(bdev, sock)

/**
 * \brief Register a new subsystem
 */
#define SPDK_SUBSYSTEM_REGISTER(_name) \
  __attribute__((constructor)) static void _name ## _register(void) \
  {                 \
    spdk_add_subsystem(&_name);         \
  }

void
spdk_add_subsystem(struct spdk_subsystem *subsystem)
{
  TAILQ_INSERT_TAIL(&g_subsystems, subsystem, tailq);
}
```

所以，subsystem注册其实是就是将`struct spdk_subsystem`放入 `g_subsystems` (`struct spdk_subsystem_list g_subsystems = TAILQ_HEAD_INITIALIZER(g_subsystems);`)。

-----------------

各subsystem初始化:

```
spdk_app_start() -> spdk_thread_send_msg(g_app_thread, bootstrap_fn, NULL);

bootstrap_fn()
  └ 在 g_delay_subsystem_init == false 时: spdk_subsystem_init(app_start_rpc, NULL)
      └  spdk_subsystem_init(app_start_rpc, NULL)
          └ 设置 g_subsystem_start_fn 和 g_subsystem_start_arg。本处即将它们设置为 app_start_rpc 和 NULL
          └ 检查各subsystem间的依赖关系。对全局变量g_subsystems中的子系统进行排序，排序的依据是子系统之间的依赖关系，这些依赖关系存储在全局变量g_subsystems_deps中。
          └ spdk_subsystem_init_next(0)
              └ 在 参数 (rc) 为0时: g_subsystem_start_fn(rc, g_subsystem_start_arg); (本处不会触发此逻辑)
              └ 在判定所有subsystem均初始化完成后: g_subsystem_start_fn(rc, g_subsystem_start_arg); (本处不会触发此逻辑)
              └ 从 g_subsystems中选择第一个未初始化的subsystem，调用其init回调。如果init不存在，就跳过，再次调用本函数(`spdk_subsystem_init_next(0)`)，尝试初始化下一个subsystem。
```

--------

bdev subsystem的初始化:

bdev_subsystem_initialize()
  └ spdk_bdev_initialize(bdev_initialize_complete, NULL);
      └ 设置 g_init_cb_fn 和 g_init_cb_arg，本处这两个值被设置为 bdev_initialize_complete 和 NULL

----------------------

chunk27: 10.72.141.216
chunk26: 10.72.141.216
chunk25: 10.72.141.216
chunk24: 10.72.141.216
chunk23: 10.72.141.216
chunk22: 10.72.141.216
chunk21: 10.72.141.216
chunk7:  10.72.141.216
chunk6:  10.72.141.216
chunk5:  10.72.141.216
chunk4:  10.72.141.216

--------------

10.72.141.216: 0-7
0: nvme0n1
97a9302e-dcc1-4f4c-a716-0e8ef6c312a5
1: nvme1n1
4e0f0eba-83da-4feb-8dc6-d38acfa18585
2: nvme2n1
989e26b0-0672-4498-878a-4a40cebd7983
3: nvme3n1
ad120e00-558b-4eb5-b6ae-3c9ff36e2359
4: nvme4n1
9a9ec2d7-817d-4cb5-8038-dc906570d1d1
5: nvme5n1
9e07bea8-7a7a-46e7-80f2-e6eb7aa2cde7
6: nvme6n1
679a58ba-49ed-4e97-8a5b-5853ad5a1851
7: nvme7n1
11bec314-8e99-4f08-8ea6-60060d712c7c

./raw_chunk_storage_tool format /dev/nvme0n1 0 false false 1048576 true
./raw_chunk_storage_tool format /dev/nvme1n1 1 false false 1048576 true
./raw_chunk_storage_tool format /dev/nvme2n1 2 false false 1048576 true
./raw_chunk_storage_tool format /dev/nvme3n1 3 false false 1048576 true
./raw_chunk_storage_tool format /dev/nvme4n1 4 false false 1048576 true
./raw_chunk_storage_tool format /dev/nvme5n1 5 false false 1048576 true
./raw_chunk_storage_tool format /dev/nvme6n1 6 false false 1048576 true
./raw_chunk_storage_tool format /dev/nvme7n1 7 false false 1048576 true

db.t_chunk_info.updateOne({id:0},{"$set":{"uuid":"97a9302e-dcc1-4f4c-a716-0e8ef6c312a5"}})
db.t_chunk_info.updateOne({id:1},{"$set":{"uuid":"4e0f0eba-83da-4feb-8dc6-d38acfa18585"}})
db.t_chunk_info.updateOne({id:2},{"$set":{"uuid":"989e26b0-0672-4498-878a-4a40cebd7983"}})
db.t_chunk_info.updateOne({id:3},{"$set":{"uuid":"ad120e00-558b-4eb5-b6ae-3c9ff36e2359"}})
db.t_chunk_info.updateOne({id:4},{"$set":{"uuid":"9a9ec2d7-817d-4cb5-8038-dc906570d1d1"}})
db.t_chunk_info.updateOne({id:5},{"$set":{"uuid":"9e07bea8-7a7a-46e7-80f2-e6eb7aa2cde7"}})
db.t_chunk_info.updateOne({id:6},{"$set":{"uuid":"679a58ba-49ed-4e97-8a5b-5853ad5a1851"}})
db.t_chunk_info.updateOne({id:7},{"$set":{"uuid":"11bec314-8e99-4f08-8ea6-60060d712c7c"}})


10.72.141.218: 8-14
8: nvm0n1
d4d15216-0002-4147-8ac6-b173b2cceeb6
9: nvme1n1
ed5c7fc4-df8d-419e-acb9-fa3962820d97
10: nvme2n1
a1b6c4dc-4781-467b-a6b6-9f18355aaeb9
11: nvme3n1
9d68ac4d-b742-4e77-a67f-fb11bcbbb5ac
12: nvme4n1
44c8a2a7-af6a-41a6-994f-de3f8037c81f
13: nvme5m1
e3c51ca8-4e51-4518-a519-82975a2a79ae
14: nvme6n1
b2e94d32-9433-4dc2-85cc-9df53d2bd7a8

./raw_chunk_storage_tool format /dev/nvme0n1 8 false false 1048576 true
./raw_chunk_storage_tool format /dev/nvme1n1 9 false false 1048576 true
./raw_chunk_storage_tool format /dev/nvme2n1 10 false false 1048576 true
./raw_chunk_storage_tool format /dev/nvme3n1 11 false false 1048576 true
./raw_chunk_storage_tool format /dev/nvme4n1 12 false false 1048576 true
./raw_chunk_storage_tool format /dev/nvme5n1 13 false false 1048576 true
./raw_chunk_storage_tool format /dev/nvme6n1 14 false false 1048576 true

db.t_chunk_info.updateOne({id:8},{"$set":{"uuid":"d4d15216-0002-4147-8ac6-b173b2cceeb6"}})
db.t_chunk_info.updateOne({id:9},{"$set":{"uuid":"ed5c7fc4-df8d-419e-acb9-fa3962820d97"}})
db.t_chunk_info.updateOne({id:10},{"$set":{"uuid":"a1b6c4dc-4781-467b-a6b6-9f18355aaeb9"}})
db.t_chunk_info.updateOne({id:11},{"$set":{"uuid":"9d68ac4d-b742-4e77-a67f-fb11bcbbb5ac"}})
db.t_chunk_info.updateOne({id:12},{"$set":{"uuid":"44c8a2a7-af6a-41a6-994f-de3f8037c81f"}})
db.t_chunk_info.updateOne({id:13},{"$set":{"uuid":"e3c51ca8-4e51-4518-a519-82975a2a79ae"}})
db.t_chunk_info.updateOne({id:14},{"$set":{"uuid":"b2e94d32-9433-4dc2-85cc-9df53d2bd7a8"}})

10.72.142.140: 15-20
15: nvme0n1
dbc41ef3-c373-41f4-abe2-394c339e6b21
16: nvme1n1
00523d78-779d-40a1-88ae-69debf98a2fc
17: nvme2n1
a86a7a45-14b9-4e1e-861b-b7223ef4d000
18: nvme3n1
e2d226c6-804c-467b-ab18-a321ea0e3532
19: nvme4n1
def20b02-1181-4a1d-bc59-3589e2f7ef6e
20: nvme5n1
767beb66-f6d8-4adb-9c15-f9d3cf5506e6

./raw_chunk_storage_tool format /dev/nvme0n1 15 false false 1048576 true
./raw_chunk_storage_tool format /dev/nvme1n1 16 false false 1048576 true
./raw_chunk_storage_tool format /dev/nvme2n1 17 false false 1048576 true
./raw_chunk_storage_tool format /dev/nvme3n1 18 false false 1048576 true
./raw_chunk_storage_tool format /dev/nvme4n1 19 false false 1048576 true
./raw_chunk_storage_tool format /dev/nvme5n1 20 false false 1048576 true

db.t_chunk_info.updateOne({id:15},{"$set":{"uuid":"dbc41ef3-c373-41f4-abe2-394c339e6b21"}})
db.t_chunk_info.updateOne({id:16},{"$set":{"uuid":"00523d78-779d-40a1-88ae-69debf98a2fc"}})
db.t_chunk_info.updateOne({id:17},{"$set":{"uuid":"a86a7a45-14b9-4e1e-861b-b7223ef4d000"}})
db.t_chunk_info.updateOne({id:18},{"$set":{"uuid":"e2d226c6-804c-467b-ab18-a321ea0e3532"}})
db.t_chunk_info.updateOne({id:19},{"$set":{"uuid":"def20b02-1181-4a1d-bc59-3589e2f7ef6e"}})
db.t_chunk_info.updateOne({id:20},{"$set":{"uuid":"767beb66-f6d8-4adb-9c15-f9d3cf5506e6"}})

-------------

10.72.141.216 net2
10.72.141.218 net2

----------

7362KiB

-------------
kunlun 新 512K 集群:
1. 保留160G大页
2. 使用新chunk配置文件: 

----------

./raw_chunk_storage_tool format 9-0000:d8:00.0 9 false false 262144 true 0x1
0371f072-50af-42f7-a382-ca39cd7f62d4

./raw_chunk_storage_tool format 1-0000:d8:00.0 1 false false 262144 true 0x1
80e26374-791e-44c1-85f7-a3e494b64239

./raw_chunk_storage_tool format 10-0000:d8:00.0 10 false false 262144 true 0x1
1f7c7494-943f-4ce6-8a69-b2c5ea36d089

------------

lc_id: 1797272261440976 lc_random_id: 773902901

-----------

chunk10 -> chunk11








帮我写一个python3脚本，它接受3个输入参数:
1. 一个文件的路径, 记作input。这个文件是一个文本文件, 内部有很多行。行的数量不定。每行的内容都是一个十进制的非负整数。整数最大值为(1<<32)-1, 最小值为0。
2. 一个单独的正整数, 记做range。
3. 一个文件的路径, 记作output。它在脚本启动时不存在。脚本需要将输出内容写到这个文件。

这个脚本执行以下逻辑:
将[0,(1<<32)-1]的范围，从小到大，按照range划分为多个区块(如果最后存在一个大小不足range部分，则也将这个不足的部分划分为一个区块)。
然后，统计每个区块中，含有多少个来自input文件的整数。
然后，将这个统计结果绘制成直方图，放进文件output中。

a + b + c = 10
a + b - c = 6
3a + 3b + c = 26

---------------

10.72.141.206:
chunk-21.conf:nvme_disk_addr                     = 0000:3b:00.0
chunk-22.conf:nvme_disk_addr                     = 0000:3c:00.0
chunk-23.conf:nvme_disk_addr                     = 0000:3d:00.0
chunk-24.conf:nvme_disk_addr                     = 0000:3e:00.0
chunk-25.conf:nvme_disk_addr                     = 0000:d8:00.0
chunk-26.conf:nvme_disk_addr                     = 0000:d9:00.0
chunk-27.conf:nvme_disk_addr                     = 0000:da:00.0
chunk-28.conf:nvme_disk_addr                     = 0000:db:00.0

10.72.141.207:
chunk-29.conf:nvme_disk_addr                     = 0000:3b:00.0
chunk-30.conf:nvme_disk_addr                     = 0000:3c:00.0
chunk-31.conf:nvme_disk_addr                     = 0000:3d:00.0
chunk-32.conf:nvme_disk_addr                     = 0000:3e:00.0
chunk-33.conf:nvme_disk_addr                     = 0000:d8:00.0
chunk-34.conf:nvme_disk_addr                     = 0000:d9:00.0
chunk-35.conf:nvme_disk_addr                     = 0000:da:00.0
chunk-36.conf:nvme_disk_addr                     = 0000:db:00.0

10.72.141.14:
chunk-37.conf:nvme_disk_addr                     = 0000:3b:00.0
chunk-38.conf:nvme_disk_addr                     = 0000:3c:00.0
chunk-39.conf:nvme_disk_addr                     = 0000:3d:00.0
chunk-40.conf:nvme_disk_addr                     = 0000:3e:00.0
chunk-41.conf:nvme_disk_addr                     = 0000:d8:00.0
chunk-42.conf:nvme_disk_addr                     = 0000:d9:00.0
chunk-43.conf:nvme_disk_addr                     = 0000:da:00.0
chunk-44.conf:nvme_disk_addr                     = 0000:db:00.0

-----------

nohup ./raw_chunk_storage_tool format 21-0000:3b:00.0 21 true false 65536 true 0x1 &

10.72.141.206
nohup ./raw_chunk_storage_tool format 21-0000:3b:00.0 21 true false 65536 true 0x1 &
nohup ./raw_chunk_storage_tool format 22-0000:3c:00.0 22 true false 65536 true 0x2 &
nohup ./raw_chunk_storage_tool format 23-0000:3d:00.0 23 true false 65536 true 0x4 &
nohup ./raw_chunk_storage_tool format 24-0000:3e:00.0 24 true false 65536 true 0x8 &
nohup ./raw_chunk_storage_tool format 25-0000:d8:00.0 25 true false 65536 true 0x10 &
nohup ./raw_chunk_storage_tool format 26-0000:d9:00.0 26 true false 65536 true 0x20 &
nohup ./raw_chunk_storage_tool format 27-0000:da:00.0 27 true false 65536 true 0x40 &
nohup ./raw_chunk_storage_tool format 28-0000:db:00.0 28 true false 65536 true 0x80 &

10.72.141.207
nohup ./raw_chunk_storage_tool format 29-0000:3b:00.0 29 true false 65536 true 0x1 &
nohup ./raw_chunk_storage_tool format 30-0000:3c:00.0 30 true false 65536 true 0x2 &
nohup ./raw_chunk_storage_tool format 31-0000:3d:00.0 31 true false 65536 true 0x4 &
nohup ./raw_chunk_storage_tool format 32-0000:3e:00.0 32 true false 65536 true 0x8 &
nohup ./raw_chunk_storage_tool format 33-0000:d8:00.0 33 true false 65536 true 0x10 &
nohup ./raw_chunk_storage_tool format 34-0000:d9:00.0 34 true false 65536 true 0x20 &
nohup ./raw_chunk_storage_tool format 35-0000:da:00.0 35 true false 65536 true 0x40 &
nohup ./raw_chunk_storage_tool format 36-0000:db:00.0 36 true false 65536 true 0x80 &

10.72.141.14
nohup ./raw_chunk_storage_tool format 37-0000:3b:00.0 37 true false 65536 true 0x1 &
nohup ./raw_chunk_storage_tool format 38-0000:3c:00.0 38 true false 65536 true 0x2 &
nohup ./raw_chunk_storage_tool format 39-0000:3d:00.0 39 true false 65536 true 0x4 &
nohup ./raw_chunk_storage_tool format 40-0000:3e:00.0 40 true false 65536 true 0x8 &
nohup ./raw_chunk_storage_tool format 41-0000:d8:00.0 41 true false 65536 true 0x10 &
nohup ./raw_chunk_storage_tool format 42-0000:d9:00.0 42 true false 65536 true 0x20 &
nohup ./raw_chunk_storage_tool format 43-0000:da:00.0 43 true false 65536 true 0x40 &
nohup ./raw_chunk_storage_tool format 44-0000:db:00.0 44 true false 65536 true 0x80 &



------------
# 10.72.141.201
nohup /root/upfs/chunk/tools/raw_chunk_storage_tool format 0-0000:3b:00.0 0 true false 65536 true 0x1 &
nohup /root/upfs/chunk/tools/raw_chunk_storage_tool format 1-0000:3c00.0 1 true false 65536 true 0x2 &
nohup /root/upfs/chunk/tools/raw_chunk_storage_tool format 2-0000:3d:00.0 2 true false 65536 true 0x4 &
nohup /root/upfs/chunk/tools/raw_chunk_storage_tool format 3-0000:3e:00.0 3 true false 65536 true 0x8 &
nohup /root/upfs/chunk/tools/raw_chunk_storage_tool format 4-0000:d8:00.0 4 true false 65536 true 0x10 &
nohup /root/upfs/chunk/tools/raw_chunk_storage_tool format 5-0000:d9:00.0 5 true false 65536 true 0x20 &
nohup /root/upfs/chunk/tools/raw_chunk_storage_tool format 6-0000:da:00.0 6 true false 65536 true 0x40 &

# 10.72.141.202
nohup /root/upfs/chunk/tools/raw_chunk_storage_tool format 7-0000:3b:00.0 7 true false 65536 true 0x1 &
nohup /root/upfs/chunk/tools/raw_chunk_storage_tool format 8-0000:3c:00.0 8 true false 65536 true 0x2 &
nohup /root/upfs/chunk/tools/raw_chunk_storage_tool format 9-0000:3d:00.0 9 true false 65536 true 0x4 &
nohup /root/upfs/chunk/tools/raw_chunk_storage_tool format 10-0000:3e:00.0 10 true false 65536 true 0x8 &
nohup /root/upfs/chunk/tools/raw_chunk_storage_tool format 11-0000:d8:00.0 11 true false 65536 true 0x10 &
nohup /root/upfs/chunk/tools/raw_chunk_storage_tool format 12-0000:d9:00.0 12 true false 65536 true 0x20 &
nohup /root/upfs/chunk/tools/raw_chunk_storage_tool format 13-0000:da:00.0 13 true false 65536 true 0x40 &

# 10.72.141.204
nohup /root/upfs/chunk/tools/raw_chunk_storage_tool format 14-0000:3b:00.0 14 true false 65536 true 0x1 &
nohup /root/upfs/chunk/tools/raw_chunk_storage_tool format 15-0000:3c:00.0 15 true false 65536 true 0x2 &
nohup /root/upfs/chunk/tools/raw_chunk_storage_tool format 16-0000:3d:00.0 16 true false 65536 true 0x4 &
nohup /root/upfs/chunk/tools/raw_chunk_storage_tool format 17-0000:3e:00.0 17 true false 65536 true 0x8 &
nohup /root/upfs/chunk/tools/raw_chunk_storage_tool format 18-0000:d8:00.0 18 true false 65536 true 0x10 &
nohup /root/upfs/chunk/tools/raw_chunk_storage_tool format 19-0000:d9:00.0 19 true false 65536 true 0x20 &
nohup /root/upfs/chunk/tools/raw_chunk_storage_tool format 20-0000:da:00.0 20 true false 65536 true 0x40 &

------------

# 10.72.141.201
uuid: 36d29e5e-538d-4c9d-9542-f689441d7f3c
chunk_id: 5
uuid: 9216eb1f-fe4f-41cd-8095-77052760b466
chunk_id: 2
uuid: 80d1ca26-6310-4719-9868-4d2e897ecf54
chunk_id: 6
uuid: 7e94ebb1-86b6-4248-82ea-78dd954dcb71
chunk_id: 1
uuid: 3f8d48f3-4fc5-40ba-9b60-4b6708e95a82
chunk_id: 4
uuid: 454277a5-9d85-490d-8956-45b3b5599a77
chunk_id: 0
uuid: 9d17cd71-1311-4115-8e83-e4d753a18b6d
chunk_id: 3

# 10.72.141.202
uuid: 51913686-3199-44a4-8bdf-169de1608695
chunk_id: 13
uuid: 7cc8a676-0633-4395-a6b6-71626c26f83a
chunk_id: 9
uuid: 6fc06dd4-c804-44c3-b72c-a2b9b6a91338
chunk_id: 12
uuid: 930e65b2-4478-4294-8fbe-7d722a326d6c
chunk_id: 8
uuid: 1f1a7cd7-473a-4506-bc32-05cf3108fbdb
chunk_id: 11
uuid: 8809b5d2-668a-4a8b-91e2-dcc83a6ca0c6
chunk_id: 7
uuid: 943462c5-bb4f-41fe-ad7b-b37da98197b3
chunk_id: 10

# 10.72.141.204
uuid: 02a4f5ea-2b8d-4906-9da2-5510db89c904
chunk_id: 20
uuid: 26a1e23e-13ce-4e4f-b08d-ae2c4c097d9b
chunk_id: 16
uuid: 2566f36b-193d-4a40-b82e-559b1a8ea78e
chunk_id: 19
uuid: 11747137-995f-4285-9e83-a16c1f3e975f
chunk_id: 15
uuid: 2c6f6a37-7a80-48aa-b027-5eae044d4edd
chunk_id: 18
uuid: 9e65f56f-c9f1-4f6e-bf92-7d5dd041e948
chunk_id: 14
uuid: 09513c08-4b54-4dcb-bb80-872da89db5ac
chunk_id: 17


10.72.141.206
21 55a6719d-45b2-4718-9f66-4d82ce0d70ec
22 5607f8bb-e26d-43cf-97f1-c675946d69ac
23 e593af7d-f8b7-44fc-9405-e83f7df21d6b
24 288c8321-7ae3-44fd-9111-0541ce7f86ba
25 4583e5ad-a6d0-4ff7-b28c-977c791d34d0
26 2ffbeca5-a5d9-43e2-8789-d916508e83b6
27 5437cd6a-ca42-46bf-b8fc-eb2623b2078b
28 9fb62460-1593-4994-a235-6cc66f27dc33

10.72.141.207
29 784a8cdd-8ee0-4922-8b51-9a30119e76b3
30 b39e1b52-d6b6-4d37-a8b9-41bf187c1786
31 57a87396-fc80-4080-9336-79a30df6c838
32 77e38fce-7b26-4b5d-9a85-031feb6e26ef
33 1a4c83dc-ad20-4835-9777-ab0af5b4af0f
34 2a30ffcf-2467-4de7-aedd-33000c0ea29f
35 6858094f-3ec0-47e8-8ade-d8ed03da58f7
36 691cec1c-c0c5-480d-a005-576b0977ba6d

10.72.141.14
37 1b6390f0-77a5-4021-941b-76a2367bdb90
38 342a358a-ee2a-4c1c-812d-ae91a3a0140c
39 68562cc5-a0a5-43a2-9280-a2216d0ee41d
40 2344d1a1-a844-415a-9132-28b7db2bc9f4
41 4a812537-a660-4979-8fb7-3075cec26432
42 b9de38df-f101-4d1c-9f7b-8de065c281ec
43 8cd9e1f3-c31d-4050-a9f6-6fa0158d7125
44 7e8a88b3-2adb-4d5a-9d3d-41f8580ffbe3

--------------

# 10.72.141.201
sed -i -E 's/device_uuid[ \t ]*=.*/device_uuid454277a5-9d85-490d-8956-45b3b5599a77  0
sed -i -E 's/device_uuid[ \t ]*=.*/device_uuid7e94ebb1-86b6-4248-82ea-78dd954dcb71  1
sed -i -E 's/device_uuid[ \t ]*=.*/device_uuid9216eb1f-fe4f-41cd-8095-77052760b466  2
sed -i -E 's/device_uuid[ \t ]*=.*/device_uuid9d17cd71-1311-4115-8e83-e4d753a18b6d  3
sed -i -E 's/device_uuid[ \t ]*=.*/device_uuid3f8d48f3-4fc5-40ba-9b60-4b6708e95a82  4
sed -i -E 's/device_uuid[ \t ]*=.*/device_uuid36d29e5e-538d-4c9d-9542-f689441d7f3c  5
sed -i -E 's/device_uuid[ \t ]*=.*/device_uuid80d1ca26-6310-4719-9868-4d2e897ecf54  6

# 10.72.141.202
8809b5d2-668a-4a8b-91e2-dcc83a6ca0c6  7
930e65b2-4478-4294-8fbe-7d722a326d6c  8
7cc8a676-0633-4395-a6b6-71626c26f83a  9
943462c5-bb4f-41fe-ad7b-b37da98197b3 10
1f1a7cd7-473a-4506-bc32-05cf3108fbdb 11
6fc06dd4-c804-44c3-b72c-a2b9b6a91338 12
51913686-3199-44a4-8bdf-169de1608695 13

# 10.72.141.204
9e65f56f-c9f1-4f6e-bf92-7d5dd041e948 14
11747137-995f-4285-9e83-a16c1f3e975f 15
26a1e23e-13ce-4e4f-b08d-ae2c4c097d9b 16
09513c08-4b54-4dcb-bb80-872da89db5ac 17
2c6f6a37-7a80-48aa-b027-5eae044d4edd 18
2566f36b-193d-4a40-b82e-559b1a8ea78e 19
02a4f5ea-2b8d-4906-9da2-5510db89c904 20

# 10.72.141.206
55a6719d-45b2-4718-9f66-4d82ce0d70ec 21
5607f8bb-e26d-43cf-97f1-c675946d69ac 22
e593af7d-f8b7-44fc-9405-e83f7df21d6b 23
288c8321-7ae3-44fd-9111-0541ce7f86ba 24
4583e5ad-a6d0-4ff7-b28c-977c791d34d0 25
2ffbeca5-a5d9-43e2-8789-d916508e83b6 26
5437cd6a-ca42-46bf-b8fc-eb2623b2078b 27
9fb62460-1593-4994-a235-6cc66f27dc33 28

# 10.72.141.207
784a8cdd-8ee0-4922-8b51-9a30119e76b3 29
b39e1b52-d6b6-4d37-a8b9-41bf187c1786 30
57a87396-fc80-4080-9336-79a30df6c838 31
77e38fce-7b26-4b5d-9a85-031feb6e26ef 32
1a4c83dc-ad20-4835-9777-ab0af5b4af0f 33
2a30ffcf-2467-4de7-aedd-33000c0ea29f 34
6858094f-3ec0-47e8-8ade-d8ed03da58f7 35
691cec1c-c0c5-480d-a005-576b0977ba6d 36

# 10.72.141.14
1b6390f0-77a5-4021-941b-76a2367bdb90 37
342a358a-ee2a-4c1c-812d-ae91a3a0140c 38
68562cc5-a0a5-43a2-9280-a2216d0ee41d 39
2344d1a1-a844-415a-9132-28b7db2bc9f4 40
4a812537-a660-4979-8fb7-3075cec26432 41
b9de38df-f101-4d1c-9f7b-8de065c281ec 42
8cd9e1f3-c31d-4050-a9f6-6fa0158d7125 43
7e8a88b3-2adb-4d5a-9d3d-41f8580ffbe3 44

--------------

use udisk
db.t_chunk_repair_task_job.dropIndexes()
db.t_chunk_repair_task_job.createIndex({"chunk_repair_task_id":1, "seq":1}, {"name": "for job loading"})
db.t_chunk_repair_task_job.createIndex({"chunk_repair_task_id":1, "pg_id":1, "tgt_chunk": 1, "lc_id": 1, "lc_random_id": 1, "pc_no": 1}, {"name": "for job updating"})
db.t_chunk_repair_task_job.createIndex({"chunk_repair_task_id":1, "state":1}, {"name": "for job checking"})

-----------

loki: [common] get_pc_timeout = 40


-----------

10.0.17.24 chunk-76

-------------

cmd: ssh root@10.72.141.206 sh -c "/root/upfs/chunk/tools/raw_chunk_storage_tool dumpPCMetaJsonByType 25-0000:d8:00.0 true /root/upfs/chunk/tools/chunk-consis-check-chaos-manager-1718077562-22320/meta_chunk_25.js

---------------

128:300000,256:1024,512:1024,1024:512,4096:512,8192:256,16384:256,32768:256,49152:256,65536:512,131072:256,262144:512,1048576:256
128:300000,256:1024,512:1024,1024:512,4096:512,8192:256,16384:256,32768:256,49152:256,65536:512,131072:256,262144:1024,1048576:256



--------------






















```
syntax = "proto3";

service FileTransferService {
    rpc UploadFile(stream UploadFileChunk) returns (UploadFileResponse);

    rpc DownloadFile(DownloadFileRequest) returns (stream DownloadFileChunk);
    
    rpc GetFileSize(GetFileSizeRequest) returns (GetFileSizeResponse);

    rpc DeleteFile(DeleteFileRequest) returns (DeleteFileResponse);

    rpc MakeDir(MakeDirRequest) returns (MakeDirResponse);

    rpc DeleteDir(DeleteDirRequest) returns (DeleteDirResponse);
}

message UploadFileChunk {
    string path = 1;
    bytes content = 2;
}

message UploadFileResponse {
}

message DownloadFileRequest {
    string path = 1;
}

message DownloadFileChunk {
  bytes content = 1;
}

message GetFileSizeRequest {
  string path = 1;
}

message GetFileSizeResponse {
  uint64 size = 1;
}

message DeleteFileRequest {
  string path = 1;
}

message DeleteFileResponse {
}

message MakeDirRequest {
  string path = 1;
}

message MakeDirResponse {
}

message DeleteDirRequest {
  string path = 1;
}

message DeleteDirResponse {
}
```



pssh -h compute.ip -l root -P "mkdir -p /upfs_data/fio_test; cd /upfs_data/fio_test && mkdir -p \`hostname -I\` && cd \`hostname -I\` && nohup fio --name=test --ioengine=libaio  --blocksize=1M --readwrite=read --filesize=1G  --numjobs=10 --iodepth=32 --direct=1 --group_reporting -time_based=1 -runtime=1000 >/dev/null 2>&1 &"

-----------

minimax upfs 带宽利用率:

(15856661924.0/1000/1000/1000*8*3)/(50*10) = 76%

minimax iperf 带宽利用率:

(17110646810.0/1000/1000/1000*8*3)/(50*10) = 82%

---------

Hi, 我们这边用iperf测3台存储机器和10台计算机器之间的多打多，平均带宽大概是400Gbps。理论上10台计算机器每台50Gbps, 3台存储机器每台200Gbps，应该是可以提供500Gbps的带宽的吧。

这个带宽性能还有可能提升吗? 我们这边期望能达到95%的带宽利用率

----------

```bash
# 计算节点:
nohup iperf -s -p 12345 &
nohup iperf -s -p 12347 &
nohup iperf -s -p 12349 &
nohup iperf -s -p 12351 &

# 存储节点:
#!/bin/bash

set -e

# clients=(192.168.1.193 192.168.1.194 192.168.1.195 192.168.1.196 192.168.1.197 192.168.1.198 192.168.1.199 192.168.1.200 192.168.1.201 192.168.1.202)
# clients=(192.168.1.193 192.168.1.194 192.168.1.195 192.168.1.196)
# clients=(192.168.1.193)
# clients=(192.168.1.198 192.168.1.199 192.168.1.200 192.168.1.201)
clients=(192.168.1.196 192.168.1.197 192.168.1.201 192.168.1.202)
ports=(12345 12347 12349 12351)
# duration=180
duration=60

for ip in ${clients[@]}
do
	for port in ${ports[@]}
	do
		iperf -c $ip -p $port -P 16 -t $duration &
	done
done

wait

```


Hi, 这个型号，DAPUSTOR DPH311T5T006T4，我们这边遇到一个问题，设备不能被spdk应用挂载，看内核日志发现提示PCIe设备link down。这个情况是必现的，请问这边有遇到过吗?

pssh -h compute.ip -l root -P "mkdir -p /upfs_data/fio_test; cd /upfs_data/fio_test && mkdir -p \`hostname -I\` && cd \`hostname -I\` && nohup fio --name=test --ioengine=libaio  --blocksize=1M --readwrite=write --filesize=1G  --numjobs=10 --iodepth=24 --direct=1 --group_reporting -time_based=1 -runtime=1000 >/dev/null 2>&1 &"

/root/mongodb-linux-x86_64-3.4.5_37021/bin/mongod -f /root/mongodb-linux-x86_64-3.4.5_37021/mongo.conf

---------------

Hi老铁，能帮我搭一个集群不? wlcb的，用来测厂商给的ssd的新固件。
这个集群里面有些服务已经存在了，但是还有 access, uidx, stateserver, tikvproxy 这4个服务需要搭。


----------------

chunk机器是这里面的 https://git.ucloudadmin.com/ucloud-pfs/pfs_document/-/issues/73#note_1208922 第4行，issue里记这个模式是tcp, 但其实已经是改成 前端tcp, 后端rdma了。

chunk, metaserver, loki, thor 都已经存在了，需要重搭 access, uidx, stateserver, tikvproxy:

set id: 3500

chunk: 10.77.69.136, 10.77.69.137, 10.77.69.147 部署了59个chunk, 没有下线状态；这些机器上也已经部署umongo

metaserver: 10.77.69.136, 10.77.69.137 下线开关已经关闭

thor: 10.77.69.136

loki: 10.77.69.137

------------

mongodb: mongodb://10.77.69.136:37021,10.77.69.137:37021,10.77.69.147:37021/?replicaSet=wlcb_upfs_part3500 这里面只有udisk的db，没有upfs_access

zk:

server: 10.77.98.17:2181,10.77.98.18:2181,10.77.9.127:2181
globalserver: 10.77.98.17:2181,10.77.98.18:2181,10.77.9.127:2181 (和上面一样)

metaserver: /NS/upfs/set3500/metaserver
umongo: /NS/upfs/set3500/umongo_gateway
thor: /NS/upfs/set3500/thor

-----------------

其它:

tiup管理机：172.29.133.205

tikv cluster:  cn-wlcb-upfs-set1

tikv server:

10.77.10.18
10.77.10.19
10.77.10.20

pd: 10.77.10.18:2379,10.77.10.19:2379,10.77.10.20:2379

-----------

我现在需要编写一组可以访问远程目标服务器上的文件系统、远程在目标服务器上执行shell命令的工具。

这组工具由2个部分组成，一个服务端，一个客户端，均使用python3实现。客户端和服务端之间通过grpc框架通信。

具体的proto文件如下, 文件名叫做`agent_rpc.proto`, 其定义了客户端和服务端之间的通信协议:

```proto
syntax = "proto3";

service FileTransferService {
  // 上传文件 
  // 如果服务端发现文件已经存在，会覆盖原文件
  // 如果服务端发现文件不存在，会创建新文件
  // 如果服务端发现目标文件无法创建，或者无法写入，会返回错误
  // 如果服务端发现目标路径是一个目录，会返回错误
  rpc UploadFile(stream UploadFileChunk) returns (UploadFileResponse);

  // 下载文件
  // 如果服务端发现文件不存在，会返回错误
  // 如果服务端发现目标路径是一个目录，会返回错误
  // 如果服务端发现目标文件无法读取，会返回错误
  rpc DownloadFile(DownloadFileRequest) returns (stream DownloadFileChunk);

  // 获取文件大小
  // 如果服务端发现文件不存在，会返回错误
  // 如果服务端发现目标路径是一个目录，会返回错误
  // 如果服务端发现目标文件不可访问，会返回错误
  rpc GetFileSize(GetFileSizeRequest) returns (GetFileSizeResponse);
  
  // 删除文件
  // 如果服务端发现文件不存在，不会返回错误
  // 如果服务端发现目标路径是一个目录，会返回错误
  // 如果服务端发现目标文件不可访问，会返回错误
  rpc DeleteFile(DeleteFileRequest) returns (DeleteFileResponse);
  
  // 创建目录
  // 创建流程类似于mkdir -p, 会递归创建目录
  // 如果服务端发现目录已经存在，不会返回错误
  // 如果服务端发现目标路径是一个文件，会返回错误
  // 如果服务端发现目标路径无法创建，会返回错误
  rpc MakeDir(MakeDirRequest) returns (MakeDirResponse);
  
  // 删除目录
  // 删除流程类似于rm -rf, 会递归删除目录
  // 如果服务端发现目录不存在，不会返回错误
  // 如果服务端发现目标路径是一个文件，会返回错误
  // 如果服务端发现目标路径无法删除，会返回错误
  rpc DeleteDir(DeleteDirRequest) returns (DeleteDirResponse);
  
  // 执行shell命令
  // 如果服务端发现命令执行超时，会返回错误
  rpc ShellCmd(ShellCmdRequest) returns (ShellCmdResponse);
}

message UploadFileChunk {
  string path = 1;
  bytes content = 2;
}

message UploadFileResponse {
}

message DownloadFileRequest {
    string path = 1;
}

message DownloadFileChunk {
  bytes content = 1;
}

message GetFileSizeRequest {
  string path = 1;
}

message GetFileSizeResponse {
  uint64 size = 1;
}

message DeleteFileRequest {
  string path = 1;
}

message DeleteFileResponse {
}

message MakeDirRequest {
  string path = 1;
}

message MakeDirResponse {
}

message DeleteDirRequest {
  string path = 1;
}

message DeleteDirResponse {
}

// 如果timeout为0，表示不限制超时时间
message ShellCmdRequest {
  string cmd = 1;
  int32 timeout = 2;
}

message ShellCmdResponse {
  int32 rc = 1;
  bytes stdout = 2;
  bytes stderr = 3;
}
```

请根据这些信息帮我实现一个服务端python3 脚本 `agent_server.py`，尽量使用协程。

/free_pcs_test/fio_conf_1.ini
/free_pcs_test/fio_conf_2.ini
/free_pcs_test/fio_conf_3.ini
/free_pcs_test/fio_conf_4.ini
/free_pcs_test/fio_conf_5.ini
/free_pcs_test/local-job1-0-verify.state
/free_pcs_test/local-job2-0-verify.state
/free_pcs_test/local-job3-0-verify.state
/free_pcs_test/local-job4-0-verify.state
/free_pcs_test/local-job5-0-verify.state
/free_pcs_test/nohup.out
/free_pcs_test/test_file-1
/free_pcs_test/test_file-2
/free_pcs_test/test_file-3
/free_pcs_test/test_file-4
/free_pcs_test/test_file-5
/free_pcs_test/test.sh

-----------

"is_used" : 1,
"has_detection" : 0,
"seq_no" : 108827009,
"pc_id" : 7240158,
"offset" : 1901335162880,
"pg_id" : 5,
"lc_id" : 1802908709638160,
"pc_no" : 3635,
"lc_random_id" : 1256703082,
"allocate_time" : 1719387734

---------

我现在需要实现一个服务拉起和退出的通知的功能，进行监控的服务叫做server，被监控的服务叫做client。我需要client启动时，server可以收到其启动的通知。client退出时，不论其是正常退出还是异常退出，server也都可以检测到client的退出。server和client都是用python写的。
这个功能有可能通过asyncio和grpc实现吗?


我现在有3个脚本，路径如下:

/a/a.py
/a/b/b1.py
/b/b2.py

我现在需要在a.py中import b1和b2这2个py文件，该怎么写?

----------------

我有一个protobuf message:

```proto
message MyMsg {
  repeated uint32 nums = 1;
}
```

然后我使用python，定义了一个这个pb类型的对象，并尝试将其序列化为json:

```python
  from google.protobuf.json_format import MessageToJson
  import my_msg_pb2

  msg = my_msg_pb2.MyMsg(nums=[])
  json_str = MessageToJson(msg)
  print(json_str)
  
```

输出的结果为

```
'{}'
```

可以看到，输出的结果中不含有字段`nums`，因为其不含有任何元素

现在我希望将这个`nums`输出到`json_str`中，即使它是一个空的集合, 该怎么办?

----------------
































































# 1. 初步筛查

选取1个PG, 1个chunk进行检查:

## 1.

调用一次`GetPGPhysicalChunkRequest+GetPGPhysicalChunkWithCursorRequest`, 获取目标chunk、目标pg内的所有pc，记录各lc_id。

## 2.

间隔30min，向索引发送请求，查询这些lc是否还存在。

如果所有lc均还存在，则代表本次未发现空间泄漏。

## 3.

如果第2步索引服务反馈部分lc已经不存在，则需要再次执行1次`GetPGPhysicalChunkRequest+GetPGPhysicalChunkWithCursorRequest`。

在第二次GetPC的结果中，查找是否存在"索引认为不存在的lc"。如果没有这种lc，则代表2次GetPC间发生了数据回收，目前没有发现空间泄漏。

如果这种lc存在，则代表发生了空间泄漏。

## 4.

如果发现了空间泄漏，则进入尝试"回收空间"的阶段。

如果没有，则间隔30min后再执行一次(每轮1小时)。

如果尝试8次后仍然没有发现空间泄漏, 则基本上可以认为不存在空间泄漏的问题。

# 2. 空间回收

根据初步筛查的方案来扩展:

利用`GetPGPhysicalChunkRequest+GetPGPhysicalChunkWithCursorRequest`, 拉取 __所有chunk__ 内的所有pc。

间隔30min，向索引查询各lc还是否存在。

随后再次使用 `GetPGPhysicalChunkRequest+GetPGPhysicalChunkWithCursorRequest`, 拉取所有chunk内的所有pc。

和第一步一样，这里可以找到所有引起空间泄漏的lc。同时, 这里还可以得到这些lc的大小。

TODO: 是否可以直接回收? 是否需要检查client是否已打开这些文件?

------------

python2中怎么将一个文件压缩成一个gz file?并且我希望压缩完成后，旧文件会被删除。并且，如果目标gz文件在开始压缩前已经存在，先将其删除然后再压缩?

--------------

1. 创建文件时，集群选择方式要改，避免fio同时将大量文件创建在一个集群中。
2. 

-------------

1. 在 `chunk_storage_errorcode.h`引入新错误码: `UDISK_BUSY`。
2. 对于 `ChunkRecyclePCRequest` 和 `ChunkBeginRepairRequest`, chunk 可以返回`UDISK_BUSY`。
3. Loki/Thor 对 `ChunkRecyclePCRequest` 和 `ChunkBeginRepairRequest`均加入重试机制。收到来自chunk的`UDISK_BUSY`返回码后，间隔数秒，随后重试。考虑实现复杂性，可以无限重试(但需要考虑路由变化的问题, thor在每次重试时都需要给当前路由中当前pg的所有chunk都发回收请求, loki则需要根据路由版本问题考虑是否需呀重试; 且修复和回收的重试间隔必须不同, loki的重试周期需长于thor)。
4. `GetPGPhysicalChunkRequest`在chunk中的处理流程不需要变化, 因为获取PC的过程中会加锁，因此期间PC不会增加或减少。但后续修复过程中必须考虑PC不存在的情况。
5. 主chunk收到来自从chunk的repair busy返回值后, 也需要向loki返回busy。事实上根据下方的逻辑可以知晓, 一旦出现busy, 则代表数据正在被回收，理论上我们立刻可以认为本次修复已完成, 但这里我们选择令loki收到busy并重试, 以简化逻辑。
6. 在chunk中, 模仿`recycle barrier`其引入`repair barrier`，其本质为类型 `std::unordered_map<PcEntry, u64>`。和`recycle barrier`一样, 每个IOHandle也拥有一个`repair barrier`。
7. Chunk 中的 recycle 流程中, 在设置 `recycle barrier` 前, 需要检查 `repair barrier` 是否存在。如果`repair barrier`存在, 则直接向thor返回本次repair busy; 否则, 则继续进行recycle。这个检查的过程需要发生在`RecyclePCHandle::SetRecycleBarriers()`内的lambda里, 因为我们需要它发生在对应的IO线程中。 (1. 保证"先repair, 后recycle"的场景下不会出现空间浪费, 即最终这个pc会被回收; 2. 在检查`repair barrier`和设置`recycle barrier`间, 不能放弃线程执行权)
8. 主chunk/requester chunk的repair 流程中, 在向从chunk/responder chunk发送请求前, 首先进入到对应IOHandle的所属线程, 进行如下操作:
  a. 检查是否存在有`recycle barrier`。 如果存在, 则回到manage 线程。在manager 线程返回busy。
  b. 设置 `repair barrier`。a和b这2步之间不能放弃线程执行权。
  c. 检查PC是否存在。这个流程需要通过`OpenPC()`来完成, 这样如果PC存在, 则后续真正发起修复时, 就可以直接从当前IOHandle的`chunk_storage_`的`udisk_handles_`中直接拿取, 减少一次加锁流程。这是一个非阻塞流程, 期间会释放当前线程的执行权。因此需要在这一步之前，先行设置repair barrier(8.b)。检查结束后, 如果PC存在, 则回到manage线程, 执行后续步骤; 如果PC不存在, 则释放repair barrier, 回到manage线程, 返回`UDISK_PC_NOT_EXIST_ERROR`。
9. 对于 `ChunkRecyclePCRequest`, loki 要能够处理返回值为`UDISK_PC_NOT_EXIST_ERROR`的情况, 其需要将这种情况视作修复成功。
10. 对于 从chunk/responder chunk在repair的流程的修改:
  a. 在 `ChunkBeginRepairHandle::PendingIOInLoops()`中的lambda里, 于`StartPendingIO()`前, 检查是否存在有`recycle barrier`。如果存在, 则回到manage 线程, 向requester报告busy。
  b. 随后, 立刻创建`repair barrier`, 然后继续`StartPendingIO()`。
11. TODO: 主chunk中清理`repair barrier`的逻辑。
12. TODO: 从chunk中清理`repair barrier`的逻辑。
13. TODO: 考虑 `repair barrier`能否与`repair buffer`合并? (倾向于不能, 因为主chunk里没有`repair buffer`)。

--------------------------
1. 具体方案设计, 包括PC Meta动态加载和淘汰机制, PC Meta scan 机制, CreatePC时的PC选取机制。预计需要 10人天, 最多可交由2人设计

2. Detection部分, 大量使用了PCMeta。需要修改PC加载的流程, 需提前将含有detection的PCMeta抽出, 放入DetectionTrigger(或者在g_context中新增一个量), 后续当detection发生时, 直接从这里拿取pc meta。(编码2-3天)

3. PC加载流程需要修改。原流程先将所有PCMeta加载至内存, 然后再将其送给数个线程扫描。现需要将其改为多段pc meta的非阻塞式读取。每次读取后, 直接扫描出其中的used pc, free pc和detection pc, 将它们的pc id记下, 随后直接丢弃pc meta数据块。这个流程还需要控制pc meta数据块的并发使用量, 以免占用过多内存。(具体方案未定, 预计编码3-7天)

4. 需要实现PC Meta的动态加载和淘汰机制(需和PC加载流程协作, 预计编码1-3天)

5. 需要修改OpenPC, CreatePC, FreePC机制, 以及Used PC的组织形式, 修改PCHandle结构和各派生类成员函数使其不再含有也不再依赖`PCMeta *`(编码3-4天)

6. 修改Write Delegation机制, 包括RMW, Buffer LRU机制(编码2天)

7. 修改 Recycle 机制(编码1-2天)

8. 不需要修改Repair/Read机制, 利用第5点的修改即可

-------------------------


1. 在 udisk proto 引入新错误码: `EC_UDISK_BUSY`。
2. 对于 `ChunkRecyclePCRequest` 和 `ChunkBeginRepairRequest`, chunk 可以返回`EC_UDISK_BUSY`。
3. Loki/Thor 对 `ChunkRecyclePCRequest` 和 `ChunkBeginRepairRequest`均加入重试机制。收到来自chunk的`EC_UDISK_BUSY`返回码后，间隔数秒，随后重试。考虑实现复杂性，可以无限重试(但需要考虑路由变化的问题, thor在每次重试时都需要给当前路由中当前pg的所有chunk都发回收请求, loki则需要根据路由版本问题考虑是否需要重试; 且修复和回收的重试间隔必须不同, loki的重试周期需长于thor)。 (thor需要在主chunk返回成功后再回收从chunk)
4. `GetPGPhysicalChunkRequest`在chunk中的处理流程不需要变化, 因为获取PC的过程中会加锁，期间PC不会增加或减少。但后续修复过程中必须考虑PC不存在的情况。
5. 主chunk收到来自从chunk的repair busy返回值后, 也需要向loki返回busy。事实上根据下方的逻辑可以知晓, 一旦出现busy, 则代表数据正在被回收，理论上我们立刻可以认为本次修复已完成, 但这里我们选择令loki收到busy并重试, 以简化逻辑。
6. 在manage线程引入 结构 `RecycleRepairApprover` 及其附属:

```c++
class RecycleRepairApprover;

class RecyclePassToken {
    RecyclePassToken(RecycleRepairApprover *approver, const PCEntry &pc_entry);
    RecyclePassToken();
    ~RecyclePassToken();
    // TODO: RAII 相关
    // TODO: 只可移动, 不可拷贝
    // TODO: 需实现 swap接口
    // TODO: 析构时, RAII逻辑需要RunInLoop到manage线程进行

    RecycleRepairApprover *approver_;
    PCEntry pc_entry_;
};

class RepairPassToken {
    RepairPassToken(RecycleRepairApprover *approver, const PCEntry &pc_entry);
    RepairPassToken();
    ~RepairPassToken();
    // TODO: RAII 相关
    // TODO: 只可移动, 不可拷贝
    // TODO: 需实现 swap接口
    // TODO: 析构时, RAII逻辑需要RunInLoop到manage线程进行

    RecycleRepairApprover *approver_;
    PCEntry pc_entry_;
};

class RecycleRepairApprover {
 public:
  std::optional<RecyclePassToken> AskForRecycleApproval(const PCEntry &pc_entry);
  std::optional<RepairPassToken> AskForRepairApproval(const PCEntry &pc_entry);
 private:
  std::unordered_map<PCEntry, u64, PCEntryHash> repairings_;
  std::unordered_map<PCEntry, u64, PCEntryHash> recyclings_;
};
```

```c++
std::optional<RecyclePassToken> RecycleRepairApprover::AskForRecycleApproval(const PCEntry &pc_entry) {
  assertinloop;
  if (repairings_.contains(pc_entry)) {
    return std::nullopt;
  }
  ++recyclings_[pc_entry];
  return RecyclePassToken(this, pc_entry);
}


std::optional<RepairPassToken> RecycleRepairApprover::AskForRepairApproval(const PCEntry &pc_entry) {
  assertinloop;
  if (recyclings_.contains(pc_entry)) {
    return std::nullopt;
  }
  ++repairings_[pc_entry];
  return RepairPassToken(this, pc_entry);
}
```

并为`class ManagerHandle` 添加成员 `RecycleRepairApprover recycle_repair_approver_`, 以及函数 `RecycleRepairApprover &GetRecycleRepairApprover()`。

7. 在 `class ChunkBeginRepairHandle` 中添加成员 `std::vector<std::optional<RepairPassToken>> pass_tokens_`; 在`class RecyclePCHandle`中添加成员 `std::optional<RepairPassToken> pass_token_`, 初值也为 `std::nullopt`。`class ChunkBeginRepairHandle`和`class RecyclePCHandle`是修复和回收的protobuf handler, 每次收到相应请求时均会创建。

8. 在 `class ResCtxRepairRequesterProfile` 中添加成员 `std::optional<RepairPassToken> pass_token_`，`ResourceContext::MallocForRepairRequester()`需要能够初始化此值。 这样, 每次requester chunk发起IO前, 我们可将相应token swap进对应的`ResourceContext`; IO如果正常结束, 我们就再将相应token swap回来; 如果IO超时, 则相应的protobuf handler释放时不会触发RAII, 而`ResourceContext`析构时会触发。

9. 不需要在其它resource context profile 中添加 `std::optional<RepairPassToken> pass_token_`。

10. 回收流程修改:
  10.a. 在`RecyclePCHandle::EntryInit()`中调用`AskForRecycleApproval()`, 如果返回`std::nullopt`, 则向pb发起方返回`UDISK_BUSY`，结束流程。
  10.b. 将上方获得的 `std::optional<RecyclePassToken>` 存入当前`class RecyclePCHandle`的 `pass_token_`。

11. requester chunk的修复流程修改:
  11.a. 在`ChunkBeginRepairHandle::ChunkBeginRepairProcess()`中, 添加对`ChunkBeginRepairHandle::CheckPcExist()`的调用。如果此函数判定目标PC不存在, 则需要被认定为修复完成; 否则则需要向下执行其它逻辑。
  11.b. 在`ChunkBeginRepairHandle::ChunkBeginRepairProcess()`中的 `if (pcs_.size() != 1)` 检查流程之后, 为`pcs_`中的所有pc调用 `AskForRepairApproval()`, 如果返回`std::nullopt`, 则向pb发起方返回`UDISK_BUSY`, 结束流程。
  11.c. 将上方获得的 `std::optional<RepairPassToken>` 存入当前 `class ChunkBeginRepairHandle` 的 `pass_tokens_`。
  11.d. `ChunkBeginRepairHandle::NotifyPendingIOResponse()`: 此函数为requester chunk处理来自responder chunk的pb response的函数。在这里需要添加对responder chunk返回`UDISK_BUSY`的处理逻辑。
  11.e. 修改 `ChunkBeginRepairHandle::SyncBaseData()`:
```c++
void ChunkBeginRepairHandle::SyncBaseData() {
  PhysicalChunkSet::iterator pc_it = pcs_.begin();
  uint64_t lc_id = pc_it->lc_id;
  uint64_t pc_no = pc_it->pc_no;
  ULOG_DEBUG << "Send repair pc to chunk_loop_handle: pg_id=" << pc_it->pg_id
             << ", repair_chunk=" << repair_chunk_id_ << ", lc_id=" << lc_id
             << ", pc_no=" << pc_no;
  // 按照lc_id和pc_no选择某一个固定的线程处理
  auto io_handle = GetIOHandle(lc_id, pc_no);
  decltype(pass_tokens_) tokens;
  tokens.swap(pass_tokens_);
  io_handle->GetLoop()->RunInLoop(std::bind(&RepairHandle::StartSyncBaseData,
                                            io_handle->repair_handle(),
                                            repair_chunk_id_, tokens));
}
```

        并令`RepairHandle::StartSyncBaseData()`接受参数`std::vector<RepairPassToken> &pass_tokens`而非`PhysicalChunkSet pcs`。
  11.f. `RepairHandle::repair_pc_list_`的类型`RepairPCList`需要从`std::list<std::pair<PhysicalChunk, base::Timestamp>>`改为`std::list<std::tuple<PhysicalChunk, std::optional<RepairPassToken>, base::Timestamp>`。
  11.g. `RepairHandle::StartSyncBaseData()` 按这种模式修改:
```c++
void RepairHandle::StartSyncBaseData(uint32_t repair_chunk,
                                     std::vector<RepairPassToken> &pass_tokens) {
  udisk::cluster::ChunkInfoMap chunk_info_map =
      io_handle_->cluster_map().chunks();
  if (chunk_info_map.find(repair_chunk) == chunk_info_map.end()) {
    ULOG_ERROR << "need repair chunk not found, chunk_id=" << repair_chunk;
    return;
  }
  if (!repair_pc_list_.empty()) {  // 已经有正在修复的pc, 加入repair队列
    base::Timestamp now_time(base::Timestamp::now());
    for (auto&& token: pass_tokens) {
      PCEntry pc_entry = token.pc_entry_;
      uint64_t lc_id = pc_entry.lc_id;
      uint64_t pc_no = pc_entry.pc_no;
      uint32_t pg_id = pc_entry.pg_id;
      ULOG_DEBUG << "repair list size=" << repair_pc_list_.size()
                 << ", append new size=" << pcs.size() << ", lc_id=" << lc_id
                 << ", pc_no=" << pc_no << ", pg_id=" << pg_id;
      repair_pc_list_.emplace_back(pc_entry, std::move(token), now_time);
    }
    return;
  }

  // 把需要修复的pc信息加入待修复列表
  base::Timestamp now_time(base::Timestamp::now());
  for (auto&& token: pass_tokens) {
    PCEntry pc_entry = token.pc_entry_;
    repair_pc_list_.emplace_back(pc_entry, std::move(token), now_time);
  }
  repair_chunk_id_ = repair_chunk;
  auto&& [pc_entry, token, time] = repair_pc_list_.front();
  std::optional<RepairPassToken> pass_token = std::move(*token);

  SendRepairPCRequest(std::move(*token), 0);
}
```
  11.h. `RepairHandle::SendRepairPCRequest()`的签名变成如下:
```c++
void RepairHandle::SendRepairPCRequest(RepairPassToken &&pass_token, uint32_t offset);
```

        其需要将参数`pass_token`传递给`ResCtxRepairRequesterProfile`。
        此外, `if (!op)`处的错误处理流程里需要添加free resource_context的逻辑。
  11.i. `RepairHandle::OpenPCResCb()`: 这是`OpenPC()`的回调函数, 这里需要对PC不存在进行处理, 这种情况需要被视为修复成功。
  11.j. `RepairHandle::DoRepairResponse()`这是收到从chunk/responder chunk回包后的处理逻辑。在这里, 成功取出`op`后, 需要将其中的`pass_token_`也取出, 作为参数传给这个函数末尾处的`SendRepairPCRequest()`调用。
  11.k. `RepairHandle::SendNextRepairPC()`中对`SendRepairPCRequest()`的调用也需要修改。

12. Responder chunk的修复流程的修改:
  12.a. 第11.b.中已经添加了对`AskForRepairApproval()`的调用, 因此这里不再添加额外的获取修复许可的逻辑。
  12.b. 每个PC的修复都会伴随生成一个`RepairBuffer`, 这里需要为 `class RepairBuffer` 添加一个成员 `std::optional<RepairPassToken> pass_token_`。
  12.c. 在 `ChunkBeginRepairHandle::PendingIOInLoops()` -> `IOHandle::StartPendingIO()` -> `UDiskHandle::CreateRepairBuffer()` 的调用链上, 需要将 `ChunkBeginRepairHandle::pass_token_` move出, 并沿此调用链一路传递下去。最终, 在`UDiskHandle::CreateRepairBuffer()`中, 将此pass_token传递给新创建的`RepairBuffer`。
  12.d. 在`RepairBuffer::MergePCFinish()` -> `RepairManager::RepairResponseHandle()` 的调用链中, 需要将当前 `RepairBuffer` 中的 `pass_token_` move出, 传给 `RepairResponseHandle()`, 因为 `RepairResponseHandle()` 会在 manage thread 中被调用。这样可以保证RAII可以发生在manage thread里, 能够减少一次跨线程调用A


-----------

1388, {99, 279, 299}














{1301, {218, 238, 435}}
{1355, {473, 454, 358}}
{1355, {473, 454, 358}}



方案1.
10.0.17.12 所有chunk下线: 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416

10.0.17.16 保留 chunk 435, 下线 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434

方案2.
10.0.17.12 所有chunk下线: 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416

10.0.17.22 保留 chunk 454, 下线: 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453,

方案3.
10.0.17.12 所有chunk下线: 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416

10.0.17.23 保留 chunk 473, 下线: 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472

--------

@unoc, Hi, 马方帮忙看下今天 01:55~02:00 test03 以下机器互为源-目的, 是否有RDMA网络异常:

10.77.145.98 <-> 10.77.149.204

这边业务在测试的时候, 一直出现有RDMA发包失败的现象。

----------------

ESXi 8: 4V492-44210-48830-931GK-2PRJ4
vCSA 8: 0Z20K-07JEH-08030-908EP-1CUK4
ESXi 8: 4F40H-4ML1K-M89U0-0C2N4-1AKL4
vCSA 8: 0F41K-0MJ4H-M88U1-0C3N0-0A214
ESXi 8: HG00K-03H8K-48929-8K1NP-3LUJ4
vCSA 8: 4F282-0MLD2-M8869-T89G0-CF240
vSAN 8: NF212-08H0K-488X8-WV9X6-1F024
vSAN 8 witness: JF61H-48K8K-488X9-W98Z0-1FH24
Horizon Enterprise v8.x: 0G4DA-49J81-M80R1-012N4-86KH4

---------------

template<class ... _Args, class _Result>
_Result std::_Bind<_Functor(_Bound_args ...)>::operator()(_Args&& ...) const volatile 

[
  with _Args = {_Args ...};
  _Result = _Result;
  _Functor = void (udisk::chunk::RepairHandle::*)(unsigned int, std::set<udisk::chunk::PhysicalChunk>, std::vector<std::optional<udisk::chunk::RoadblockKeeper> >&&);
  _Bound_args = {udisk::chunk::RepairHandle*, unsigned int, std::set<udisk::chunk::PhysicalChunk, std::less<udisk::chunk::PhysicalChunk>, std::allocator<udisk::chunk::PhysicalChunk> >, std::vector<std::optional<udisk::chunk::RoadblockKeeper>, std::allocator<std::optional<udisk::chunk::RoadblockKeeper> > >}
]

std::_Bind<
  void (
    udisk::chunk::RepairHandle::*(
      udisk::chunk::RepairHandle*, unsigned int, std::set<udisk::chunk::PhysicalChunk>, std::vector<std::optional<udisk::chunk::RoadblockKeeper> >
    )
  )
  (
    unsigned int, std::set<udisk::chunk::PhysicalChunk>, std::vector<std::optional<udisk::chunk::RoadblockKeeper> >&&
  )
>

std::_Bind<
  void (
    udisk::chunk::RepairHandle::*(
      udisk::chunk::RepairHandle*, unsigned int, std::set<udisk::chunk::PhysicalChunk>, std::vector<std::optional<udisk::chunk::RoadblockKeeper> >
    )
  )
  (unsigned int, std::set<udisk::chunk::PhysicalChunk>, std::vector<std::optional<udisk::chunk::RoadblockKeeper> >)
>

udisk::chunk::RecycleHandle::FormatPC(udisk::chunk::RoadblockKeeper&&)::<
lambda(int, udisk::chunk::PCHandle*)
>::<lambda>(const udisk::chunk::RecycleHandle::FormatPC(udisk::chunk::RoadblockKeeper&&)::<lambda(int, udisk::chunk::PCHandle*)>&)

--------------

Hi, 有个问题想请教一下。我们这里在test03有一台机器 10.72.142.4 , 上面的这个目录 /root/node-framework 占了有60G的空间。

大概看了一下，这个目录应该是和主机这边有关系的, 里面主要的内容好像都是log。

我们这边想问下这些log可以删吗?会对主机业务有什么影响吗？


------------

chunk 需要的注错点:
requester: 17 # (responder chunk 发送pb, 致使修复成功, 但因为IO未完成op仍然存在, 因此回收仍然会提示busy) 通过
requester: 139 # 通过
requester: 143 # (IO均已完成, 但responder chunk未响应pb; 此时回收应能够正常进行) 通过
responder: 141 # (responder chunk修复数据落盘超时, 在修复IO返回前, 对responder chunk的回收不能进行, 但对requester chunk的回收可以继续) 通过
修复不存在pc # 通过
recycle: 142 # (主chunk/从chunk修复IO完成前, 回收会提示busy) 通过

----------

黑盒:
1. 存在write only chunk时能否进行回收? # 可以, 通过
2. 主chunk修复busy # 已验证, 减少非待修复chunk的repair_rate和repair_size
3. 从chunk修复busy # 
4. 主chunk回收busy
5. 从chunk回收busy # 已验证
6. 主chunk修复, pc不存在
7. 前台一致性, 副本间一致性 # 通过
8. 无空间泄漏
9. 无预期外的ERROR日志


---------------

1. 收到IO请求
2. 开始dispatch
3. OpenPCCb
4. 本地IO完成 (各LocalAioCb)
5. 每次收到副本的IOResponse (DoChunkResponse)
6. SubmitXXXIOResponse

-----------------
[common]
max_op_pool_num = 4000

[options]
# rdma_qp_max_send_wr                = 2048
rdma_qp_max_send_wr                = 4096
# rdma_mempool_slab_size = 128:150000,512:1024,1024:1024,4096:1024,8192:1024,16384:1024,32768:1024,49152:512,65536:512,131072:256,262144:128,1048576:128
# tcp_mempool_slab_size = 128:300000,256:1024,512:1024,1024:512,4096:512,8192:256,16384:256,32768:256,49152:256,65536:512,131072:256,262144:512,1048576:256
rdma_mempool_slab_size             = 128:150000,512:1024,1024:1024,4096:4096,8192:1024,16384:1024,32768:1024,49152:512,65536:512,131072:512
tcp_mempool_slab_size              = 128:300000,256:1024,512:1024,1024:512,4096:2048,8192:256,16384:256,32768:512,49152:256,65536:512,131072:1024


sed -i -E 's/max_op_pool_num                    = 2500/max_op_pool_num                    = 6000/g' *.conf
sed -i -E 's/rdma_qp_max_send_wr                = 2048/rdma_qp_max_send_wr                = 4096/g' *.conf
ls -1 | grep conf | xargs sed -i '/rdma_qp_max_send_wr                = 4096/a tcp_mempool_slab_size              = 128:300000,256:1024,512:1024,1024:512,4096:2048,8192:256,16384:256,32768:512,49152:256,65536:512,131072:1024' 
ls -1 | grep conf | xargs sed -i '/rdma_qp_max_send_wr                = 4096/a rdma_mempool_slab_size             = 128:150000,512:1024,1024:1024,4096:4096,8192:1024,16384:1024,32768:1024,49152:512,65536:512,131072:512' 



https://git.ucloudadmin.com/ucloud-pfs/message/-/merge_requests/6
https://git.ucloudadmin.com/ucloud-pfs/xutil/-/merge_requests/16
https://git.ucloudadmin.com/ucloud-pfs/thor/-/merge_requests/2
https://git.ucloudadmin.com/ucloud-pfs/loki/-/merge_requests/5
https://git.ucloudadmin.com/ucloud-pfs/xstore/-/merge_requests/18
https://git.ucloudadmin.com/ucloud-pfs/xstore/-/merge_requests/19
https://git.ucloudadmin.com/ucloud-pfs/xstore/-/merge_requests/20

--------------

1. Thor全换多线程支持版本, 但需要单独使用旧message打包。
2. 扫描并删除已删除的lc的pc。参考 https://git.ucloudadmin.com/ucloud-pfs/autodevops/-/blob/146_leakage_tracing/scripts/leakage_tracing/trace_leakage.py 注意有3个set。

----------

5分23秒
5分13秒
3分37秒


------------

1. 拉取所有chunk的所有PC, 并合并lc 
2. 获取当前集群内的所有fs id
3. 查询这些lc是否存在
上面三步是否要并发? 如果并发, 如何控制内存占用
4. 合并上述筛选后剩下的lc
5. 再次从各个chunk中拉取所有PC, 并合并lc, 并和第4步筛选出的lc取交集
6. 再次获取当前集群内的所有fs id
7. 利用第6步的fs id, 再次筛选第5步剩下的lc


-----

我想在python3中执行一个命令行指令, 并且在脚本中实时打印这个命令的stdout与stderr输出, 怎么做?

-----------

⨽(⨽(disksize-8192)/4198464⨼/64)⨼*64

# 无XDP方案
7.68TB盘可以输出约2.33TiB的容量。

因此，使用这种盘搭建3副本UDisk集群, 物理容量价格约为3.27 元/GiB

1829184

# 有 XDP方案
此前测试期间, XDP的压缩率设置为50%。这里仍然依据此值计算。

使用XDP后, 每张盘可以输出约4.65TiB的容量。

因此，使用这种盘搭建3副本UDisk集群, 物理容量价格约为 1.66 元/GiB。

3658432

1090美金 (当前汇率7.02, 7,655.29人民币)

----------------

1. 这里只考虑磁盘和XDP卡的成本, 没有考虑服务器成本。
2. 两种方案均假设使用卡S2.U2.V8.32, 容量为7.68T, 单盘最低价格为7793.625元。
3. XDP卡的价格为1090美金, 按当前汇率1:7.02折算人民币 7655.29元。
4. 根据当前的装机方案, 这里假设每8张SSD共用一张XDP卡。
5. XDP卡的压缩比, 采用此前相关测试时所设置的值, 为50%。也就是说一张XDP卡可以使8张SSD的容量放大100%。
6. 这里仅计算裸盘集群的成本, 不考虑文件系统集群。
7. PC Number和盘的容量的计算关系, 在PC Size <= 4M时为 `pc_cnt = ⨽(⨽((disksize - 8192) * 4194304 / pcsize / ((pc_size + 64) * 4194304 / pcsize + 4096))⨼/64)⨼*64` (disksize)单位为Byte, 因UDisk的裸盘pc size固定为4M, 因此公式为 `pc_num = ⨽(⨽((disksize-8192)/4198464)⨼/64)⨼*64`。
8. 假设集群均为3副本UDisk集群

因此得到的价格计算公式为:

无XDP方案: `7793.625 / (⨽(⨽((7680000000000-8192)/4198464)⨼/64)⨼ * 64 * 4194304 / 3)` 元/字节, 约为 3.27 元/GiB

有XDP方案: `(7793.625 + (7655.29 / 8)) / (⨽(⨽((15360000000000-8192)/4198464)⨼/64)⨼ * 64 * 4194304 / 3)` 元/字节, 约为 1.84 元/GiB

1. 无XDP方案

集群物理容量价格约为3.2722263042 元/GiB

2. 有XDP方案

集群物理容量价格约为 1.8369650823 元/GiB

---

1. 这里只考虑磁盘和XDP卡的成本, 没有考虑服务器成本
2. 两种方案均假设使用卡S2.U2.V8.32, 容量为7.68T, 单盘最低价格为7,793.625元
3. XDP卡的价格为1090美金, 按当前汇率1:7.02折算人民币 7,655.29元

-----------------------------------

1. 修改 used_pc 表的结构:

将每个pg一张表改为每个pg 4张表。

增: 初始化完成后, 只向每个pg的第一张表中增
删: 4个表中都删
改: 不存在
查: 4个表都查, 取seq最高者

不过原有的逻辑存在问题, 如果存在多个相同的pc, 则回收流程只会回收seq最高的pc。这个问题需要解决。

```c++
// 原used pc 表
typedef ska::bytell_hash_map<PCKey, uint32_t, PCKeyHash> PCEntryPCMap;
std::unordered_map<uint32_t, PCEntryPCMap> used_pcs_;

// 新used pc 表
constexpr size_t kPcMapCount = 4u;
using PCEntryPCMap = ska::bytell_hash_map<PCKey, uint32_t, PCKeyHash>;
std::array<std::unordered_map<uint32_t, PCEntryPCMap>, kPcMapCount> used_pcs_;
```

并修改used_pcs_的增删改查接口:
```c++
RawChunkPool::DeallocatePC()
RawChunkPool::GetExistPCByPG()
RawChunkPool::CheckExistingPcNoLock()
RawChunkPool::OpenNoLock()
RawChunkPool::AddFreePCNoLock()
RawChunkPool::AddUsePCNoLock()
RawChunkPool::DumpChunkPoolInfo()
```



2. 修改 pc 的加载、解析、扫描、验证流程, 这几个流程需要合并。

```c++
// **伪码**
// 主要流程在 RawChunkPool::LoadAllPcMetaOnInit()中:

int32_t RawChunkPool::Init(uevent::EventLoop* loop) {
  if (is_init_) {
    ULOG_ERROR << "raw chunk pool has inited";
    return UDISK_ALREADY_INIT;
  }

  // fd_对于spdk_nvme无效，默认是0
  fd_ = dev_manager_->Open(g_context->config().my_id(),
                           g_context->config().device_uuid(),
                           g_context->config().device_prefix());
  if (fd_ < 0) {
    ULOG_ERROR << "open dev error";
    return UDISK_DISK_ERROR;
  }

  LoadAllPcMetaOnInit(loop);
}

// 使用以下函数替代 RawDeviceManager::ReadAllPCMeta(), RawDeviceManager::LoadAllPCMeta(), RawDeviceManager::ParsePCMeta(), RawChunkPool::ScanPCMeta()

void KernelRawDeviceManager::AllocPcMetaBuf() {
  uint64_t pc_meta_total_size = super_block_.pc_num * super_block_.pc_meta_size;
  if (pc_metas_buf_ == nullptr) {
    ::posix_memalign(&pc_metas_buf_, dev_block_size_, pc_meta_total_size);
  }
  if (!pc_metas_buf_) {
    ULOG_SYSERR << "Failed to posix_memalign, size=" << pc_meta_total_size;
    ULOG_FATAL << "Cannot allocate Pc Meta Buf";
  }
}

void SpdkNvmeRawDeviceManager::AllocPcMetaBuf() {
  // 暂保留 pc_meta_buf_, 未来再去掉
  uint64_t pc_meta_total_size = super_block_.pc_num * super_block_.pc_meta_size;
  if (pc_metas_buf_ == nullptr) {
    pc_metas_buf_ = SpdkZmalloc(pc_meta_total_size, dev_block_size_, nullptr,
                                UEVENT_SPDK_SOCKET_ANY);
  }
  if (!pc_metas_buf_) {
    ULOG_SYSERR << "Failed to spdk_zmalloc, size=" << pc_meta_total_size;
    ULOG_FATAL << "Cannot allocate Pc Meta Buf";
  }
}

void RawChunkPool::LoadAllPcMetaOnInit(uevent::EventLoop* loop) {
  AllocPcMetaBuf();
  uevent::Option calc_stack_option;

  // option的设置需要查漏补缺
  stack_option->enable_disk_io_stats = false;
  stack_option->mempool_option.align = uevent::MrManager::kAlignment;
  stack_option->worker_strategy = uevent::Option::kRoundRobin;
  stack_option->mempool_option.enable_spdk = false;
  stack_option->mempool_option.max_arena_size = 256u * 1024u * 1024u;
  stack_option->mempool_option.size_map = {
        {128, 300000},     {256, 1024},       {512, 1024},
        {1 * 1024, 512},   {4 * 1024, 512},   {8 * 1024, 256},
        {16 * 1024, 256},  {32 * 1024, 256}}; // 其实可能根本不需要内存池
  stack_option->mempool_option.look_up_level = 5;
  stack_option->enable_aio = false;

  // 以下代码不能通过编译, 只是通过尝试编写以确认编码难度
  // 总体策略是, 将整个pc meta区切分为4份, stack中的每个线程负责
  // 一片pc meta区。每个线程独立负责一个片区的read下发、MD5检查、used pc和free pc的收集。

  struct LoadLoopHandle : public uevent::LoopHandle {
      static uevent::LoopHandle* CreateMyself(uevent::EventLoop* loop, uevent::EventLoop *io_loop, RawChunkPool *pool, std::mutex &m, CountDownLatch &cl, SuperBlock &sblock) {
        // ...
      }

      uevent::EventLoop *loop;
      uevent::EventLoop *io_loop;
      RawChunkPool *pool;
      size_t current_pc_id = 0;
      size_t end_pc_id = 0;
      int on_going_tasks = 0;
      std::mutex &mu;
      std::condition_variable &cv;
      CountDownLatch &latch;
      std::vector<int> free_blocks;
      size_t free_blocks_report_threshold;
      SuperBlock &super_block;
      size_t size_per_task;
      size_t pcs_per_task;
      size_t max_on_going_tasks

      void Start(size_t fblocks_report_threshold, size_t start_pc_id, size_t total_pc_cnt, size_t concurent_pcs) {
        // 设置各成员变量...

        max_on_going_tasks = 1u * 1024u * 1024u * 1024u / kPcMapCount / kSizePerTask;

        // 批量调用PostOneTask, 直到并发量达到max_on_going_task, 或者发送完所有的pc
      }

      void PostOneTask() {
        size_t this_pcs_cnt = std::min(end_pc_id - current_pc_id, pcs_per_task);
        void *this_mem_start = static_cast<char *>(pool->pc_metas_buf_) + current_pc_id * sizeof(PCMeta);
        // 计算本次task的size
        int ret = pool->AsyncRead(/*填入各参数*/, std::bind(&LoadLoopHandle::PostOneTaskCb, this, /*...*/));
        if (ret != 0) {
          // FATAL
        }
        ++on_going_tasks;
        current_pc_id += this_pcs_cnt;
      }

      void PostOneTaskCb(int ret, void *arg, size_t this_pc_id, size_t this_pc_cnt) {
        // 检查ret, 如果读取失败, 直接fatal

        // 校验各pc的MD5, 如果校验失败, 直接fatal

        // 将各used pc加到 used_pcs_[worker_i], 将各free pc添加到this->free_blocks

        --on_going_tasks;
        if (on_going_tasks == 0 && current_pc_id == end_pc_id) {
          UpdateFreePCs();
          cl.countDown();
          return;
        }
        PostOneTask(); // 考虑是否需要queueinloop
        if (this->free_blocks.size() >= free_blocks_report_threshold) {
          UpdateFreePCs();
        }
      }

      void UpdateFreePCs() {
        std::lock_guard<std::mutex> l(mu);
        for (auto free_pc : free_pcs) {
          // 添加进pool->free_pcs
        }
        free_pcs.clear();
      }
  };

  free_pcs_.Init(/*...*/);

  // 计算各loading worker需要处理多少pc, 以及size等信息
  std::mutex mu;
  CountDownLatch cl(kPcMapCount);
  EventStack stk(kPcMapCount, calc_stack_option, std::bind(&LoadLoopHandle::CreateMyself, /*...*/));
  // 调用stk上各worker的loophandle的Start(), 注意每次的free_blocks_report_threshold都应不相等、互质, 以期各loop错峰添加free pcs
  cl.wait();
}
```

3. 添加后台逻辑, 缓慢将后面3个used pc map中的项目添加到第一个map中。(这点是否需要执行, 存疑)

-------

{ "_id" : ObjectId("66b0793ed6946d6cd1c9977b"), "id" : 2 }
{ "_id" : ObjectId("66b0793ed6946d6cd1c9977c"), "id" : 3 }
{ "_id" : ObjectId("66b0793ed6946d6cd1c9977d"), "id" : 4 }
{ "_id" : ObjectId("66b0793ed6946d6cd1c9977e"), "id" : 5 }
{ "_id" : ObjectId("66b0793ed6946d6cd1c9978b"), "id" : 18 }
{ "_id" : ObjectId("66b0793fd6946d6cd1c9978c"), "id" : 19 }
{ "_id" : ObjectId("66b0793fd6946d6cd1c9978d"), "id" : 20 }
{ "_id" : ObjectId("66b0793fd6946d6cd1c9978e"), "id" : 21 }
{ "_id" : ObjectId("66b0793fd6946d6cd1c9979b"), "id" : 34 }
{ "_id" : ObjectId("66b0793fd6946d6cd1c9979c"), "id" : 35 }
{ "_id" : ObjectId("66b0793fd6946d6cd1c9979d"), "id" : 36 }
{ "_id" : ObjectId("66b0793fd6946d6cd1c9979e"), "id" : 37 }
{ "_id" : ObjectId("66b0793fd6946d6cd1c997ab"), "id" : 50 }
{ "_id" : ObjectId("66b0793fd6946d6cd1c997ac"), "id" : 51 }
{ "_id" : ObjectId("66b0793fd6946d6cd1c997ad"), "id" : 52 }
{ "_id" : ObjectId("66b0793fd6946d6cd1c997ae"), "id" : 53 }
{ "_id" : ObjectId("66b0793fd6946d6cd1c997bb"), "id" : 66 }
{ "_id" : ObjectId("66b0793fd6946d6cd1c997bc"), "id" : 67 }
{ "_id" : ObjectId("66b0793fd6946d6cd1c997bd"), "id" : 68 }
{ "_id" : ObjectId("66b0793fd6946d6cd1c997be"), "id" : 69 }
{ "_id" : ObjectId("66b07940d6946d6cd1c997cb"), "id" : 82 }
{ "_id" : ObjectId("66b07940d6946d6cd1c997cc"), "id" : 83 }
{ "_id" : ObjectId("66b07940d6946d6cd1c997cd"), "id" : 84 }
{ "_id" : ObjectId("66b07940d6946d6cd1c997ce"), "id" : 85 }
{ "_id" : ObjectId("66b07940d6946d6cd1c997db"), "id" : 98 }
{ "_id" : ObjectId("66b07940d6946d6cd1c997dc"), "id" : 99 }
{ "_id" : ObjectId("66b07940d6946d6cd1c997dd"), "id" : 100 }
{ "_id" : ObjectId("66b07940d6946d6cd1c997de"), "id" : 101 }
{ "_id" : ObjectId("66b07940d6946d6cd1c997eb"), "id" : 114 }
{ "_id" : ObjectId("66b07940d6946d6cd1c997ec"), "id" : 115 }
{ "_id" : ObjectId("66b07940d6946d6cd1c997ed"), "id" : 116 }
{ "_id" : ObjectId("66b07940d6946d6cd1c997ee"), "id" : 117 }
{ "_id" : ObjectId("66b07940d6946d6cd1c997fb"), "id" : 130 }
{ "_id" : ObjectId("66b07940d6946d6cd1c997fc"), "id" : 131 }
{ "_id" : ObjectId("66b07940d6946d6cd1c997fd"), "id" : 132 }
{ "_id" : ObjectId("66b07940d6946d6cd1c997fe"), "id" : 133 }
{ "_id" : ObjectId("66b07941d6946d6cd1c9980b"), "id" : 146 }
{ "_id" : ObjectId("66b07941d6946d6cd1c9980c"), "id" : 147 }
{ "_id" : ObjectId("66b07941d6946d6cd1c9980d"), "id" : 148 }
{ "_id" : ObjectId("66b07941d6946d6cd1c9980e"), "id" : 149 }
{ "_id" : ObjectId("66b07941d6946d6cd1c9981b"), "id" : 162 }
{ "_id" : ObjectId("66b07941d6946d6cd1c9981c"), "id" : 163 }
{ "_id" : ObjectId("66b07941d6946d6cd1c9981d"), "id" : 164 }
{ "_id" : ObjectId("66b07941d6946d6cd1c9981e"), "id" : 165 }
{ "_id" : ObjectId("66b07941d6946d6cd1c9982b"), "id" : 178 }
{ "_id" : ObjectId("66b07941d6946d6cd1c9982c"), "id" : 179 }
{ "_id" : ObjectId("66b07941d6946d6cd1c9982d"), "id" : 180 }
{ "_id" : ObjectId("66b07941d6946d6cd1c9982e"), "id" : 181 }
{ "_id" : ObjectId("66b07941d6946d6cd1c9983b"), "id" : 194 }
{ "_id" : ObjectId("66b07941d6946d6cd1c9983c"), "id" : 195 }
{ "_id" : ObjectId("66b07941d6946d6cd1c9983d"), "id" : 196 }
{ "_id" : ObjectId("66b07941d6946d6cd1c9983e"), "id" : 197 }
{ "_id" : ObjectId("66b07942d6946d6cd1c9984b"), "id" : 210 }
{ "_id" : ObjectId("66b07942d6946d6cd1c9984c"), "id" : 211 }
{ "_id" : ObjectId("66b07942d6946d6cd1c9984d"), "id" : 212 }
{ "_id" : ObjectId("66b07942d6946d6cd1c9984e"), "id" : 213 }
{ "_id" : ObjectId("66b07942d6946d6cd1c9985b"), "id" : 226 }
{ "_id" : ObjectId("66b07942d6946d6cd1c9985c"), "id" : 227 }
{ "_id" : ObjectId("66b07942d6946d6cd1c9985d"), "id" : 228 }
{ "_id" : ObjectId("66b07942d6946d6cd1c9985e"), "id" : 229 }
{ "_id" : ObjectId("66b07942d6946d6cd1c9986b"), "id" : 242 }
{ "_id" : ObjectId("66b07942d6946d6cd1c9986c"), "id" : 243 }
{ "_id" : ObjectId("66b07942d6946d6cd1c9986d"), "id" : 244 }
{ "_id" : ObjectId("66b07942d6946d6cd1c9986e"), "id" : 245 }
{ "_id" : ObjectId("66b07942d6946d6cd1c9987b"), "id" : 258 }
{ "_id" : ObjectId("66b07942d6946d6cd1c9987c"), "id" : 259 }
{ "_id" : ObjectId("66b07942d6946d6cd1c9987d"), "id" : 260 }
{ "_id" : ObjectId("66b07942d6946d6cd1c9987e"), "id" : 261 }
{ "_id" : ObjectId("66b07943d6946d6cd1c9988b"), "id" : 274 }
{ "_id" : ObjectId("66b07943d6946d6cd1c9988c"), "id" : 275 }
{ "_id" : ObjectId("66b07943d6946d6cd1c9988d"), "id" : 276 }
{ "_id" : ObjectId("66b07943d6946d6cd1c9988e"), "id" : 277 }
{ "_id" : ObjectId("66b07943d6946d6cd1c9989b"), "id" : 290 }
{ "_id" : ObjectId("66b07943d6946d6cd1c9989c"), "id" : 291 }
{ "_id" : ObjectId("66b07943d6946d6cd1c9989d"), "id" : 292 }
{ "_id" : ObjectId("66b07943d6946d6cd1c9989e"), "id" : 293 }
{ "_id" : ObjectId("66b07943d6946d6cd1c998ab"), "id" : 306 }
{ "_id" : ObjectId("66b07943d6946d6cd1c998ac"), "id" : 307 }
{ "_id" : ObjectId("66b07943d6946d6cd1c998ad"), "id" : 308 }
{ "_id" : ObjectId("66b07943d6946d6cd1c998ae"), "id" : 309 }


